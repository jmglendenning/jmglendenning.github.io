[
  {
    "objectID": "project01.html",
    "href": "project01.html",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Here are several project ideas that combine voting, education, and health data, and lend themselves to interactive visualization in R Shiny with artistic 2D charts.\n\n\n\n\nConcept: Explore the relationship between civic participation (voting turnout) and community well-being (educational attainment, health outcomes like obesity, life expectancy, or access to healthcare).\n\nGeospatial Component: County- or state-level choropleth maps showing turnout vs.¬†health/education indices.\n\nSophisticated 2D Visualizations:\n\nChord diagrams linking regions with high turnout to health and education performance.\n\nRadar charts comparing regions across civic health indicators.\n\nInteractive beeswarm plots showing how counties cluster by turnout and well-being.\n\n\n\n\n\n\n\nConcept: Visualize how educational inequality and health disparities predict or correlate with voter participation.\n\nGeospatial Component: Optional hexbin maps to avoid irregular geographic shapes.\n\nSophisticated Visualizations:\n\nRidgeline plots comparing turnout distributions across education or health quintiles.\n\nSankey diagrams connecting educational attainment ‚Üí health outcomes ‚Üí turnout.\n\nAnimated scatter plots over time (2000‚Äì2024) showing shifting relationships.\n\n\n\n\n\n\n\nConcept: Focus on temporal change. How have turnout rates, graduation rates, and public health measures shifted together over the past decades?\n\nGeospatial Component: Time-lapse choropleths or animated small multiples by state.\n\nSophisticated Visualizations:\n\nStreamgraphs to show trends of participation vs.¬†education vs.¬†health.\n\nAlluvial plots tracking states moving between ‚Äúlow ed/low health/low vote‚Äù categories.\n\nCircular barplots for an artistic, unconventional style.\n\n\n\n\n\n\n\nConcept: Examine whether educational attainment and health outcomes correlate with swing state behavior in elections (close margins, shifts between parties).\n\nGeospatial Component: State-level cartograms emphasizing electoral votes rather than land area.\n\nSophisticated Visualizations:\n\nHexbin scatterplots of turnout vs.¬†education/health metrics with party overlays.\n\nDensity contour plots emphasizing clusters of swing vs.¬†safe states.\n\nInteractive ternary plots (education, health, voting as vertices).\n\n\n\n\n\n\n\nConcept: Identify areas where low education, poor health, and low turnout overlap (‚Äúcivic deserts‚Äù) vs.¬†areas with high values on all three (‚Äúcivic gardens‚Äù).\n\nGeospatial Component: Interactive maps highlighting these clusters.\n\nSophisticated Visualizations:\n\nVoronoi diagrams to map ‚Äúcivic desert‚Äù zones without strict boundaries.\n\nTreemaps or sunbursts showing nested categories (education ‚Üí health ‚Üí turnout).\n\nPolar coordinate plots for community comparisons.\n\n\n\n\n\n\n\nConcept: Investigate whether counties/states with higher or lower life expectancy tend to vote differently and whether education mediates this effect.\n\nGeospatial Component: Interactive choropleths by county with linked scatterplots.\n\nSophisticated Visualizations:\n\nBubble plots in polar coordinates to highlight clustering.\n\nHeatmaps with dendrograms showing cross-indicator correlations.\n\nInteractive slopegraphs showing turnout and life expectancy changes.\n\n\n\n\n\n\nTo make the visualizations sophisticated, artistic, and attractive: - Use custom color palettes (viridis, wesanderson, scientific palettes).\n- Explore non-traditional layouts (circular, radial, layered).\n- Add interactive storytelling: clicking on a region brings up linked health/education/voting histories.\n\n\n\nHere is a curated table list from IPUMS NHGIS that aligns with your project‚Äôs goals for identifying civic deserts and civic gardens at the county level, using the domains of education, health, and voter turnout (via external source).\n‚∏ª\nüßæ IPUMS NHGIS Table List\nThese tables are from ACS 5-Year Estimates, which provide the most detailed and stable county-level data.\n\nEducation\n\nTable ID Table Title Key Columns B15003 Educational Attainment for the Population 25 Years and Over Count of individuals by highest level of education completed C15002 Sex by Educational Attainment for the Population 25 Years and Over Gender comparison (optional) S1501 Educational Attainment Summary Table Percent with &lt; HS, HS, Some College, BA+, etc.\n‚∏ª\n\nHealth (Proxies)\n\nTable ID Table Title Key Columns S1810 Disability Characteristics % with hearing, vision, cognitive, ambulatory, self-care, independent living difficulties B27010 Types of Health Insurance Coverage by Age % uninsured, private, public insurance S2701 Health Insurance Coverage Summary Coverage breakdowns by age and gender\nüí° Note: If needed, supplement with CDC PLACES or County Health Rankings for obesity, diabetes, poor health days, etc.\n‚∏ª\n\nTurnout\n\nNo direct IPUMS NHGIS table provides turnout, so use external data and join by FIPS:\nSource Metric Notes MIT Election Lab Total votes / Voting-age population (VAP) County-level turnout in general elections Harvard Election Data Archive Voter turnout Also includes registration and vote choice by county State Election Offices For state-specific turnout rates Check Secretary of State or elections board\n‚∏ª\n\nGeographic Identifiers & Population\n\nTable ID Table Title Key Columns B01001 Sex by Age Use total count for denominator in rates (e.g., % uninsured) ‚Äî County shapefiles Download with ‚ÄúNHGIS Boundary Files‚Äù (match via GISJOIN or FIPS)\n‚∏ª\nOptional Enhancements\nDomain Table Use Poverty S1701 Correlate with civic deserts Employment S2301 Add context (e.g., joblessness and civic disengagement) Language B16004 Language isolation may correlate with civic participation\n‚∏ª\nFinal Package\nWhen ordering from IPUMS NHGIS: ‚Ä¢ Use the Data Finder tool ‚Ä¢ Select ACS 5-Year Estimates ‚Ä¢ Filter by County geographic level ‚Ä¢ Include tables: ‚Ä¢ B15003, S1501, S1810, B27010, S2701, B01001, plus any others above ‚Ä¢ Include 2020 boundaries (or consistent year across tables)\n‚∏ª\nWould you like me to generate a pre-filled download script for NHGIS or help prep these for use in a Shiny app or Quarto dashboard?"
  },
  {
    "objectID": "project01.html#civic-health-dashboard",
    "href": "project01.html#civic-health-dashboard",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Concept: Explore the relationship between civic participation (voting turnout) and community well-being (educational attainment, health outcomes like obesity, life expectancy, or access to healthcare).\n\nGeospatial Component: County- or state-level choropleth maps showing turnout vs.¬†health/education indices.\n\nSophisticated 2D Visualizations:\n\nChord diagrams linking regions with high turnout to health and education performance.\n\nRadar charts comparing regions across civic health indicators.\n\nInteractive beeswarm plots showing how counties cluster by turnout and well-being."
  },
  {
    "objectID": "project01.html#the-social-gradient-in-democracy",
    "href": "project01.html#the-social-gradient-in-democracy",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Concept: Visualize how educational inequality and health disparities predict or correlate with voter participation.\n\nGeospatial Component: Optional hexbin maps to avoid irregular geographic shapes.\n\nSophisticated Visualizations:\n\nRidgeline plots comparing turnout distributions across education or health quintiles.\n\nSankey diagrams connecting educational attainment ‚Üí health outcomes ‚Üí turnout.\n\nAnimated scatter plots over time (2000‚Äì2024) showing shifting relationships."
  },
  {
    "objectID": "project01.html#education-health-and-the-vote-over-time",
    "href": "project01.html#education-health-and-the-vote-over-time",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Concept: Focus on temporal change. How have turnout rates, graduation rates, and public health measures shifted together over the past decades?\n\nGeospatial Component: Time-lapse choropleths or animated small multiples by state.\n\nSophisticated Visualizations:\n\nStreamgraphs to show trends of participation vs.¬†education vs.¬†health.\n\nAlluvial plots tracking states moving between ‚Äúlow ed/low health/low vote‚Äù categories.\n\nCircular barplots for an artistic, unconventional style."
  },
  {
    "objectID": "project01.html#education-health-as-predictors-of-swing-state-outcomes",
    "href": "project01.html#education-health-as-predictors-of-swing-state-outcomes",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Concept: Examine whether educational attainment and health outcomes correlate with swing state behavior in elections (close margins, shifts between parties).\n\nGeospatial Component: State-level cartograms emphasizing electoral votes rather than land area.\n\nSophisticated Visualizations:\n\nHexbin scatterplots of turnout vs.¬†education/health metrics with party overlays.\n\nDensity contour plots emphasizing clusters of swing vs.¬†safe states.\n\nInteractive ternary plots (education, health, voting as vertices)."
  },
  {
    "objectID": "project01.html#civic-deserts-and-civic-gardens",
    "href": "project01.html#civic-deserts-and-civic-gardens",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Concept: Identify areas where low education, poor health, and low turnout overlap (‚Äúcivic deserts‚Äù) vs.¬†areas with high values on all three (‚Äúcivic gardens‚Äù).\n\nGeospatial Component: Interactive maps highlighting these clusters.\n\nSophisticated Visualizations:\n\nVoronoi diagrams to map ‚Äúcivic desert‚Äù zones without strict boundaries.\n\nTreemaps or sunbursts showing nested categories (education ‚Üí health ‚Üí turnout).\n\nPolar coordinate plots for community comparisons."
  },
  {
    "objectID": "project01.html#life-expectancy-and-the-vote",
    "href": "project01.html#life-expectancy-and-the-vote",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Concept: Investigate whether counties/states with higher or lower life expectancy tend to vote differently and whether education mediates this effect.\n\nGeospatial Component: Interactive choropleths by county with linked scatterplots.\n\nSophisticated Visualizations:\n\nBubble plots in polar coordinates to highlight clustering.\n\nHeatmaps with dendrograms showing cross-indicator correlations.\n\nInteractive slopegraphs showing turnout and life expectancy changes."
  },
  {
    "objectID": "project01.html#additions",
    "href": "project01.html#additions",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "To make the visualizations sophisticated, artistic, and attractive: - Use custom color palettes (viridis, wesanderson, scientific palettes).\n- Explore non-traditional layouts (circular, radial, layered).\n- Add interactive storytelling: clicking on a region brings up linked health/education/voting histories."
  },
  {
    "objectID": "project01.html#datasets-for-option-5",
    "href": "project01.html#datasets-for-option-5",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Here is a curated table list from IPUMS NHGIS that aligns with your project‚Äôs goals for identifying civic deserts and civic gardens at the county level, using the domains of education, health, and voter turnout (via external source).\n‚∏ª\nüßæ IPUMS NHGIS Table List\nThese tables are from ACS 5-Year Estimates, which provide the most detailed and stable county-level data.\n\nEducation\n\nTable ID Table Title Key Columns B15003 Educational Attainment for the Population 25 Years and Over Count of individuals by highest level of education completed C15002 Sex by Educational Attainment for the Population 25 Years and Over Gender comparison (optional) S1501 Educational Attainment Summary Table Percent with &lt; HS, HS, Some College, BA+, etc.\n‚∏ª\n\nHealth (Proxies)\n\nTable ID Table Title Key Columns S1810 Disability Characteristics % with hearing, vision, cognitive, ambulatory, self-care, independent living difficulties B27010 Types of Health Insurance Coverage by Age % uninsured, private, public insurance S2701 Health Insurance Coverage Summary Coverage breakdowns by age and gender\nüí° Note: If needed, supplement with CDC PLACES or County Health Rankings for obesity, diabetes, poor health days, etc.\n‚∏ª\n\nTurnout\n\nNo direct IPUMS NHGIS table provides turnout, so use external data and join by FIPS:\nSource Metric Notes MIT Election Lab Total votes / Voting-age population (VAP) County-level turnout in general elections Harvard Election Data Archive Voter turnout Also includes registration and vote choice by county State Election Offices For state-specific turnout rates Check Secretary of State or elections board\n‚∏ª\n\nGeographic Identifiers & Population\n\nTable ID Table Title Key Columns B01001 Sex by Age Use total count for denominator in rates (e.g., % uninsured) ‚Äî County shapefiles Download with ‚ÄúNHGIS Boundary Files‚Äù (match via GISJOIN or FIPS)\n‚∏ª\nOptional Enhancements\nDomain Table Use Poverty S1701 Correlate with civic deserts Employment S2301 Add context (e.g., joblessness and civic disengagement) Language B16004 Language isolation may correlate with civic participation\n‚∏ª\nFinal Package\nWhen ordering from IPUMS NHGIS: ‚Ä¢ Use the Data Finder tool ‚Ä¢ Select ACS 5-Year Estimates ‚Ä¢ Filter by County geographic level ‚Ä¢ Include tables: ‚Ä¢ B15003, S1501, S1810, B27010, S2701, B01001, plus any others above ‚Ä¢ Include 2020 boundaries (or consistent year across tables)\n‚∏ª\nWould you like me to generate a pre-filled download script for NHGIS or help prep these for use in a Shiny app or Quarto dashboard?"
  },
  {
    "objectID": "prepare05.html",
    "href": "prepare05.html",
    "title": "Prepare for Class 05",
    "section": "",
    "text": "1 Reading:\n\nWickham, Hadley. 2010. ‚ÄúA layered grammar of graphics.‚Äù Journal of Computational and Graphical Statistics 19, no. 1: 3-28. (Great primer for ggplot2)\n\n\nCompleted\n\n\nWickham, Hadley. 2016. ggplot2: elegant graphics for data analysis. Springer.\n\n\nCompleted\n\n\nThe Parable of Google Flu: Traps in Big Data Analysis (Original article on Nature)\n\n\nCompleted\n\n\nPost a one-page note on your website on:\n\nBig data analytics pitfall\nOverfitting and overparameterization\n\n\nBig Data Analytics Pitfall\nThe influenza surveillance study by Ginsberg et al.¬†showed both the promise and risks of big data usage. By analyzing billions of Google search queries, the authors showed that search data possibly could estimate influenza-like illness 1 to 2 weeks earlier than CDC reports.\nThe primary pitfall lay in assuming that correlation is equivalent to causation. Search patterns can shift because of media coverage, public concern, or unrelated events, for example, a drug recall or a news story about influenza may increase search volume even when actual cases do not rise. Such patterns could create false alerts and reduce the reliability of the system. Big data signals are a complementary tool to traditional methods, but not replacements for them.\nOverfitting and Overparameterization\nAnother risk is methodological. The study tested 50 million search queries, ultimately using forty-five that provided the strongest correlation with CDC data. While the results showed a very high correlation, the danger and probability of overfitting remained.\nOverfitting occurs when a model captures patterns that are coincidental rather than meaningful. For example, queries about ‚Äòhigh school basketball‚Äô correlated with flu season simply because both occur in winter, not because basketball is related to influenza. This illustrated the danger of overparameterization. If too many predictors were considered, strange relationships could appear statistically strong but lack predictive validity in new or different contexts. It is a classic problem in machine learning, where a model could fix the training data extremely well, but fail to have any predicitive power.\nConclusion\nBig data analytics have provided valuable early warning systems, but they require caution. Validation, model simplicity, and domain knowledge are essential to prevent misleading outputs. Analysts must balance any predictive accuracy with interpretability to ensure models remain robust under changing conditions.\n\n\n2 Watch:\n\nHadley Wickham: Data Visualization and Data Science (EMBL)\nName the technologies/techniques Wickham introduced. What are his main points? Summarize and comment.\n\nHadley Wickham: Technologies, Techniques, and Main Ideas\nTechnologies and Techniques Introduced\nIn this talk, Hadley Wickham describes the evolution of his ideas and tools for data visualization and analysis in R. The main technologies and techniques he introduced include:\n\nLattice ‚Äì an early R package for creating small multiples or repeated visualizations of subsets of data. This experience revealed the limits of static visualization frameworks.\nggplot and ggplot2 ‚Äì R packages implementing the Grammar of Graphics. These introduced a modular, layered way to construct plots by mapping data variables to visual aesthetics (aes) and choosing geometric objects (geoms).\nTidy Data ‚Äì a systematic data structure where each variable forms a column and each observation forms a row. This concept ensures data is organized for analysis and visualization.\nThe Tidyverse ‚Äì a collection of interrelated R packages (such as dplyr, tidyr, readr, ggplot2, purrr, tibble, and others) that share consistent design principles and syntax to support the full data science workflow.\nR Markdown ‚Äì a framework for combining prose, code, and visualizations into reproducible reports, ensuring that results can be updated automatically when data changes.\nFaceting, Mapping, and Animation ‚Äì ggplot2 features such as faceting (small multiples) and packages like gganimate that extend visualization to temporal and spatial dimensions.\n\nMain Points\nWickhams talk was broken up into three sections.\nWhat Is a Visualization\nWickham described visualization as a way of thinking about data, not just making pictures.\nVisualization reveals structure, error, and meaning. It is an analytic tool, not purely a decorative addtion.\nUsing Rosling‚Äôs Gapminder example, Wickham showed that many visualization challenges are actually data structure problems. Wickham‚Äôs solution was ‚Äòtidy data‚Äô: each variable in a column and each observation in a row. This consistent format allows data to map cleanly to perception, forming the conceptual foundation for the Grammar of Graphics.\n‚ÄúThe first step, whenever you‚Äôre dealing with a data set, is to tidy it up‚Äîit will make the rest of your life much easier.‚Äù\nThe Power of Orthogonal Components\nWickham builds on the Grammar of Graphics, emphasizing orthogonality, which is the idea that every visualization consists of modular, independent parts such as data, aesthetics, geometries, scales, and facets.\nRather than using chart types, mappings between variables and visual properties are defined. Isolating these components, ggplot2 allows any element to be swapped or recombined. Wickham demonstrated this through successive refinements of the Gapminder plot‚Äîadjusting scales, colors, and sizes‚Äîand by turning the code into a reusable function.\n\n\n3 The Power of Code\nWickham acknowledged that code can be daunting, but insists code is text, which is readable, shareable, and reproducible. He highlighted three strengths of using code. - Readability: Code can be read like a sentence\n- Reproducibility: Every transformation is explicit and reviewable - Integration: R Markdown merges prose, code, and results in one reproducible document\nThis philosophy underlies the tidyverse which is that tools should make the path of least resistance into the path to good practice. Code is not an obstacle to visualization. It is what makes visualization transparent and sustainable.\nWickham‚Äôs ultimate goal is to build a ‚Äòpit of success‚Äô, which is a design environment where users naturally fall into effective habits. Visualization provides immediate feedback that motivates learning and connects reasoning to visual evidence.\n\n\n4 Quarto recap:\n\nQuarto Workshop by Tom Mock\n\n\nCompleted\n\nCreate & Publish a Quarto Blog on Quarto Pub in 100 Seconds by Tom Mock\n\nCompleted\n\nQuarto Cheatsheet (bookmark it!)\n\n\n\n\n5 Notes\nCertain elements of this preparation were enhanced with an LLM including code restructuring, commenting, and information layout."
  },
  {
    "objectID": "coins01.html",
    "href": "coins01.html",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "This project was a comprehensive analysis of the spatial distribution of coinage originating from various mints and subsequently deposited in hoards across the Roman world between approximately 500 BCE and 200 CE. By examining patterns in coin dispersal and quantifying the distances between points of minting and findspots, the study aimed to examine the broader dynamics of monetary circulation, and economic integration within the Roman economy. The ultimate objective was to generate insights into the mechanisms of coin movement and to contribute to our understanding of the structure and function of the ancient Roman economic system.\nThe first step that I‚Äôd like to look at is the probability of a coin existing in a certain location base on hoard finds.\nNote that there are several errors and mislabeled figures in this. This is for discussion purposes only."
  },
  {
    "objectID": "coins01.html#data-source",
    "href": "coins01.html#data-source",
    "title": "Project - Brainstorming",
    "section": "Data Source",
    "text": "Data Source\nFor the initial analysis, the data from the Coin Hoards of the Roman Republic was accessed. This dataset is based of the work of Crawford (1969), as enhanced by Gruber and Lockyear.\nThis data can be accessed via the website‚Äôs APIs or via the SPARQL endpoint at Nomisma. Not all hoards were appropriate for analysis. This initial analysis only included hoard findspots that had the following:\n\nGeolocation of hoard\nAt least 1 coin with a known mint\nKnown mint geolocation\n\nThe findspot information was mostly within a 5 kilometer radius of the findspot. Some was 10 km, but that was less often. For security reasons, the exact geolocation was never directly published to prevent theft and looting.\nFor now, all the findspots meeting the criteria were used, but this may be narrowed in the future."
  },
  {
    "objectID": "coins01.html#data-acquisition",
    "href": "coins01.html#data-acquisition",
    "title": "Project - Brainstorming",
    "section": "Data Acquisition",
    "text": "Data Acquisition\nTo acquire the data, first a list of all the hoards in the CHRR dataset was gathered from the SPARQL endpoint.\n\nPREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\nPREFIX nmo: &lt;http://nomisma.org/ontology#&gt;\n\nSELECT DISTINCT ?hoardID\nWHERE {\n  ?hoard a nmo:Hoard .\n  FILTER(STRSTARTS(STR(?hoard), \"http://numismatics.org/chrr/id/\"))\n  BIND(REPLACE(STR(?hoard), \"http://numismatics.org/chrr/id/\", \"\") AS ?hoardID)\n}\nORDER BY ?hoardID\n\nUsing the list of three digit hoard identifiers from this list, an R program was written to download all of the identified hoards using the schema:\n# ---- 3. Define File Formats and URI Templates ----\nformats &lt;- list(\n  xml     = \"http://numismatics.org/chrr/id/XXX\",\n  rdf     = \"https://numismatics.org/chrr/id/XXX.rdf\",\n  ttl     = \"https://numismatics.org/chrr/id/XXX.ttl\",\n  jsonld  = \"https://numismatics.org/chrr/id/XXX.jsonld\",\n  geojson = \"https://numismatics.org/chrr/id/XXX.geojson\"\n)\n‚ÄòXXX‚Äô was replaced by the three digit hoard identifier.\nExample hoard identifier list:\n...\nAVO\nAVV\nAVZ\nAZA\nAZN\nAZU\n...\nAll of the RDF/XML, TTL, JSON-LD, and GeoJSON data were gathered for the hoards. The code for this is not included in this file.\nFrom this dataset, all the coin identifiers in the dataset were extracted. The URIs were in the form:\n\n# ---- 3. Define Format URIs ----\nformat_urls &lt;- list(\n  xml      = \"http://numismatics.org/crro/id/XXX.xml\",\n  rdf      = \"https://numismatics.org/crro/id/XXX.rdf\",\n  ttl      = \"https://numismatics.org/crro/id/XXX.ttl\",\n  jsonld   = \"https://numismatics.org/crro/id/XXX.jsonld\",\n  geojson  = \"https://numismatics.org/crro/id/XXX.geojson\",\n  manifest = \"http://numismatics.org/crro/manifest/XXX\"\n)\n\nThe coin ids, which replaced the XXX above, were in the form:\n...\nrrc-210.1\nrrc-214.1b\nrrc-232.1\nrrc-235.1c\nrrc-236.1a\nrrc-243.1\nrrc-247.1\nrrc-260.1\nrrc-270.1\n...\n\nAll of the RDF/XML, TTL, JSON-LD, and GeoJSON data were gathered for the hoards. The code for this is not included in this file.\nIn addition"
  },
  {
    "objectID": "coins01.html#data-processing",
    "href": "coins01.html#data-processing",
    "title": "Project - Brainstorming",
    "section": "Data Processing",
    "text": "Data Processing\nThe a sample of the initial set of data looks like:\n\n# Load the data\nhoards_df &lt;- read_csv(\"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/Roman Italy GIS/Coin Project/chrr_hoards/hoard_geojson/combined_hoards_with_distance.csv\")\n\nRows: 926 Columns: 13\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (7): hoard_id, hoard_name, hoard_uri, hoard_geometry, mint_name, mint_ur...\ndbl (6): hoard_lat, hoard_lon, mint_average_count, mint_lat, mint_lon, mint_...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Filter for Rome mint, take first 20 rows, keep all columns\nrome_subset &lt;- hoards_df %&gt;%\n  filter(mint_name == \"Rome\") %&gt;%\n  slice_head(n = 10)\n\n# Print as a nicely formatted table\nkable(rome_subset, caption = \"First 20 Hoards with Rome as Mint\")\n\n\nFirst 20 Hoards with Rome as Mint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhoard_id\nhoard_name\nhoard_uri\nhoard_lat\nhoard_lon\nhoard_geometry\nmint_name\nmint_uri\nmint_average_count\nmint_lat\nmint_lon\nmint_geometry\nmint_hoard_distance_km\n\n\n\n\n1PO\nPoiana CrƒÉcƒÉoani (Romania)\nhttps://sws.geonames.org/670093/\n47.06667\n26.31667\nPOINT (26.31667 47.06667)\nRome\nhttp://nomisma.org/id/rome\n123\n41.9\n12.5\nPOINT (12.5 41.9)\n1236.48816\n\n\nABE\nAbertura (Spain)\nhttps://sws.geonames.org/2522486/\n39.24352\n-5.81394\nPOINT (-5.81394 39.24352)\nRome\nhttp://nomisma.org/id/rome\n23\n41.9\n12.5\nPOINT (12.5 41.9)\n1573.37081\n\n\nACT\n√Åktion (Greece)\nhttps://sws.geonames.org/265389/\n38.94019\n20.76875\nPOINT (20.76875 38.94019)\nRome\nhttp://nomisma.org/id/rome\n3\n41.9\n12.5\nPOINT (12.5 41.9)\n773.90425\n\n\nADJ\nAmƒÉrƒÉ≈ütii de Jos (Romania)\nhttps://sws.geonames.org/686386/\n43.95000\n24.16667\nPOINT (24.16667 43.95)\nRome\nhttp://nomisma.org/id/rome\n4\n41.9\n12.5\nPOINT (12.5 41.9)\n977.06559\n\n\nADM\nMassa d‚ÄôAlbe (Italy)\nhttps://sws.geonames.org/3173765/\n42.10723\n13.39429\nPOINT (13.39429 42.10723)\nRome\nhttp://nomisma.org/id/rome\n93\n41.9\n12.5\nPOINT (12.5 41.9)\n77.49022\n\n\nADR\nAlcal√° del R√≠o (Spain)\nhttps://sws.geonames.org/2522474/\n37.51780\n-5.98185\nPOINT (-5.98185 37.5178)\nRome\nhttp://nomisma.org/id/rome\n159\n41.9\n12.5\nPOINT (12.5 41.9)\n1652.39657\n\n\nADU\nAlbanchez de M√°gina (Spain)\nhttps://sws.geonames.org/2522239/\n37.79263\n-3.46833\nPOINT (-3.46833 37.79263)\nRome\nhttp://nomisma.org/id/rome\n16\n41.9\n12.5\nPOINT (12.5 41.9)\n1436.74096\n\n\nAGG\nAggius (Italy)\nhttps://sws.geonames.org/3183443/\n40.92995\n9.06517\nPOINT (9.06517 40.92995)\nRome\nhttp://nomisma.org/id/rome\n10\n41.9\n12.5\nPOINT (12.5 41.9)\n306.37972\n\n\nAGN\nAgnona (Italy)\nhttps://sws.geonames.org/6693917/\n45.72602\n8.25957\nPOINT (8.25957 45.72602)\nRome\nhttp://nomisma.org/id/rome\n244\n41.9\n12.5\nPOINT (12.5 41.9)\n545.19648\n\n\nAID\nAid√≥na (Greece)\nhttps://sws.geonames.org/265542/\n39.60542\n21.46797\nPOINT (21.46797 39.60542)\nRome\nhttp://nomisma.org/id/rome\n4\n41.9\n12.5\nPOINT (12.5 41.9)\n797.75946\n\n\n\n\n\nThis is limited only to hoards with coins from the Rome mint, and the distance is crudely calculated as the great circle distance. We expect to use ORBIS to more accurately model transportation networks and distances.\nThere is more granular data that includes Terminus Ante Quem (closing date), specific coin types and authorities, and dates. This has not yet been combined into the data set.\nIt is expected that there would be some spatial autocorrelation of the data since there is the expectation that coins would not be distributed along whatever routes they took to their final destination in the hoard."
  },
  {
    "objectID": "coins01.html#mints",
    "href": "coins01.html#mints",
    "title": "Project - Brainstorming",
    "section": "Mints",
    "text": "Mints"
  },
  {
    "objectID": "coins01.html#all-hoard-findspots",
    "href": "coins01.html#all-hoard-findspots",
    "title": "Project - Brainstorming",
    "section": "All Hoard Findspots",
    "text": "All Hoard Findspots"
  },
  {
    "objectID": "coins01.html#spatial-autocorrelation",
    "href": "coins01.html#spatial-autocorrelation",
    "title": "Project - Brainstorming",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\n\n\nRows: 926 Columns: 13\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (7): hoard_id, hoard_name, hoard_uri, hoard_geometry, mint_name, mint_ur...\ndbl (6): hoard_lat, hoard_lon, mint_average_count, mint_lat, mint_lon, mint_...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nData Quality Summary:\n\n\nTotal hoards: 486 \n\n\nPresence distribution:\n\n\n\n  0   1 \n  3 483 \n\n\nMissing coordinates: 0 \n\n\nProportion with Rome mint presence: 0.994 \n\n\n\nTesting multiple distance thresholds:\n\n\nWarning in dnearneigh(coords, d1 = 0, d2 = dist): neighbour object has 84\nsub-graphs\n\n\nWarning in dnearneigh(coords, d1 = 0, d2 = dist): neighbour object has 49\nsub-graphs\n\n\nWarning in dnearneigh(coords, d1 = 0, d2 = dist): neighbour object has 28\nsub-graphs\n\n\nWarning in dnearneigh(coords, d1 = 0, d2 = dist): neighbour object has 16\nsub-graphs\n\n\nWarning in dnearneigh(coords, d1 = 0, d2 = dist): neighbour object has 13\nsub-graphs\n\n\n  distance_km moran_i p_value islands avg_neighbors\n1         100      NA      NA      58            NA\n2         150      NA      NA      29            NA\n3         200      NA      NA      15            NA\n4         250      NA      NA      11            NA\n5         300      NA      NA      10            NA\n\n\nWarning in dnearneigh(coords, d1 = 0, d2 = optimal_distance): neighbour object\nhas 28 sub-graphs\n\n\n\nSpatial neighborhood summary:\n\n\nDistance threshold: 200 km\n\n\nNumber of points with no neighbors: 15 \n\n\nAverage number of neighbors: 23.66 \n\n\nRange of neighbors: 0 - 56 \n\n\n\nGlobal Moran's I Results:\n\n\nRow-standardized weights:\n\n\n\n    Moran I test under randomisation\n\ndata:  rome_proj$presence  \nweights: lw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 0.83075, p-value = 0.2031\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.0123304614     -0.0021276596      0.0003028863 \n\n\n\nBinary weights:\n\n\n\n    Moran I test under randomisation\n\ndata:  rome_proj$presence  \nweights: lw_binary  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.5754, p-value = 0.05758\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.0141599037     -0.0021276596      0.0001068888 \n\n\n\nComputing Local Moran's I...\n\n\n\nCluster Summary (original p-values):\n\n\n\n       High-Low         Low-Low Not significant \n             33               2             451 \n\n\n\nCluster Summary (FDR-adjusted p-values):\n\n\n\n       High-Low Not significant \n             20             466 \n\n\n\nSignificance levels:\n\n\n\nNot significant        p &lt; 0.01        p &lt; 0.05 \n            451              23              12 \n\n\n‚Ñπ tmap modes \"plot\" - \"view\"\n‚Ñπ toggle with `tmap::ttm()`\n‚Ñπ tmap modes \"plot\" - \"view\"\n\n\nWarning: tm_scale_intervals `label.style = \"continuous\"` implementation in view mode\nwork in progress\n\n\n\n\n\n\n\nMultiple palettes called \"gray\" found: \"matplotlib.gray\", \"tableau.gray\", \"ocean.gray\", \"gmt.gray\". The first one, \"matplotlib.gray\", is returned.\nMultiple palettes called \"gray\" found: \"matplotlib.gray\", \"tableau.gray\", \"ocean.gray\", \"gmt.gray\". The first one, \"matplotlib.gray\", is returned.\n\n\nWarning: tm_scale_intervals `label.style = \"continuous\"` implementation in view mode\nwork in progress\n\n\n\n\n\n\n\n\n\n\nAlternative analysis with k-nearest neighbors:\n\n\nWarning in knearneigh(coords, k = k): knearneigh: identical points found\n\n\nWarning in knearneigh(coords, k = k): knearneigh: kd_tree not available for\nidentical points\n\n\nWarning in knearneigh(coords, k = k): knearneigh: identical points found\n\n\nWarning in knearneigh(coords, k = k): knearneigh: kd_tree not available for\nidentical points\n\n\nWarning in knearneigh(coords, k = k): knearneigh: identical points found\n\n\nWarning in knearneigh(coords, k = k): knearneigh: kd_tree not available for\nidentical points\n\n\nWarning in knearneigh(coords, k = k): knearneigh: identical points found\n\n\nWarning in knearneigh(coords, k = k): knearneigh: kd_tree not available for\nidentical points\n\n\n                    k      moran_i   p_value\nMoran I statistic   6 -0.007936508 0.6165404\nMoran I statistic1  8 -0.009316770 0.6642648\nMoran I statistic2 10 -0.008695652 0.6677864\nMoran I statistic3 12 -0.008626639 0.6811293\n\n\n\nDiagnostics:\n\n\nDistribution of presence variable:\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  1.0000  1.0000  0.9938  1.0000  1.0000 \n\n\n\nLocal Moran's I statistics summary:\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-1.000000  0.006211  0.006211  0.012330  0.006211  4.225807 \n\n\n\nSignificant clusters by type:\n\n\n\nHigh-Low  Low-Low \n      33        2"
  },
  {
    "objectID": "coins01.html#kriging-probability-surface",
    "href": "coins01.html#kriging-probability-surface",
    "title": "Project - Brainstorming",
    "section": "Kriging / probability surface",
    "text": "Kriging / probability surface\nThis is not functional. I have developed some code but it does not work well.\nI am trying to use indicator Kriging, but I am running into trouble excuting the analysis. I only have places that have coins, and a few that don‚Äôt have coins from the Rome mint. I don‚Äôt think I have enough data."
  },
  {
    "objectID": "coins01.html#distance-decay",
    "href": "coins01.html#distance-decay",
    "title": "Project - Brainstorming",
    "section": "Distance Decay",
    "text": "Distance Decay\n\n# Roman Coin Probability Surface Analysis with Hex Binning\n# Load required libraries\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\n# Try to load optional basemap packages\nbasemap_packages &lt;- c(\"rnaturalearth\", \"rnaturalearthdata\", \"maps\")\navailable_packages &lt;- sapply(basemap_packages, function(pkg) {\n  suppressWarnings(require(pkg, character.only = TRUE, quietly = TRUE))\n})\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:viridis':\n\n    unemp\n\nif(!any(available_packages)) {\n  warning(\"No basemap packages available. Install 'rnaturalearth' or 'maps' for geographic context.\")\n}\n\n# Resolve potential conflicts\nselect &lt;- dplyr::select\nfilter &lt;- dplyr::filter\nmutate &lt;- dplyr::mutate\n\n# Read and prepare data\ndata &lt;- readr::read_csv(\"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/Roman Italy GIS/Coin Project/chrr_hoards/hoard_geojson/combined_hoards_with_distance.csv\")\n\nRows: 926 Columns: 13\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (7): hoard_id, hoard_name, hoard_uri, hoard_geometry, mint_name, mint_ur...\ndbl (6): hoard_lat, hoard_lon, mint_average_count, mint_lat, mint_lon, mint_...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Focus on Rome mint only\nrome_data &lt;- data %&gt;%\n  dplyr::filter(mint_name == \"Rome\") %&gt;%\n  dplyr::select(hoard_id, hoard_name, hoard_lat, hoard_lon, \n                mint_average_count, mint_hoard_distance_km)\n\n# Convert findspots to spatial points\nfindspots_sf &lt;- st_as_sf(rome_data, \n                         coords = c(\"hoard_lon\", \"hoard_lat\"), \n                         crs = 4326)\n\n# Create hexagonal grid function\ncreate_hex_grid &lt;- function(bbox, hex_size_km = 75) {\n  # Convert to projected CRS for equal area hexagons (Europe-focused)\n  bbox_proj &lt;- st_transform(st_as_sfc(bbox), crs = 3035)\n  \n  # Create hex grid\n  hex_grid &lt;- st_make_grid(bbox_proj, \n                           cellsize = hex_size_km * 1000, # convert to meters\n                           square = FALSE) # hexagonal\n  \n  # Convert back to WGS84 and add IDs\n  hex_grid &lt;- st_sf(hex_id = 1:length(hex_grid), \n                    geometry = st_transform(hex_grid, crs = 4326))\n  return(hex_grid)\n}\n\n# Create study area bounding box (expand around findspots)\nbbox &lt;- st_bbox(findspots_sf)\nbbox_expanded &lt;- bbox + c(-2, -2, 2, 2)  # expand by 2 degrees\nstudy_area &lt;- st_as_sfc(st_bbox(bbox_expanded, crs = st_crs(findspots_sf)))\n\n# Create hex grid\nhex_grid &lt;- create_hex_grid(bbox_expanded, hex_size_km = 75)\n\n# Spatial join: assign each findspot to a hex\nfindspots_hex &lt;- st_join(findspots_sf, hex_grid)\n\n# Aggregate data by hex\nhex_summary &lt;- findspots_hex %&gt;%\n  sf::st_drop_geometry() %&gt;%\n  dplyr::group_by(hex_id) %&gt;%\n  dplyr::summarise(\n    total_coins = sum(mint_average_count, na.rm = TRUE),\n    n_findspots = n(),\n    avg_distance_to_rome = mean(mint_hoard_distance_km, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# Join aggregated data back to hex grid\nhex_grid &lt;- hex_grid %&gt;%\n  dplyr::left_join(hex_summary, by = \"hex_id\") %&gt;%\n  dplyr::mutate(\n    total_coins = ifelse(is.na(total_coins), 0, total_coins),\n    has_findspots = !is.na(n_findspots)\n  )\n\n# Calculate distance from each hex centroid to Rome\nrome_coords &lt;- st_sfc(st_point(c(12.5, 41.9)), crs = 4326)\n\nhex_grid &lt;- hex_grid %&gt;%\n  dplyr::mutate(\n    dist_to_rome = as.numeric(sf::st_distance(sf::st_centroid(.), rome_coords)) / 1000\n  )\n\nWarning: There was 1 warning in `stopifnot()`.\n‚Ñπ In argument: `dist_to_rome = as.numeric(sf::st_distance(sf::st_centroid(.),\n  rome_coords))/1000`.\nCaused by warning:\n! st_centroid assumes attributes are constant over geometries\n\n# Fit distance decay model using hexes with actual data\nmodel_data &lt;- hex_grid %&gt;%\n  dplyr::filter(has_findspots == TRUE & total_coins &gt; 0) %&gt;%\n  sf::st_drop_geometry()\n\n# Check if we have enough data for modeling\nif(nrow(model_data) &lt; 3) {\n  stop(\"Not enough data points for modeling. Need at least 3 hexes with coin data.\")\n}\n\n# Fit exponential decay model\n# Try different starting parameters if the first attempt fails\ntryCatch({\n  decay_model &lt;- nls(total_coins ~ a * exp(-b * dist_to_rome), \n                     data = model_data,\n                     start = list(a = max(model_data$total_coins), b = 0.001))\n}, error = function(e) {\n  # Alternative: simple linear model on log scale\n  decay_model &lt;- lm(log(total_coins + 1) ~ dist_to_rome, data = model_data)\n})\n\n# Predict for all hexes\nif(class(decay_model)[1] == \"nls\") {\n  hex_grid$predicted_coins &lt;- predict(decay_model, \n                                      newdata = data.frame(dist_to_rome = hex_grid$dist_to_rome))\n} else {\n  # For linear model, back-transform predictions\n  hex_grid$predicted_coins &lt;- exp(predict(decay_model, \n                                          newdata = data.frame(dist_to_rome = hex_grid$dist_to_rome))) - 1\n}\n\n# Create probability surface (normalize to 0-1)\nhex_grid$probability &lt;- pmax(0, hex_grid$predicted_coins) / max(pmax(0, hex_grid$predicted_coins))\n\n# Combine observed and predicted values\nhex_grid$final_coins &lt;- ifelse(hex_grid$has_findspots, \n                               hex_grid$total_coins, \n                               hex_grid$predicted_coins)\n\nhex_grid$final_probability &lt;- hex_grid$final_coins / max(hex_grid$final_coins)\n\n# Get basemap data (with error handling)\nworld &lt;- NULL\nbasemap_available &lt;- FALSE\n\n# Try Natural Earth first\nif(available_packages[\"rnaturalearth\"]) {\n  tryCatch({\n    world &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\")\n    basemap_available &lt;- TRUE\n  }, error = function(e) {\n    message(\"Natural Earth data not available, trying maps package...\")\n  })\n}\n\n# Try maps package as fallback\nif(!basemap_available && available_packages[\"maps\"]) {\n  tryCatch({\n    world_map &lt;- maps::map(\"world\", fill = TRUE, plot = FALSE)\n    world &lt;- sf::st_as_sf(world_map)\n    basemap_available &lt;- TRUE\n  }, error = function(e) {\n    message(\"Maps package not working, proceeding without basemap...\")\n  })\n}\n\n# Create study area boundary for clipping\nstudy_bbox &lt;- sf::st_bbox(hex_grid)\nstudy_area_poly &lt;- sf::st_as_sfc(study_bbox)\n\n# Create visualization with optional basemap\nif(basemap_available) {\n  p1 &lt;- ggplot() +\n    # Add basemap first\n    geom_sf(data = world, fill = \"grey90\", color = \"grey95\", size = 0.2) +\n    # Add hex grid\n    geom_sf(data = hex_grid, aes(fill = final_probability), \n            color = \"white\", size = 0.1, alpha = 0.8) +\n    scale_fill_viridis_c(name = \"Coin\\nProbability\", \n                         trans = \"sqrt\",\n                         option = \"turbo\",\n                         labels = scales::percent) +\n    # Add findspot points\n    geom_sf(data = findspots_sf, size = 1, alpha = 0.9, color = \"white\") +\n    # Add Rome\n    geom_sf(data = rome_coords, size = 4, color = \"red\", shape = 18) +\n    # Set map extent to study area\n    coord_sf(xlim = c(study_bbox[\"xmin\"], study_bbox[\"xmax\"]),\n             ylim = c(study_bbox[\"ymin\"], study_bbox[\"ymax\"]),\n             expand = FALSE) +\n    theme_void() +\n    theme(\n      legend.position = \"right\",\n      plot.title = element_text(size = 14, hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5),\n      panel.background = element_rect(fill = \"lightblue\", color = NA)\n    ) +\n    labs(title = \"Probability Surface for Roman Coins\",\n         subtitle = \"Hexagonal binning with distance decay model from Rome\")\n} else {\n  # Fallback map without basemap\n  p1 &lt;- ggplot(hex_grid) +\n    geom_sf(aes(fill = final_probability), color = \"white\", size = 0.1) +\n    scale_fill_viridis_c(name = \"Coin\\nProbability\", \n                         trans = \"sqrt\",\n                         option = \"turbo\",\n                         labels = scales::percent) +\n    geom_sf(data = findspots_sf, size = 1, alpha = 0.8, color = \"white\") +\n    geom_sf(data = rome_coords, size = 4, color = \"red\", shape = 18) +\n    theme_void() +\n    theme(\n      legend.position = \"right\",\n      plot.title = element_text(size = 14, hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5)\n    ) +\n    labs(title = \"Probability Surface for Roman Coins\",\n         subtitle = \"Hexagonal binning with distance decay model from Rome\",\n         caption = \"Note: Install 'rnaturalearth' or 'maps' package for geographic context\")\n}\n\nprint(p1)\n\n\n\n\n\n\n\n# Create enhanced version if basemap is available\nif(basemap_available) {\n  p1_labeled &lt;- ggplot() +\n    # Add basemap\n    geom_sf(data = world, fill = \"grey90\", color = \"grey95\", size = 0.2) +\n    # Add hex grid\n    geom_sf(data = hex_grid, aes(fill = final_probability), \n            color = \"white\", size = 0.1, alpha = 0.8) +\n    scale_fill_viridis_c(name = \"Coin\\nProbability\", \n                         trans = \"sqrt\",\n                         option = \"plasma\",\n                         labels = scales::percent) +\n    # Add findspot points\n    geom_sf(data = findspots_sf, size = 1.2, alpha = 0.9, color = \"white\") +\n    # Add Rome with label\n    geom_sf(data = rome_coords, size = 4, color = \"red\", shape = 18) +\n    annotate(\"text\", x = 12.5, y = 41.2, label = \"Rome\", \n             color = \"red\", fontface = \"bold\", size = 3) +\n    # Set map extent\n    coord_sf(xlim = c(study_bbox[\"xmin\"], study_bbox[\"xmax\"]),\n             ylim = c(study_bbox[\"ymin\"], study_bbox[\"ymax\"]),\n             expand = FALSE) +\n    theme_void() +\n    theme(\n      legend.position = \"right\",\n      plot.title = element_text(size = 14, hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5),\n      panel.background = element_rect(fill = \"lightblue\", color = NA)\n    ) +\n    labs(title = \"Roman Coin Distribution Probability\",\n         subtitle = \"Based on distance decay modeling from Rome\",\n         caption = \"White dots = Coin findspots | Red diamond = Rome\")\n  \n  print(p1_labeled)\n}\n\n\n\n\n\n\n\n# Summary statistics\ncat(\"Analysis Summary:\\n\")\n\nAnalysis Summary:\n\ncat(\"=================\\n\")\n\n=================\n\ncat(\"Total findspots:\", nrow(findspots_sf), \"\\n\")\n\nTotal findspots: 486 \n\ncat(\"Total coins from Rome:\", sum(rome_data$mint_average_count), \"\\n\")\n\nTotal coins from Rome: NA \n\ncat(\"Hexes with data:\", sum(hex_grid$has_findspots, na.rm = TRUE), \"\\n\")\n\nHexes with data: 229 \n\ncat(\"Total hexes:\", nrow(hex_grid), \"\\n\")\n\nTotal hexes: 5000 \n\ncat(\"Max distance to Rome:\", round(max(hex_grid$dist_to_rome), 2), \"km\\n\")\n\nMax distance to Rome: 4372.11 km\n\ncat(\"Min distance to Rome:\", round(min(hex_grid$dist_to_rome), 2), \"km\\n\")\n\nMin distance to Rome: 17.74 km\n\n# Model diagnostics\nif(class(decay_model)[1] == \"nls\") {\n  cat(\"\\nDistance Decay Model (Exponential):\\n\")\n  cat(\"===================================\\n\")\n  print(summary(decay_model))\n} else {\n  cat(\"\\nDistance Decay Model (Linear on log scale):\\n\")\n  cat(\"==========================================\\n\")\n  print(summary(decay_model))\n}\n\n\nDistance Decay Model (Exponential):\n===================================\n\nFormula: total_coins ~ a * exp(-b * dist_to_rome)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \na 2.013e+03  3.304e+02   6.092 4.72e-09 ***\nb 2.583e-03  4.720e-04   5.473 1.17e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 814.1 on 227 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 9.615e-06\n\n# Create distance vs coins plot\np2 &lt;- ggplot(model_data, aes(x = dist_to_rome, y = total_coins)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"red\") +\n  scale_y_log10() +\n  labs(title = \"Distance Decay Relationship\",\n       x = \"Distance to Rome (km)\",\n       y = \"Total Coins (log scale)\") +\n  theme_minimal()\n\nprint(p2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Export results (optional)\n# hex_results &lt;- hex_grid %&gt;%\n#   dplyr::select(hex_id, dist_to_rome, total_coins, predicted_coins, \n#          final_probability, has_findspots) %&gt;%\n#   sf::st_drop_geometry()\n# \n# readr::write_csv(hex_results, \"roman_coin_probability_results.csv\")"
  },
  {
    "objectID": "prepare02.html",
    "href": "prepare02.html",
    "title": "Prepare for Class 02",
    "section": "",
    "text": "NY (2024): 1, 2 (\\(\\cdot\\)), PM (\\(\\cdot\\cdot\\cdot\\))\nNote: \\(\\cdot\\) technical difficulty low; \\(\\cdot\\cdot\\cdot\\) technical difficulty high\n\nCompleted"
  },
  {
    "objectID": "prepare02.html#inge-druckrey",
    "href": "prepare02.html#inge-druckrey",
    "title": "Prepare for Class 02",
    "section": "2.1 Inge Druckrey",
    "text": "2.1 Inge Druckrey\nTeaching to see by Andrei Severny and Edward Tufte (~ 37 minutes)\n\nCompleted\n\n\n2.1.1 Write a one-page review:\n\nLearn thin/thick and curve/linearity from Druckrey\nHow did Holmes describe and contrast the eye and the hand in writing?\nWhy calligraphy is important to Steve Jobs and the design of Mac computers?\nDifferentiate geometric accuracy and optical accuracy\n\nReview: Teaching to See ‚Äì Lessons from Inge Druckrey and Beyond\nIn the documentary Teaching to See, we learned about Inge Druckrey, a design teacher who changed how students look at the world around them. Her teaching method was strict but eye-opening. One of her biggest lessons was about understanding contrasts, the use of negative and positive. Contrasts between thin and thick lines, between curved and straight shapes, and different textures were prominent in her teaching. When students practiced brush lettering and Roman capital letters, they learned how these visual elements work together to create rhythm and balance. Making an ‚ÄòR‚Äô or a ‚ÄòS‚Äô wasn‚Äôt just about making something pretty or functional, it was about mastering the basic contrasts that make letters work and communicate clearly. She pushed students to make small improvements to enhance their work. Make a curve deeper. Adjust a line thickness. She taught them to see the shape, not merely copy it.\nOne student, Holmes, shared an interesting idea about the eye versus the hand in writing. The eye is conservative and wants everything to look neat, orderly, and easy to read. However, the hand is radical, and wants to write fast and show personality. This tension between what the eye wants and what the hand does is the story of handwriting and fonts. The eye demands that we can read something easily, while the hand wants to add personal style. When these two contrasting ideas work together, the world of typography and calligraphy opens up. Holmes‚Äôs comparison captured the main challenge of typography. Follow the rules, but do it expressively and with purpose.\nThis balanced contrast had a huge impact on Steve Jobs. Jobs, known for his design sense, was thoughtfully affected by her tutelage. Jobs said that this class, taken when he was not even a student at Reed, shaped his approach to design. Many years later as he was envisioning the successor to the Apple II, he incorporated advanced computer typography to the interface. The first Macintosh computer included properly spaced fonts and multiple typefaces, which was something completely different from the terminals of the time. Jobs believed that beauty and function should go together, and that computers should have the aesthetic and care found in handwritten letters.\nAn important lessons from her teaching was about the difference between geometric and optical accuracy. Two letters could measure the same size, but they can look different to your eyes. Using a reducing glass, Druckrey would look at student work essentially from far away, spotting visual problems that careful measuring alone couldn‚Äôt find. Students learned that our eyes see weight, proportion, and alignment as a whole, not piece by piece. Design is not about mathematical perfection but about visual truth. She taught students to trust their eyes and keep improving their work until it looked right.\nTeaching to See examined the importance of observing carefully, and working patiently to improve. Students were invited into a lifelong practice of noticing the world around them more clearly and more beautifully."
  },
  {
    "objectID": "prepare02.html#john-glendennings-data-quality-checklist",
    "href": "prepare02.html#john-glendennings-data-quality-checklist",
    "title": "Prepare for Class 02",
    "section": "4.1 John Glendenning‚Äôs Data Quality Checklist",
    "text": "4.1 John Glendenning‚Äôs Data Quality Checklist\nA customized guide for Roman Coinage based on the Quartz Bad Data Guide\n1. Structural Cleanliness\n\nHeaders are correct (no shifted cells, multi-row headers, or misaligned labels)\nEvery column has a consistent type (no mixed datatypes in a single column)\nNo blank rows or columns (especially dangerous in CSVs and spreadsheets)\nNo merged cells (common in XLS/XLSX files‚Äîflatten before use)\nConsistent encoding (UTF-8 preferred for Python/R compatibility)\n\n2. Column-Level Validation\n\nAll dates are parsable and timezone-aware if needed\nString fields are stripped of whitespace (\"Rome \" ‚Üí \"Rome\")\nNo rogue delimiters in text fields (e.g., commas in CSVs without proper quoting)\nNumerical fields are clean (no currency symbols, commas, or locale-specific formatting like 1.000.000)\n\n3. Semantic Accuracy\n\nCheck for mislabeled columns (e.g., latitude/longitude swaps)\nVerify units (e.g., km vs miles, BCE vs CE)\nDecimal vs percentage (e.g., 0.07 vs 7%)\n\n4. Consistency Over Time\n\nHistorical changes tracked (e.g., column schema, CRS, projection systems)\nDecade or time-period tagging consistent (e.g., -90 to -81 BCE)\nAudit trail/logs for any manual cleaning (especially footnote/bibliography removal in PDFs)\n\n5. Missing or Placeholder Data\n\nAll forms of NA/NULL recognized (NA, NaN, null, \"\", -99, etc.)\nMissing values logged by column and count\nDon‚Äôt treat 0 as missing unless contextually appropriate\n\n6. Duplicate Detection\n\nCheck for duplicate rows (especially when scraping or merging)\nCheck for near-duplicates (e.g., Roma, Rome, ROME)\nCheck for duplicate IDs (e.g., coin ID, hoard ID, document IDs)\n\n7. Geospatial Sanity\n\nLatitude between -90 and 90; longitude between -180 and 180\nCRS consistent (e.g., EPSG:3035 for European LAEA)\nNo flipped coordinates (common error: lat/lon reversed)\nHoard and mint points lie on land (not sea) if appropriate\n\n8. Textual Consistency\n\nLigatures and hyphens cleaned (e.g., Ô¨Å ‚Üí fi, eco-\\nnomic ‚Üí economic)\nSpellchecked output, especially OCR text\nEntity redactions logged ([REDACTED_PERSON], etc.)\nBibliographic references stripped only after saving original\n\n9. File-Level Integrity\n\nAvoid Excel-specific features (e.g., macros, pivot tables, styles)\nAll outputs saved in open formats (.csv, .txt, .geojson, .rds)\nFolder names consistent and informative (data/raw/, output/cleaned/)\nFile naming conventions follow: source_var_cleaned_YYYYMMDD.csv\n\n10. Reproducibility and Logging\n\nEvery cleaning step is logged (log_cleaning_YYYY-MM-DD.csv)\nEvery script is version-controlled (e.g., Git, Quarto render logs)\nPipeline can skip existing files\nWarnings and anomalies are caught and reported\n\n11. Statistical Sanity Checks\n\nExpected distribution of values? (e.g., coin counts by year)\nLook for spikes and dips (e.g., all hoards dated -87 BCE?)\nCorrelated fields make sense (e.g., coin start ‚â§ end year)\nGeometric vs optical accuracy assessed for visualizations\n\n12. Common Traps from Quartz Guide\n\nDates as integers in Excel turn into nonsense\nExcel corrupts CSVs with large numbers or 1-1 turning into Jan 1\nInvisible Unicode characters (zero-width space, BOM)\nTrailing commas or missing headers in CSVs\n‚ÄúTotals‚Äù or ‚ÄúNotes‚Äù rows left in"
  },
  {
    "objectID": "prepare02.html#meetings",
    "href": "prepare02.html#meetings",
    "title": "Prepare for Class 02",
    "section": "5.1 Meetings",
    "text": "5.1 Meetings\nSchedule regular meetings to discuss on team project (proposal and presentation due on schedule."
  },
  {
    "objectID": "prepare01.html",
    "href": "prepare01.html",
    "title": "Prepare for Class 01",
    "section": "",
    "text": "Prepare for Class 01\n\nPrepare your own device\n\nComputer on either Windows or MacOS operating system (OS). Tablets are not recommended.\n\n\nCompleted\n\n\nRun the latest update on OS since the computer is your most important companion in this class.\n\n\nCompleted\n\n\nInstall software\n\nProgram/text editor at your choice. The following is only recommended but not required:\n   Visual Studio Code\n\nCompleted - BBEdit, not VSC\n\nR (https://cran.r-project.org)\n   RStudio (https://posit.co/downloads/)\n\nCompleted\n\n\n\n\n=== R VERSION AND PLATFORM ===\n\n\n               _                           \nplatform       aarch64-apple-darwin20      \narch           aarch64                     \nos             darwin20                    \nsystem         aarch64, darwin20           \nstatus                                     \nmajor          4                           \nminor          5.1                         \nyear           2025                        \nmonth          06                          \nday            13                          \nsvn rev        88306                       \nlanguage       R                           \nversion.string R version 4.5.1 (2025-06-13)\nnickname       Great Square Root           \n\n\n\nTeam up\n\nBuild a team of three members\n\nCompleted (Adiline, Elise, John)\n\n\nRead\n\nWilkinson, Leland. 2005. The Grammar of Graphics. Second edition. Springer\n\nCompleted\n\nNext week: Murrell, Paul. 2016. R graphics. CRC Press.\n\nStarted\n\n\nData\n\nIdentify a literature of academic topic and collect data (secondary data will work)\nI am currently working on analyzing Roman coin hoard data in some novel ways. For the initial analysis, data from the Coin Hoards of the Roman Republic was accessed. This dataset is based of the work of Crawford (1969), as enhanced by Gruber and Lockyear.\n\n\nFirst Approach\nThis first approach to the dataset was gathered by downloading data from the web pages.\n\n\n\nFirst 10 Hoards with Rome as Mint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhoard_id\nhoard_name\nhoard_uri\nhoard_lat\nhoard_lon\nhoard_geometry\nmint_name\nmint_uri\nmint_average_count\nmint_lat\nmint_lon\nmint_geometry\nmint_hoard_distance_km\n\n\n\n\n1PO\nPoiana CrƒÉcƒÉoani (Romania)\nhttps://sws.geonames.org/670093/\n47.06667\n26.31667\nPOINT (26.31667 47.06667)\nRome\nhttp://nomisma.org/id/rome\n123\n41.9\n12.5\nPOINT (12.5 41.9)\n1236.48816\n\n\nABE\nAbertura (Spain)\nhttps://sws.geonames.org/2522486/\n39.24352\n-5.81394\nPOINT (-5.81394 39.24352)\nRome\nhttp://nomisma.org/id/rome\n23\n41.9\n12.5\nPOINT (12.5 41.9)\n1573.37081\n\n\nACT\n√Åktion (Greece)\nhttps://sws.geonames.org/265389/\n38.94019\n20.76875\nPOINT (20.76875 38.94019)\nRome\nhttp://nomisma.org/id/rome\n3\n41.9\n12.5\nPOINT (12.5 41.9)\n773.90425\n\n\nADJ\nAmƒÉrƒÉ≈ütii de Jos (Romania)\nhttps://sws.geonames.org/686386/\n43.95000\n24.16667\nPOINT (24.16667 43.95)\nRome\nhttp://nomisma.org/id/rome\n4\n41.9\n12.5\nPOINT (12.5 41.9)\n977.06559\n\n\nADM\nMassa d‚ÄôAlbe (Italy)\nhttps://sws.geonames.org/3173765/\n42.10723\n13.39429\nPOINT (13.39429 42.10723)\nRome\nhttp://nomisma.org/id/rome\n93\n41.9\n12.5\nPOINT (12.5 41.9)\n77.49022\n\n\nADR\nAlcal√° del R√≠o (Spain)\nhttps://sws.geonames.org/2522474/\n37.51780\n-5.98185\nPOINT (-5.98185 37.5178)\nRome\nhttp://nomisma.org/id/rome\n159\n41.9\n12.5\nPOINT (12.5 41.9)\n1652.39657\n\n\nADU\nAlbanchez de M√°gina (Spain)\nhttps://sws.geonames.org/2522239/\n37.79263\n-3.46833\nPOINT (-3.46833 37.79263)\nRome\nhttp://nomisma.org/id/rome\n16\n41.9\n12.5\nPOINT (12.5 41.9)\n1436.74096\n\n\nAGG\nAggius (Italy)\nhttps://sws.geonames.org/3183443/\n40.92995\n9.06517\nPOINT (9.06517 40.92995)\nRome\nhttp://nomisma.org/id/rome\n10\n41.9\n12.5\nPOINT (12.5 41.9)\n306.37972\n\n\nAGN\nAgnona (Italy)\nhttps://sws.geonames.org/6693917/\n45.72602\n8.25957\nPOINT (8.25957 45.72602)\nRome\nhttp://nomisma.org/id/rome\n244\n41.9\n12.5\nPOINT (12.5 41.9)\n545.19648\n\n\nAID\nAid√≥na (Greece)\nhttps://sws.geonames.org/265542/\n39.60542\n21.46797\nPOINT (21.46797 39.60542)\nRome\nhttp://nomisma.org/id/rome\n4\n41.9\n12.5\nPOINT (12.5 41.9)\n797.75946\n\n\n\n\n\nSecond Approach\nThe second method used the following SPARQL query through the Nomisma SPARQL endpoint. The SPARQL code is can be accessed above.\n\n\n\nFirst 10 Coins in Hoards with Rome as Mint, Method 2\n\n\nhoard\nhoardID\ntype\ntypeLabel\nstartYear\nendYear\ndenomination\ndenominationLabel\nmaterial\nmaterialLabel\nmint\nmintID\nhoardLat\nhoardLong\nmintLat\nmintLong\n\n\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-187.1\n‚ÄúRRC 187/1‚Äù@en\n-169\n-158\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-200.1\n‚ÄúRRC 200/1‚Äù@en\n-155\n-155\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-216.1\n‚ÄúRRC 216/1‚Äù@en\n-148\n-148\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-228.2\n‚ÄúRRC 228/2‚Äù@en\n-140\n-140\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-231.1\n‚ÄúRRC 231/1‚Äù@en\n-138\n-138\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-233.1\n‚ÄúRRC 233/1‚Äù@en\n-138\n-138\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-237.1a\n‚ÄúRRC 237/1a‚Äù@en\n-136\n-136\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-244.1\n‚ÄúRRC 244/1‚Äù@en\n-134\n-134\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-245.1\n‚ÄúRRC 245/1‚Äù@en\n-134\n-134\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\nhttp://numismatics.org/chrr/id/1PO\n1PO\nhttp://numismatics.org/crro/id/rrc-250.1\n‚ÄúRRC 250/1‚Äù@en\n-132\n-132\nhttp://nomisma.org/id/denarius\n‚ÄúDenarius‚Äù@en\nhttp://nomisma.org/id/ar\n‚ÄúSilver‚Äù@en\nhttp://nomisma.org/id/rome\nrome\n47.06903\n26.30626\n41.9\n12.5\n\n\n\n\n\n\nCollect charts from studies\n\nSome charts based on the data from the CHRR data.\n      \n\nDiscussion for next week:\n\nWhat is the difference between graphics and:\n\nmusical notes\nGraphics are primarily spatial and visual, representing information through location, shape, color, and other visual relationships. Musical notes, which are visual on paper, are based on auditory waveforms over time. Graphics can be processed visually and interpreted nearly instantly as a whole, while music is interpreted sequentially over time to create meaning.\nverbal language\nGraphics communicate through direct visual representation and spatial relationships, showing rather than telling. Verbal language uses symbolic abstraction. Arbitrary words and grammatical structures which require learning conventions to decode meaning. Graphics can transcend spoken language barriers and can often convey complex spatial or quantitative relationships more efficiently than words. Verbal language excels at expressing abstract concepts, logical arguments, and narrative sequences that would be difficult to represent visually in graphics.\nmathematical notations\nBoth graphics and mathematical notation may represent quantitative relationships, but they do so differently. Graphics present information through visual metaphors and relationships suah as showing data through bar heights or trend lines. Mathematical notation uses precise symbolic logic with specific meanings to represent these quantitative relationships. A graph might show that one value is much different than another, while mathematical notation can specify exactly how much different. Graphics also can create a more emotional connnection to the information than mathematics, which is one way that graphics can be more manipulative than mathematics. Mathematical notation is also more compact and precise for complex calculations, while graphics make patterns and relationships more immediately apparent to human visual perception.\n\nHow many ways messages can be conveyed?\nAny way that humans can perceive is a way that we can convey a message. Some are more efficient than others. Some are task-specific. We can categorize these, roughly:\n\nVisual: text, images, graphics, gestures, facial expressions, sign language, light signals\nAuditory: speech, music, sounds, tones\nTactile: touch, braille, vibrations, textures\nOlfactory: scents, pheromones (not a concious choice but worth noting)\nGustatory: taste (very limited uses, though)\n\nDifferent methods of communication serve different purposes, prioritizing different things such as speed, accuracy, distance, etc.\n\n\n\n\nBibliography\nCrawford, Michael H. 1974. Roman Republican Coinage. 2 vols. Cambridge: Cambridge University Press."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Glendenning",
    "section": "",
    "text": "I am a masters student studying geographical information systems.\n\nContact\njmg220005@utdallas.edu"
  },
  {
    "objectID": "assignment03.html",
    "href": "assignment03.html",
    "title": "Assignment 03",
    "section": "",
    "text": "#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n\n\n\n\n# === Barplot Example with VADeaths dataset ===\n\n# Adjust the margins of the plotting region.\n# Arguments to par(mar=) are c(bottom, left, top, right).\n# Here, margins are set smaller than default to fit the barplot labels nicely.\npar(mar = c(2, 3.1, 2, 2.1)) \n\n# Draw the barplot for the VADeaths dataset.\n# - 'VADeaths' is a built-in R matrix containing mortality rates in Virginia.\n# - 'col' sets the bar colors to a sequence of grays (darker for higher values).\n# - 'names=rep(\"\", 4)' removes default axis labels for the 4 groups.\n# - The function returns midpoints (x-coordinates) of each group of bars,\n#   which are stored in 'midpts' for later use in labeling.\nmidpts &lt;- barplot(\n  VADeaths,\n  col = gray(0.1 + seq(1, 9, 2) / 11),  # generate grayscale shading\n  names = rep(\"\", 4)                     # suppress x-axis names\n)\n\n# Add custom group labels below the bars.\n# - mtext() places text in the margins of the plot.\n# - sub(\" \", \"\\n\", colnames(VADeaths)) replaces spaces with line breaks\n#   in the column names, so labels like \"Rural Male\" become:\n#   \"Rural\\nMale\" (two lines).\n# - 'at=midpts' places each label under the correct bar group.\n# - 'side=1' means place text on the x-axis side (bottom).\n# - 'line=0.5' controls how far below the axis the text is placed.\n# - 'cex=0.5' makes the text half the default size.\nmtext(\n  sub(\" \", \"\\n\", colnames(VADeaths)), \n  at = midpts, side = 1, line = 0.5, cex = 0.5\n)\n\n# Add numerical values inside the bars.\n# - 'rep(midpts, each=5)' gives the x-positions for each of the 5 stacked bars\n#   within each group (since VADeaths has 5 rows).\n# - 'apply(VADeaths, 2, cumsum)' computes cumulative sums for each column,\n#   giving the top edge of each stacked bar.\n# - Subtract 'VADeaths/2' to center the labels vertically within each bar.\n# - 'VADeaths' provides the actual numbers to display as labels.\n# - 'col=rep(c(\"white\",\"black\"), times=3:2)' alternates text color (white/black)\n#   depending on bar shading, so text is visible against background.\n# - 'cex=0.8' scales the text smaller than default.\ntext(\n  rep(midpts, each = 5), \n  apply(VADeaths, 2, cumsum) - VADeaths / 2, \n  VADeaths, \n  col = rep(c(\"white\", \"black\"), times = 3:2), \n  cex = 0.8\n)\n\n\n\n\n\n\n\n# Reset margins to default (bottom=5.1, left=4.1, top=4.1, right=2.1).\n# Always good practice so later plots don‚Äôt inherit modified margins.\npar(mar = c(5.1, 4.1, 4.1, 2.1))"
  },
  {
    "objectID": "assignment03.html#choose-one-of-the-six-charts-and-explain-how-it-is-configured-by-adding-documentation-to-the-codes.",
    "href": "assignment03.html#choose-one-of-the-six-charts-and-explain-how-it-is-configured-by-adding-documentation-to-the-codes.",
    "title": "Assignment 03",
    "section": "",
    "text": "# === Barplot Example with VADeaths dataset ===\n\n# Adjust the margins of the plotting region.\n# Arguments to par(mar=) are c(bottom, left, top, right).\n# Here, margins are set smaller than default to fit the barplot labels nicely.\npar(mar = c(2, 3.1, 2, 2.1)) \n\n# Draw the barplot for the VADeaths dataset.\n# - 'VADeaths' is a built-in R matrix containing mortality rates in Virginia.\n# - 'col' sets the bar colors to a sequence of grays (darker for higher values).\n# - 'names=rep(\"\", 4)' removes default axis labels for the 4 groups.\n# - The function returns midpoints (x-coordinates) of each group of bars,\n#   which are stored in 'midpts' for later use in labeling.\nmidpts &lt;- barplot(\n  VADeaths,\n  col = gray(0.1 + seq(1, 9, 2) / 11),  # generate grayscale shading\n  names = rep(\"\", 4)                     # suppress x-axis names\n)\n\n# Add custom group labels below the bars.\n# - mtext() places text in the margins of the plot.\n# - sub(\" \", \"\\n\", colnames(VADeaths)) replaces spaces with line breaks\n#   in the column names, so labels like \"Rural Male\" become:\n#   \"Rural\\nMale\" (two lines).\n# - 'at=midpts' places each label under the correct bar group.\n# - 'side=1' means place text on the x-axis side (bottom).\n# - 'line=0.5' controls how far below the axis the text is placed.\n# - 'cex=0.5' makes the text half the default size.\nmtext(\n  sub(\" \", \"\\n\", colnames(VADeaths)), \n  at = midpts, side = 1, line = 0.5, cex = 0.5\n)\n\n# Add numerical values inside the bars.\n# - 'rep(midpts, each=5)' gives the x-positions for each of the 5 stacked bars\n#   within each group (since VADeaths has 5 rows).\n# - 'apply(VADeaths, 2, cumsum)' computes cumulative sums for each column,\n#   giving the top edge of each stacked bar.\n# - Subtract 'VADeaths/2' to center the labels vertically within each bar.\n# - 'VADeaths' provides the actual numbers to display as labels.\n# - 'col=rep(c(\"white\",\"black\"), times=3:2)' alternates text color (white/black)\n#   depending on bar shading, so text is visible against background.\n# - 'cex=0.8' scales the text smaller than default.\ntext(\n  rep(midpts, each = 5), \n  apply(VADeaths, 2, cumsum) - VADeaths / 2, \n  VADeaths, \n  col = rep(c(\"white\", \"black\"), times = 3:2), \n  cex = 0.8\n)\n\n\n\n\n\n\n\n# Reset margins to default (bottom=5.1, left=4.1, top=4.1, right=2.1).\n# Always good practice so later plots don‚Äôt inherit modified margins.\npar(mar = c(5.1, 4.1, 4.1, 2.1))"
  },
  {
    "objectID": "assignment03.html#compare-the-regression-models",
    "href": "assignment03.html#compare-the-regression-models",
    "title": "Assignment 03",
    "section": "2.1 Compare the regression models",
    "text": "2.1 Compare the regression models\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nRegression Equation (≈∑)\nCorrelation (r)\nR¬≤\nMean(x)\nVar(x)\nMean(y)\nVar(y)\n\n\n\n\nI\n≈∑ = 3.0 + 0.50x\n0.816\n0.67\n9.0\n11.0\n7.5\n4.127\n\n\nII\n≈∑ = 3.0 + 0.50x\n0.816\n0.67\n9.0\n11.0\n7.5\n4.127\n\n\nIII\n≈∑ = 3.0 + 0.50x\n0.816\n0.67\n9.0\n11.0\n7.5\n4.127\n\n\nIV\n≈∑ = 3.0 + 0.50x\n0.816\n0.67\n9.0\n11.0\n7.5\n4.127\n\n\n\n\n\n\n\n\n\n\nDataset\nNotes from Plot\n\n\n\n\nI\nA classical linear relationship. The points fall close to the regression line with moderate scatter; regression analysis is appropriate here.\n\n\nII\nThe data follow a curved, but nonlinear relationship. A regression line misrepresents the pattern. This set highlights how correlation can mask nonlinear structure.\n\n\nIII\nMost points are flat with no clear trend, except for one high-leverage outlier. That single point forces the slope, giving a misleading regression fit. An example of outlier influence.\n\n\nIV\nNearly all points align vertically at x = 8. A single extreme outlier creates the correlation and slope. Shows how one influential point can dominate results.\n\n\n\nDespite identical summary statistics (same means, variances, regression line, correlation, and R^2), the visual story is completely different. Anscombe designed these datasets to remind us that ‚Äúlook at the data‚Äù is as important as statistical summaries."
  },
  {
    "objectID": "assignment03.html#compare-different-ways-to-create-the-plots-e.g.-changing-colors-line-types-plot-characters",
    "href": "assignment03.html#compare-different-ways-to-create-the-plots-e.g.-changing-colors-line-types-plot-characters",
    "title": "Assignment 03",
    "section": "2.2 Compare different ways to create the plots (e.g.¬†changing colors, line types, plot characters)",
    "text": "2.2 Compare different ways to create the plots (e.g.¬†changing colors, line types, plot characters)\n\n## Anscombe's Quartet Comparative Plots\ndata(anscombe)\n\n# Set up regression models\nmods &lt;- setNames(vector(\"list\", 4), paste0(\"lm\", 1:4))\nff &lt;- y ~ x\nfor (i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  mods[[i]] &lt;- lm(ff, data = anscombe)\n}\n\n# === Comparative Plotting Styles ===\n# Now line color (linecol) is independent of point color (col)\nstyles &lt;- list(\n  list(col=\"red\",   pch=16, bg=\"pink\",      cex=3.0, lty=1, lwd=0.5, linecol=\"black\"),\n  list(col=\"blue\",  pch=17, bg=\"lightblue\", cex=1.4, lty=2, lwd=2, linecol=\"darkorange\"),\n  list(col=\"darkgreen\", pch=15, bg=\"lightgreen\", cex=1.6, lty=3, lwd=3, linecol=\"brown\"),\n  list(col=\"purple\",pch=18, bg=\"lavender\",  cex=1.8, lty=4, lwd=10, linecol=\"darkgray\")\n)\n\n# Plot grid\nop &lt;- par(mfrow=c(2,2), mar=0.1+c(4,4,1,1), oma=c(0,0,2,0))\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  style &lt;- styles[[i]]\n  plot(ff, data=anscombe, \n       col=style$col, pch=style$pch, bg=style$bg, cex=style$cex,\n       xlim=c(3,19), ylim=c(3,13),\n       main=paste(\"Dataset\", i))\n  abline(mods[[i]], col=style$linecol, lty=style$lty, lwd=style$lwd)\n}\nmtext(\"Anscombe's Quartet - Style Options\", outer=TRUE, cex=1.5)\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment03.html#team-work-replicate-the-scatterplot-matrix-below-hint-acquire-data-using-the-following-codes",
    "href": "assignment03.html#team-work-replicate-the-scatterplot-matrix-below-hint-acquire-data-using-the-following-codes",
    "title": "Assignment 03",
    "section": "5.1 Team work: Replicate the Scatterplot matrix below (hint: Acquire data using the following codes)",
    "text": "5.1 Team work: Replicate the Scatterplot matrix below (hint: Acquire data using the following codes)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggrepel)\n\n# --- Load OWID data ---\nowidall &lt;- read.csv(\"https://catalog.ourworldindata.org/garden/covid/latest/compact/compact.csv\")\n\n# --- Filter to Europe (remove OWID aggregates) ---\nowideu &lt;- owidall %&gt;%\n filter(!grepl(\"^OWID\", code), continent == \"Europe\") %&gt;%\n mutate(date = as.Date(date)) %&gt;%\n filter(!is.na(new_deaths_smoothed))\n\n# --- Select the single peak per country ---\nlabel_countries &lt;- c(\"Spain\", \"Germany\", \"Ukraine\", \"Italy\")\nlabel_points &lt;- owideu %&gt;%\n filter(country %in% label_countries) %&gt;%\n group_by(country) %&gt;%\n slice_max(order_by = new_deaths_smoothed, n = 1, with_ties = FALSE) %&gt;%\n ungroup()\n\n# --- Plot ---\nggplot(owideu, aes(x = date, y = new_deaths)) +\n geom_point(shape = 16, size = 1.2, alpha = 0.7, color = \"#e91e63\") +\n geom_text_repel(\n   data = label_points,\n   aes(label = country),\n   size = 3,\n   family = \"serif\",\n   color = \"black\",\n   box.padding = 0.2,\n   point.padding = 0.15,\n   min.segment.length = 0\n ) +\n scale_x_date(date_breaks = \"4 months\", date_labels = \"%Y-%m\") +\n labs(x = \"Date\", y = \"COVID Deaths in Europe (Daily)\") +\n theme_classic(base_family = \"serif\") +\n theme(\n   axis.text.x = element_text(angle = 60, hjust = 1, size = 9),\n   axis.text.y = element_text(angle = 90, vjust = 0.5, size = 9),\n   axis.title  = element_text(size = 14),\n   legend.position = \"none\"\n )\n\n\n\n\n\n\n\n\n# Send the codes to the TA. The first team delivering the code and chart will win a prize (by time stamp and product)\n\n# Download COVID data from OWID GitHub\nowidall = read.csv(\"https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv?raw=true\")\n\n# Deselect cases/rows with OWID\nowidall = owidall[!grepl(\"^OWID\", owidall$iso_code), ]\n\n# Subset by continent: Europe\nowideu = subset(owidall, continent==\"Europe\")"
  },
  {
    "objectID": "Work.html",
    "href": "Work.html",
    "title": "Work",
    "section": "",
    "text": "Work samples will go here.\n\n\n\nPrincess Donut"
  },
  {
    "objectID": "posts/2025-08-18-First-Post/index.html",
    "href": "posts/2025-08-18-First-Post/index.html",
    "title": "First Post",
    "section": "",
    "text": "Maybe I should put the something useful here."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Example Blog",
    "section": "",
    "text": "Article Review\n\n\n\n\n\n\narticle\n\n\nreview\n\n\n\n\n\n\n\n\n\nOct 4, 2025\n\n\nJohn Glendenning\n\n\n\n\n\n\n\n\n\n\n\n\nSilk Road Cities\n\n\n\n\n\n\nsilk\n\n\nroad\n\n\nmap\n\n\n\n\n\n\n\n\n\nSep 26, 2025\n\n\nJohn Glendenning\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 20, 2025\n\n\nJohn Glendenning\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Post\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\n\nLet‚Äôs see if this works.\n\n\n\n\n\nAug 18, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2025-10-04-Articles/index.html",
    "href": "posts/2025-10-04-Articles/index.html",
    "title": "Article Review",
    "section": "",
    "text": "flowchart TD\n\nA[\"Define Question & Scope\"] --&gt; B[\"Initial Search Strategy\"]\nB --&gt; C[\"Selection Criteria\"]\nC --&gt; D[\"Screening\"]\n\n%% Screening branch\nD -- Keep --&gt; E[\"References\"]\nD -- Hold --&gt; X[\"Hold Pile\"]\n\nE --&gt; F[\"Citation Analysis\"]\nF --&gt; H[\"Quality Assessment\"]\n\n%% Quality branch\nH -- Keep --&gt; I[\"Read & Analyze\"]\nH -- Hold --&gt; X\n\n%% Loop back\nE -- Loop back --&gt; D\n\n\n\n\n\n\n\nThis is my standard routine for doing a literature search, which has been used in the past. This review for methodologies, not for studies like health outcomes, some analysis is skipped.\n\nDefine the question and limit the scope In this case, How has this thing been used in this area for this function?\nInitial search strategy, web & primarily library\n\nKeywords\nVary keyword combinations\nDownload files\n\nSelection criteria\n\nInclude: Reviewed articles, journal articles, primarily English, biased to social sciences, book chapters\nExclude: Opinions, reviews, editorials, dupes, anything not empirical\n\nScreening\nAfter downloading screen the documents based on\n\nTitle\nAbstract\nConclusion\n\nAppropriate articles moved into screened pile. Put into keep or hold piles.\nReferences\nQuickly review the references of the keep documents based on:\n\nRepeated references\nInteresting titles\nKnown authors\n\nAdd to download pile and loop back to screening\nCites\nCreate a bibiliography of screened files. Can be run through an LLM to review the number of citations for the kept documents. More important for documents that are older. Useless for recently released publications.\nQuality\n\nSkim document to assess quality and applicability\nAssess group trends\nIdentify gaps\nRebin the articles into keep or hold\n\nRead\nRead the selected articles from the keep pile."
  },
  {
    "objectID": "assignment02.html",
    "href": "assignment02.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Run Paul Murrell‚Äôs RGraphics basic R programs (murrell01.R)\n\nBe sure to run line by line and note the changes\nPay attention to the comments and address the question if there is one\nPlotting functions (note: exercise using the happy planet data set http://happyplanetindex.org)\n\n\n\nOriginal Code with questions addressed\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=16)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\nChange pch\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=5)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n# Exercise: Can you generate these charts individually?  Try these functions \n# using another dataset. Be sure to work on the layout and margins\n\n\nExercise\n\nGenerate these charts individually All, with HPI dataset\nUse a different dataset All\nOptimize the layout an margins All\nTry different cex value? All\nDifferent background color exercise04, bar charts\nIn axis(1, at=seq(0, 16, 4)) What is the first number standing for? This the side of the plot that the axis is drawn. \n\n\n\nHPI_data &lt;- read.csv(\"HPI_data.csv\")\n\npar(mar = c(4, 4, 2, 2), cex = 1)\n\n# Remove rows with NA in GDP or Life Expectancy\nplot_data &lt;- na.omit(HPI_data[, c(\"GDP_capita\", \"Life_exp\")])\n\nx &lt;- plot_data$GDP_capita\ny &lt;- plot_data$Life_exp\n\nplot(x, y, pch = 21, bg = \"lightblue\", col = \"blue\",\n     main = \"Life Expectancy vs GDP per Capita\")\nlines(lowess(x, y, f = 2/3), col = \"red\", lwd = 2)\npoints(x, y, pch = 21, bg = \"lightblue\", cex = 1.2)\n\n\n\n\n\n\n\n\n\npar(mar = c(4, 4, 2, 2), cex = 0.9)\n\nhist(HPI_data$Carbon, col = \"gray80\", main = \"Carbon Footprint per Capita\", freq = FALSE)\n\nlines(density(HPI_data$Carbon, na.rm = TRUE), col = \"blue\", lwd = 2)\n\naxis(1); axis(2); box()\ntext(x = max(HPI_data$Carbon, na.rm = TRUE)*0.8, y = 0.5, labels = \"Density Curve\", col = \"blue\", cex = 0.9)\n\n\n\n\n\n\n\n\n\nop &lt;- par(mar = c(4, 4, 4.5, 2), cex = 0.9)\n\nboxplot(HPI ~ as.factor(Continent), data = HPI_data,\n        col = \"orange\",\n        main = \"HPI by Continent\",\n        ylab = \"HPI\", xlab = \"Continent\")\n\nmtext(\"Happy Planet Index by Region\", side = 3, line = 0.5, cex = 0.7)\n\n\n\n\n\n\n\npar(op)\n\n\nop &lt;- par(mar = c(5, 4, 4, 2) + 0.1, bg = \"gray95\")\n\n# Aggregate HPI by Continent\nmean_hpi &lt;- tapply(HPI_data$HPI, HPI_data$Continent, mean, na.rm = TRUE)\n\n# Barplot of mean HPI by continent\nbarplot(mean_hpi,\n        col = \"steelblue\",\n        main = \"Mean HPI by Continent\",\n        ylab = \"Happy Planet Index (HPI)\",\n        xlab = \"Continent\",\n        names.arg = paste(\"Cont\", names(mean_hpi)))\n\n\n\n\n\n\n\npar(op)\n\n\nop &lt;- par(mar = c(2, 2, 2, 2), cex = 0.8)\n\npie_data &lt;- aggregate(Population ~ Continent, data = HPI_data, sum)\nsoft_colors &lt;- c(\"#C6DBEF\", \"#AED9A4\", \"#FDE0DD\", \"#D0D1E6\", \"#FFED6F\", \"#BC80BD\", \"#B3DE69\")\n\npie(pie_data$Population, \n    labels = paste(\"Continent\", pie_data$Continent),\n    col = soft_colors,\n    main = \"Population by Continent\")\n\nlegend(\"topright\", legend = paste(\"Continent\", pie_data$Continent), fill = soft_colors, cex = 0.7, bty = \"n\")\n\n\n\n\n\n\n\npar(op)\n\n\n# Load required package\nif (!requireNamespace(\"akima\", quietly = TRUE)) install.packages(\"akima\")\nlibrary(akima)\n\n# Filter and clean data\ndf &lt;- na.omit(HPI_data[, c(\"GDP_capita\", \"Carbon\", \"HPI\")])\n\n# Interpolate to create regular grid\ninterp_result &lt;- with(df, interp(x = GDP_capita, y = Carbon, z = HPI, duplicate = \"mean\"))\n\n# 3D surface plot\nop &lt;- par(mar = c(1, 1, 4, 1))\n\npersp(interp_result$x, interp_result$y, interp_result$z,\n      theta = 30, phi = 30, expand = 0.5,\n      col = \"lightblue\", border = \"gray30\",\n      xlab = \"GDP per Capita\", ylab = \"Carbon (tons/capita)\", zlab = \"HPI\",\n      main = \"HPI Surface: GDP vs Carbon\")\n\n\n\n\n\n\n\npar(op)\n\n\n\n\nModfied murrell01.R with all the plots\n\nPlotting functions (note: exercise using the happy planet data set http://happyplanetindex.org)\n\n\npar()\nlines()\npoints()\naxis()\nbox()\ntext()\nmtext()\nhist()\nboxplot()\nlegend()\npersp()\nnames()\npie()\nPost your works on your blog/website. Read Murrell‚Äôs R Graphics for reference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nScatterplot: Life Exp vs GDP\nHistogram: Carbon footprint\nBoxplot: HPI by Continent\nBarplot: Mean HPI by Continent\nPie chart: Population by Continent\n3D Surface: HPI ~ GDP + Carbon\n\n\n\n\npar..\nYes\nYes\nYes\nYes\nYes\nYes\n\n\nlines..\nYes (lowess)\nYes (density)\nNo\nNo\nNo\nNo\n\n\npoints..\nYes (pch, bg)\nNo\nNo\nNo\nNo\nNo\n\n\naxis..\nNo\nYes\nNo\nNo\nNo\nNo\n\n\nbox..\nNo\nYes\nNo\nNo\nNo\nNo\n\n\ntext..\nNo\nYes\nNo\nNo\nNo\nNo\n\n\nmtext..\nNo\nNo\nYes\nNo\nNo\nNo\n\n\nhist..\nNo\nYes\nNo\nNo\nNo\nNo\n\n\nboxplot..\nNo\nNo\nYes\nNo\nNo\nNo\n\n\nlegend..\nNo\nNo\nNo\nNo\nYes\nNo\n\n\npersp..\nNo\nNo\nNo\nNo\nNo\nYes\n\n\nnames..\nNo\nNo\nNo\nYes (x-axis)\nYes\nNo\n\n\npie..\nNo\nNo\nNo\nNo\nYes\nNo"
  },
  {
    "objectID": "assignment02.html#original",
    "href": "assignment02.html#original",
    "title": "Assignment 02",
    "section": "",
    "text": "Original Code with questions addressed\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=16)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\nChange pch\n\n### Paul Murrell's R examples (selected)\n\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=5)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for?\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n# Exercise: Can you generate these charts individually?  Try these functions \n# using another dataset. Be sure to work on the layout and margins\n\n\nExercise\n\nGenerate these charts individually All, with HPI dataset\nUse a different dataset All\nOptimize the layout an margins All\nTry different cex value? All\nDifferent background color exercise04, bar charts\nIn axis(1, at=seq(0, 16, 4)) What is the first number standing for? This the side of the plot that the axis is drawn. \n\n\n\nHPI_data &lt;- read.csv(\"HPI_data.csv\")\n\npar(mar = c(4, 4, 2, 2), cex = 1)\n\n# Remove rows with NA in GDP or Life Expectancy\nplot_data &lt;- na.omit(HPI_data[, c(\"GDP_capita\", \"Life_exp\")])\n\nx &lt;- plot_data$GDP_capita\ny &lt;- plot_data$Life_exp\n\nplot(x, y, pch = 21, bg = \"lightblue\", col = \"blue\",\n     main = \"Life Expectancy vs GDP per Capita\")\nlines(lowess(x, y, f = 2/3), col = \"red\", lwd = 2)\npoints(x, y, pch = 21, bg = \"lightblue\", cex = 1.2)\n\n\n\n\n\n\n\n\n\npar(mar = c(4, 4, 2, 2), cex = 0.9)\n\nhist(HPI_data$Carbon, col = \"gray80\", main = \"Carbon Footprint per Capita\", freq = FALSE)\n\nlines(density(HPI_data$Carbon, na.rm = TRUE), col = \"blue\", lwd = 2)\n\naxis(1); axis(2); box()\ntext(x = max(HPI_data$Carbon, na.rm = TRUE)*0.8, y = 0.5, labels = \"Density Curve\", col = \"blue\", cex = 0.9)\n\n\n\n\n\n\n\n\n\nop &lt;- par(mar = c(4, 4, 4.5, 2), cex = 0.9)\n\nboxplot(HPI ~ as.factor(Continent), data = HPI_data,\n        col = \"orange\",\n        main = \"HPI by Continent\",\n        ylab = \"HPI\", xlab = \"Continent\")\n\nmtext(\"Happy Planet Index by Region\", side = 3, line = 0.5, cex = 0.7)\n\n\n\n\n\n\n\npar(op)\n\n\nop &lt;- par(mar = c(5, 4, 4, 2) + 0.1, bg = \"gray95\")\n\n# Aggregate HPI by Continent\nmean_hpi &lt;- tapply(HPI_data$HPI, HPI_data$Continent, mean, na.rm = TRUE)\n\n# Barplot of mean HPI by continent\nbarplot(mean_hpi,\n        col = \"steelblue\",\n        main = \"Mean HPI by Continent\",\n        ylab = \"Happy Planet Index (HPI)\",\n        xlab = \"Continent\",\n        names.arg = paste(\"Cont\", names(mean_hpi)))\n\n\n\n\n\n\n\npar(op)\n\n\nop &lt;- par(mar = c(2, 2, 2, 2), cex = 0.8)\n\npie_data &lt;- aggregate(Population ~ Continent, data = HPI_data, sum)\nsoft_colors &lt;- c(\"#C6DBEF\", \"#AED9A4\", \"#FDE0DD\", \"#D0D1E6\", \"#FFED6F\", \"#BC80BD\", \"#B3DE69\")\n\npie(pie_data$Population, \n    labels = paste(\"Continent\", pie_data$Continent),\n    col = soft_colors,\n    main = \"Population by Continent\")\n\nlegend(\"topright\", legend = paste(\"Continent\", pie_data$Continent), fill = soft_colors, cex = 0.7, bty = \"n\")\n\n\n\n\n\n\n\npar(op)\n\n\n# Load required package\nif (!requireNamespace(\"akima\", quietly = TRUE)) install.packages(\"akima\")\nlibrary(akima)\n\n# Filter and clean data\ndf &lt;- na.omit(HPI_data[, c(\"GDP_capita\", \"Carbon\", \"HPI\")])\n\n# Interpolate to create regular grid\ninterp_result &lt;- with(df, interp(x = GDP_capita, y = Carbon, z = HPI, duplicate = \"mean\"))\n\n# 3D surface plot\nop &lt;- par(mar = c(1, 1, 4, 1))\n\npersp(interp_result$x, interp_result$y, interp_result$z,\n      theta = 30, phi = 30, expand = 0.5,\n      col = \"lightblue\", border = \"gray30\",\n      xlab = \"GDP per Capita\", ylab = \"Carbon (tons/capita)\", zlab = \"HPI\",\n      main = \"HPI Surface: GDP vs Carbon\")\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment02.html#modified-code",
    "href": "assignment02.html#modified-code",
    "title": "Assignment 02",
    "section": "",
    "text": "Modfied murrell01.R with all the plots\n\nPlotting functions (note: exercise using the happy planet data set http://happyplanetindex.org)\n\n\npar()\nlines()\npoints()\naxis()\nbox()\ntext()\nmtext()\nhist()\nboxplot()\nlegend()\npersp()\nnames()\npie()\nPost your works on your blog/website. Read Murrell‚Äôs R Graphics for reference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nScatterplot: Life Exp vs GDP\nHistogram: Carbon footprint\nBoxplot: HPI by Continent\nBarplot: Mean HPI by Continent\nPie chart: Population by Continent\n3D Surface: HPI ~ GDP + Carbon\n\n\n\n\npar..\nYes\nYes\nYes\nYes\nYes\nYes\n\n\nlines..\nYes (lowess)\nYes (density)\nNo\nNo\nNo\nNo\n\n\npoints..\nYes (pch, bg)\nNo\nNo\nNo\nNo\nNo\n\n\naxis..\nNo\nYes\nNo\nNo\nNo\nNo\n\n\nbox..\nNo\nYes\nNo\nNo\nNo\nNo\n\n\ntext..\nNo\nYes\nNo\nNo\nNo\nNo\n\n\nmtext..\nNo\nNo\nYes\nNo\nNo\nNo\n\n\nhist..\nNo\nYes\nNo\nNo\nNo\nNo\n\n\nboxplot..\nNo\nNo\nYes\nNo\nNo\nNo\n\n\nlegend..\nNo\nNo\nNo\nNo\nYes\nNo\n\n\npersp..\nNo\nNo\nNo\nNo\nNo\nYes\n\n\nnames..\nNo\nNo\nNo\nYes (x-axis)\nYes\nNo\n\n\npie..\nNo\nNo\nNo\nNo\nYes\nNo"
  },
  {
    "objectID": "assignment01.html",
    "href": "assignment01.html",
    "title": "Assignment 01",
    "section": "",
    "text": "Ground rules: All assigned works are to be published on own GitHub website/blog."
  },
  {
    "objectID": "assignment01.html#revisit-anscombes-examples-anscombe01.r-on-teams",
    "href": "assignment01.html#revisit-anscombes-examples-anscombe01.r-on-teams",
    "title": "Assignment 01",
    "section": "1 Revisit Anscombe‚Äôs examples (anscombe01.R on Teams)",
    "text": "1 Revisit Anscombe‚Äôs examples (anscombe01.R on Teams)\n\n## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nanscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n#View(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\n\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\n\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\n\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)\n\n\n1.1 Read: Anscombe, Francis J. 1973.‚ÄùGraphs in statistical analysis.‚Äù The American Statistician 27, no. 1: 17-21.\n\nCompleted\n\n\n\n1.2 Write one paragraph to analyze and suggest solutions.\nIn the paper Graphs in Statistical Analysis (Anscombe 1973), it is demonstrated that visual inspection of data is essential to the process of analysis. With four datasets, now commonly referred to as the Anscombe Quartet, it is demonstrated that four divergent datasets can have the same statistical properties, but be very different in their structure. The underlying issue was that relying solely on summary data can obfuscate the underlying data features like non-linearity, outliers or other influential points. Anscombe demonstrated the need for residual plots and scatterplots to detect deviations from the underlying model assumptions, such as heteroscedasticity. To address these types of problems, graphic diagnostics should be integrated as a routine component of statistical analysis, especially in a regression or mulivariate analysis."
  },
  {
    "objectID": "assignment01.html#run-fall.r-on-teams",
    "href": "assignment01.html#run-fall.r-on-teams",
    "title": "Assignment 01",
    "section": "2 Run Fall.R (on Teams)",
    "text": "2 Run Fall.R (on Teams)\n\n2.1 Give your own colors (e.g.¬†Winter).\nColor: steelblue, output below\n\n\n2.2 Export the file and post on your GitHub website.\nThis is the file.\nSee below.\n\n2.2.1 Single Color\n\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\n# install.packages(\"gsubfn\")\n# install.packages(\"tidyverse\")\nlibrary(gsubfn)\n\nLoading required package: proto\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"steelblue\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\n\n\n\n\n\n\n2.2.2 More Fall Colors\n\nfall_colors &lt;- c(\"darkorange\", \"goldenrod\", \"firebrick\", \"sienna\", \"chocolate\")\npoints_clean &lt;- na.omit(points)\n\n# Create a separate plot for each color\nfor (color in fall_colors) {\n  # Add color column with current color for all segments\n  points_clean$current_color &lt;- color\n  \n  p &lt;- ggplot(points_clean) + \n    geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), \n                 color = color, lineend = \"round\") +\n    coord_fixed(ratio = 1) + \n    theme_void() +\n    ggtitle(paste(\"Plot with color:\", color))\n  \n  print(p)\n}"
  },
  {
    "objectID": "assignment01.html#write-a-critique-on-a-chart-in-published-work-bookarticlenews-website",
    "href": "assignment01.html#write-a-critique-on-a-chart-in-published-work-bookarticlenews-website",
    "title": "Assignment 01",
    "section": "3 Write a critique on a chart in published work (book/article/news website)",
    "text": "3 Write a critique on a chart in published work (book/article/news website)\n(Hint: Learn from Nathan Yau‚Äôs example discussed in class). Post on your website.\n\n\n\nFigure 01\n\n\nCritique of a Graph\nThe chart (Fig 01) shows the cumulative percentage over time (210‚Äì30 BCE) across four coin hoards. While it provides a useful comparison of coins over time, it falls short in several ways, and could be made easier to read while being graphically more rich.\nTopic\n\nProblem: The chart does not clearly state what is being accumulated. From the context of the article, it should be apparent, but as a stand alone chart, it is not.\n\nSolution: Add a more precise title and subtitle that clarify the subject of the chart.\n\nProblem: What are the sizes of each hoard. It is not clear how many coins are in each hoard or in each epoch.\n\nSolution: Some method of tracking relative sizes with gradient shading or dot size.\nMessage and Narrative\n\nProblem: The visual comparison between sites is good, but no guidance is provided for interpreting these patterns. For instance, Actium‚Äôs rise is extremely late, and HEN shows a sharp increase around 130‚Äì100 BCE. These are visually significant but go unmarked.\n\nSolution: Use annotations, callouts, or direct labeling to highlight key inflection points and guide the viewer‚Äôs attention to what matters. Tying in historical dates into the timeline will give the reader more connection to the data.\nVisual Encoding and Labeling\n\nProblem: Label is unhelpful and hard to read.\n\nSolution: Move it to the side of the graph, delineate each hoard better, and better show color and shape of each.\n\nProblem: Lines are hard read quickly.\n\nSolution: Change the color palette to something that is easier to read. Vary the line styles. Change the point shape or size.\n\nProblem: The graphs text are hard to read.\n\nSolution: Change font, weight, kerning, or any other parameter to be more legible\nGeography\n\nProblem: Are these hoards located near each other or far? The geographic interrelationship is unknown.\n\nSolution: Add a very tiny map in some of the blank space of the chart. Extend the y-axis if more room is needed.\nAxis and Scale Choices\n\nProblem: Axis labels start with odd numbers. Usually it is quicker and easier to conceptualize dates that start with even numbers.\n\nSolution: Change the labels."
  },
  {
    "objectID": "assignment01.html#bibliography",
    "href": "assignment01.html#bibliography",
    "title": "Assignment 01",
    "section": "4 Bibliography",
    "text": "4 Bibliography\nAnscombe, F. J. (1973). Graphs in Statistical Analysis. The American Statistician, 27(1), 17‚Äì21. https://doi.org/10.1080/00031305.1973.10478966"
  },
  {
    "objectID": "SPARQL.html",
    "href": "SPARQL.html",
    "title": "SPARQL",
    "section": "",
    "text": "Nomisma SPARQL Endpoint"
  },
  {
    "objectID": "SPARQL.html#example-1",
    "href": "SPARQL.html#example-1",
    "title": "SPARQL",
    "section": "Example 1",
    "text": "Example 1\nModified from R networks analysis / works\nSELECT DISTINCT ?hoard ?mint ?mintlat ?mintlong WHERE {\n{\n  ?hoard void:inDataset &lt;http://numismatics.org/chrr/&gt; ;\n  dcterms:tableOfContents/nmo:hasTypeSeriesItem/nmo:hasMint ?mint .\n}\nUNION\n{ ?hoard a nmo:Hoard ;\ndcterms:tableOfContents [ nmo:hasTypeSeriesItem ?tsi ] .\n?tsi nmo:hasMint ?mint .\n}\nOPTIONAL { ?hoard nmo:hasFindspot [\ngeo:lat ?hoardlat ;\ngeo:long ?hoardlong ] }\nOPTIONAL { ?mint geo:location [\ngeo:lat ?mintlat ;\ngeo:long ?mintlong ] }\n}"
  },
  {
    "objectID": "SPARQL.html#example-2",
    "href": "SPARQL.html#example-2",
    "title": "SPARQL",
    "section": "Example 2",
    "text": "Example 2\nPREFIX crm:   &lt;http://www.cidoc-crm.org/cidoc-crm/&gt;\nPREFIX dcterms:&lt;http://purl.org/dc/terms/&gt;\nPREFIX foaf:  &lt;http://xmlns.com/foaf/0.1/&gt;\nPREFIX geo:   &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt;\nPREFIX nm:    &lt;http://nomisma.org/id/&gt;\nPREFIX nmo:   &lt;http://nomisma.org/ontology#&gt;\nPREFIX rdfs:  &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPREFIX skos:  &lt;http://www.w3.org/2004/02/skos/core#&gt;\n\nSELECT DISTINCT\n  ?hoard ?hoardID\n  ?type (COALESCE(?tlabel, ?plabel, ?rlabel, ?flabel) AS ?typeLabel)\n  (STR(?startDate) AS ?startYear)\n  (STR(?endDate)   AS ?endYear)\n  ?denomination ?denominationLabel\n  ?material     ?materialLabel\n  ?mint ?mintID\n  ?hoardLat ?hoardLong\n  ?mintLat  ?mintLong\nWHERE {\n  # --- All CHRR hoards and their contents ---\n  ?hoard a nmo:Hoard ;\n         dcterms:tableOfContents ?contents .\n  FILTER(STRSTARTS(STR(?hoard), \"http://numismatics.org/chrr/id/\"))\n  BIND(REPLACE(STR(?hoard), \"http://numismatics.org/chrr/id/\", \"\") AS ?hoardID)\n\n  # --- Mint (via type-series item or directly on contents) ---\n  {\n    ?contents nmo:hasTypeSeriesItem ?type .\n    ?type     nmo:hasMint ?mint .\n  }\n  UNION\n  {\n    ?contents nmo:hasMint ?mint .\n    OPTIONAL { ?contents nmo:hasTypeSeriesItem ?type }\n  }\n  BIND(REPLACE(STR(?mint), \"http://nomisma.org/id/\", \"\") AS ?mintID)\n\n  # --- Type labels & dates (from CRRO type, when present) ---\n  OPTIONAL { ?type dcterms:title    ?tlabel  FILTER(LANG(?tlabel)  = \"\" || LANGMATCHES(LANG(?tlabel),  \"en\")) }\n  OPTIONAL { ?type skos:prefLabel   ?plabel  FILTER(LANG(?plabel) = \"\" || LANGMATCHES(LANG(?plabel), \"en\")) }\n  OPTIONAL { ?type rdfs:label       ?rlabel  FILTER(LANG(?rlabel)  = \"\" || LANGMATCHES(LANG(?rlabel),  \"en\")) }\n  OPTIONAL { ?type foaf:name        ?flabel  FILTER(LANG(?flabel)  = \"\" || LANGMATCHES(LANG(?flabel),  \"en\")) }\n\n  OPTIONAL { ?type nmo:hasStartDate ?startDate }\n  OPTIONAL { ?type nmo:hasEndDate   ?endDate }\n\n  # --- Denomination & material (from type) ---\n  OPTIONAL {\n    ?type nmo:hasDenomination ?denomination .\n    OPTIONAL {\n      ?denomination (skos:prefLabel|rdfs:label|foaf:name) ?denominationLabel .\n      FILTER(LANG(?denominationLabel) = \"\" || LANGMATCHES(LANG(?denominationLabel), \"en\"))\n    }\n  }\n  OPTIONAL {\n    ?type nmo:hasMaterial ?material .\n    OPTIONAL {\n      ?material (skos:prefLabel|rdfs:label|foaf:name) ?materialLabel .\n      FILTER(LANG(?materialLabel) = \"\" || LANGMATCHES(LANG(?materialLabel), \"en\"))\n    }\n  }\n\n  # --- Mint geolocation (Nomisma places) ---\n  OPTIONAL {\n    { ?mint geo:location [ geo:lat ?mintLat ;  geo:long ?mintLong ] }\n    UNION\n    { ?mint geo:lat ?mintLat . ?mint geo:long ?mintLong }\n  }\n\n  # --- Hoard geolocation (several fallbacks common in CHRR) ---\n  OPTIONAL {\n    # A) Direct findspot place with geo\n    { ?hoard nmo:hasFindspot ?hPlace .\n      { ?hPlace geo:location [ geo:lat ?hoardLat ; geo:long ?hoardLong ] }\n      UNION\n      { ?hPlace geo:lat ?hoardLat . ?hPlace geo:long ?hoardLong }\n    }\n    UNION\n    # B) CRM region chain with geo\n    {\n      ?hoard nmo:hasFindspot [\n        crm:P7_took_place_at [\n          crm:P89_falls_within ?region\n        ]\n      ] .\n      ?region geo:location ?spatialThing .\n      ?spatialThing geo:lat ?hoardLat ; geo:long ?hoardLong .\n    }\n    UNION\n    # C) dcterms:spatial to a place with geo\n    {\n      ?hoard dcterms:spatial ?hPlace2 .\n      { ?hPlace2 geo:location [ geo:lat ?hoardLat ; geo:long ?hoardLong ] }\n      UNION\n      { ?hPlace2 geo:lat ?hoardLat . ?hPlace2 geo:long ?hoardLong }\n    }\n  }\n}\nORDER BY ?hoardID ?typeLabel ?type ?mint"
  },
  {
    "objectID": "SPARQL.html#mint-data-only",
    "href": "SPARQL.html#mint-data-only",
    "title": "SPARQL",
    "section": "Mint Data Only",
    "text": "Mint Data Only"
  },
  {
    "objectID": "SPARQL.html#coins-in-hoard",
    "href": "SPARQL.html#coins-in-hoard",
    "title": "SPARQL",
    "section": "Coins in Hoard",
    "text": "Coins in Hoard"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Master‚Äôs student at the University of Texas at Dallas studying GIS.\nI am interested in applying GIS tools and techniques to historical questions. Projects that I have worked on include:\n\nRoman villa site location analysis and predictive modeling\nA Python implementation of Geomorphons\nApplication of low-cost drones to disaster efforts\nPython-based DEM-based spatial statistics via WCS data-acquisition\n\nI am currently working on analyzing Roman coin hoards.\nI have a strong interest in remote sensing and tools.\nCurriculum vitae"
  },
  {
    "objectID": "prepare03.html",
    "href": "prepare03.html",
    "title": "Prepare for Class 03",
    "section": "",
    "text": "Completed\n\nIn the pull down menu above."
  },
  {
    "objectID": "prepare03.html#mcghee-geoff.-2011.",
    "href": "prepare03.html#mcghee-geoff.-2011.",
    "title": "Prepare for Class 03",
    "section": "3.1 McGhee, Geoff. 2011.",
    "text": "3.1 McGhee, Geoff. 2011.\nJournalism in the Age of Data, available at Stanford website.\n\nCompleted\n\n\nWrite a one page review comparing journalistic data visualization and academic data visualization\n\nComparing Journalistic and Academic Data Visualization\nThe landscape of data visualization has expanded rapidly, but two dominant modes‚Äîjournalistic and academic‚Äîillustrate divergent goals, aesthetics, and methodologies. Both serve the essential function of communicating complex data to human audiences, but they differ sharply in how they define success.\nAcademic visualizations emphasize exploration, rigor, and precision. As Martin Wattenberg of IBM notes, visualization is most effective when it‚Äôs treated as both art and writing‚Äînot just a programming task. In academic contexts, visualizations are often designed to reveal patterns, support hypothesis generation, or offer granular, high-dimensional data explorations, typically for an expert audience. Tools like Many Eyes, Protovis, or Flare reflect this ethos: democratizing access while preserving analytical depth. However, these tools often come with steeper learning curves and require knowledge of statistics, programming, and domain-specific theory. The ideal output here is less about beauty and more about insight, even at the cost of complexity or abstraction.\nJournalistic visualizations, on the other hand, aim to inform, persuade, or emotionally resonate with a general audience. They prioritize narrative clarity, accessibility, and aesthetic engagement. Amanda Cox of The New York Times exemplifies this approach with her ‚Äúporcupine chart‚Äù on federal deficit projections, where visual storytelling clarified complex economic data in seconds. Newsrooms now integrate statisticians, GIS experts, and interface designers, focusing on quick-turnaround visuals that must function under tight editorial deadlines. As noted by Sarah Slobin and others, journalistic visuals must ‚Äúgrab attention within four seconds,‚Äù and often adopt slideshow, martini-glass, or drill-down formats to layer depth without overwhelming users.\nYet, challenges remain on both sides. Journalistic visualizations often sacrifice nuance for clarity, potentially leading to shallow or misleading interpretations. Conversely, academic visuals may alienate non-experts by lacking guidance, interactivity, or visual appeal. As Nigel Holmes warns, beautiful but incomprehensible visuals are just as dangerous as poorly written stories.\nThe future likely lies in hybrid approaches. Projects like CrimeSpotting, or the Crisis of Credit video, exemplify a fusion of academic depth and journalistic clarity, blending narrative, interactivity, and motion graphics. As Google‚Äôs public data tools and open data initiatives proliferate, both academics and journalists must adapt‚Äîreimagining their roles not just as analysts or storytellers, but as designers of discovery experiences.\nIn short, academic visualizations illuminate, journalistic ones communicate. The best visualizations do both."
  },
  {
    "objectID": "prepare03.html#isabella-vel√°squez.",
    "href": "prepare03.html#isabella-vel√°squez.",
    "title": "Prepare for Class 03",
    "section": "3.2 Isabella Vel√°squez.",
    "text": "3.2 Isabella Vel√°squez.\nBuilding a Blog with Quarto (YouTube link)\n\nPosit: Creating a Blog\n\nChoose RStudio method\nPublish to GitHub Pages\n\n\n\nCompleted"
  },
  {
    "objectID": "prepare04.html",
    "href": "prepare04.html",
    "title": "Prepare for Class 04",
    "section": "",
    "text": "NY: 3, Cairo, Alberto. 2012. The Functional Art: An introduction to information graphics and visualization. New Riders.\n\nCompleted"
  },
  {
    "objectID": "prepare04.html#note-and-review",
    "href": "prepare04.html#note-and-review",
    "title": "Prepare for Class 04",
    "section": "2.1 Note and Review",
    "text": "2.1 Note and Review"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Test",
    "section": "",
    "text": "# === Libraries ===\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(tigris)  # for county boundaries and roads\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\noptions(tigris_use_cache = TRUE)\n\n# === Step 1: Read the precinct shapefile ===\nprecincts &lt;- st_read(\"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/EPPS6356/dataviz/website/tx_2020/tx_2020.shp\", quiet = TRUE)\n\n# === Step 2: Get county boundaries FIRST (needed for cropping roads) ===\ntx_counties &lt;- counties(state = \"TX\", year = 2020, cb = TRUE) %&gt;%\n  st_transform(st_crs(precincts))  # match CRS\n\n# === Step 3: Get primary roads, filter to Interstates, and clip to TX ===\ntx_roads &lt;- primary_roads() %&gt;%\n  st_transform(st_crs(precincts)) %&gt;%\n  filter(grepl(\"^I-\", FULLNAME)) %&gt;%\n  st_intersection(st_union(tx_counties))\n\nRetrieving data for the year 2024\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# === Step 4: Coerce vote columns to numeric, compute total_votes and turnout ===\nvote_cols &lt;- c(\"G20PRERTRU\", \"G20PREDBID\", \"G20PRELJOR\", \"G20PREGHAW\", \"G20PREOWRI\")\n\nprecincts &lt;- precincts %&gt;%\n  mutate(across(all_of(vote_cols), ~ as.numeric(as.character(.)))) %&gt;%  # Ensure numeric\n  mutate(\n    total_votes = rowSums(across(all_of(vote_cols)), na.rm = TRUE),\n    participation_rate = total_votes / as.numeric(G20VR)\n  ) %&gt;%\n  filter(is.finite(participation_rate) & participation_rate &gt; 0)\n\n# === Step 5: Create turnout quintiles ===\nprecincts &lt;- precincts %&gt;%\n  mutate(\n    turnout_quintile = cut(\n      participation_rate,\n      breaks = quantile(participation_rate, probs = seq(0, 1, 0.2), na.rm = TRUE),\n      include.lowest = TRUE,\n      labels = c(\"Q1 (Lowest)\", \"Q2\", \"Q3\", \"Q4\", \"Q5 (Highest)\")\n    )\n  )\n\n# === Step 6: Plot the map ===\nggplot() +\n  # Precinct fill\n  geom_sf(data = precincts, aes(fill = turnout_quintile), color = NA) +\n  \n  # County boundaries\n  geom_sf(data = tx_counties, fill = NA, color = \"black\", size = 0.2) +\n  \n  # Freeways overlay\n  geom_sf(data = tx_roads, color = \"white\", size = 0.4, alpha = 0.7) +\n  \n  # Color scale\n  scale_fill_viridis_d(name = \"Turnout Quintile\", option = \"cividis\", direction = -1) +\n  \n  labs(\n    title = \"2020 Voter Participation by Precinct in Texas\",\n    subtitle = \"Turnout quintiles with county and freeway overlays\",\n    caption = \"Data: tx_2020.shp + TIGER/Line (Interstates)\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# === Libraries ===\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\n\n# === Load spatial data ===\nprecincts &lt;- st_read(\"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/EPPS6356/dataviz/website/tx_2020/tx_2020.shp\", quiet = TRUE)\ntx_counties &lt;- counties(state = \"TX\", cb = TRUE, year = 2020) %&gt;%\n  st_transform(st_crs(precincts))\n\n# === Add freeways ===\ntx_roads &lt;- primary_roads() %&gt;%\n  st_transform(st_crs(precincts)) %&gt;%\n  filter(grepl(\"^I-\", FULLNAME)) %&gt;%\n  st_intersection(st_union(tx_counties))\n\n# === Load county-level data ===\nedu &lt;- read.csv(\"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/EPPS6356/Project/nhgis0025_csv/nhgis0025_ds249_20205_county.csv\")\nhlth &lt;- read.csv(\"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/EPPS6356/Project/nhgis0025_csv/nhgis0025_ds250_20205_county.csv\")\n\nedu &lt;- edu %&gt;%\n  filter(STUSAB == \"TX\") %&gt;%\n  mutate(pct_bachelor = AMRYE003 / AMRYE001) %&gt;%\n  select(GISJOIN, pct_bachelor, COUNTY, COUNTYA)\n\nhlth &lt;- hlth %&gt;%\n  filter(STUSAB == \"TX\") %&gt;%\n  mutate(uninsured = ANB0M169 / 100) %&gt;%\n  select(GISJOIN, uninsured, COUNTY, COUNTYA)\n\n# === Combine metrics ===\ncivic_county &lt;- left_join(edu, hlth, by = c(\"GISJOIN\", \"COUNTY\", \"COUNTYA\"))\n\n# === Aggregate turnout at county level ===\nprecincts &lt;- precincts %&gt;%\n  mutate(across(c(G20PRERTRU, G20PREDBID, G20PRELJOR, G20PREGHAW, G20PREOWRI), ~ as.numeric(as.character(.)))) %&gt;%\n  mutate(\n    total_votes = rowSums(across(starts_with(\"G20PRE\")), na.rm = TRUE),\n    participation_rate = total_votes / as.numeric(G20VR),\n    fips = sprintf(\"%03s\", as.character(CNTY))\n  )\n\ncounty_turnout &lt;- precincts %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(fips) %&gt;%\n  summarise(avg_turnout = mean(participation_rate, na.rm = TRUE))\n\ncivic_county &lt;- civic_county %&gt;%\n  mutate(fips = sprintf(\"%03d\", COUNTYA)) %&gt;%\n  left_join(county_turnout, by = \"fips\")\n\n# === Classify counties ===\ncivic_county &lt;- civic_county %&gt;%\n  mutate(\n    edu_bin = ifelse(pct_bachelor &gt; median(pct_bachelor, na.rm = TRUE), \"High\", \"Low\"),\n    hlth_bin = ifelse(uninsured &lt; median(uninsured, na.rm = TRUE), \"High\", \"Low\"),\n    turnout_bin = ifelse(avg_turnout &gt; median(avg_turnout, na.rm = TRUE), \"High\", \"Low\"),\n    civic_type = case_when(\n      edu_bin == \"High\" & hlth_bin == \"High\" & turnout_bin == \"High\" ~ \"Civic Garden\",\n      edu_bin == \"Low\"  & hlth_bin == \"Low\"  & turnout_bin == \"Low\"  ~ \"Civic Desert\",\n      TRUE ~ \"Mixed\"\n    )\n  )\n\n# === Join to precincts ===\nprecincts &lt;- precincts %&gt;%\n  left_join(civic_county %&gt;% select(fips, civic_type), by = \"fips\")\n\n# === Plot ===\nggplot() +\n  geom_sf(data = precincts, aes(fill = civic_type), color = NA) +\n  geom_sf(data = tx_counties, fill = NA, color = \"black\", size = 0.2) +\n  geom_sf(data = tx_roads, color = \"white\", size = 0.3, alpha = 0.8) +  # Add freeways in white\n  scale_fill_manual(values = c(\"Civic Garden\" = \"#1a9850\", \"Civic Desert\" = \"#d73027\", \"Mixed\" = \"grey70\")) +\n  labs(\n    title = \"Civic Gardens and Deserts in Texas\",\n    subtitle = \"Overlaid with Interstates and County Boundaries\",\n    fill = \"Civic Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(deldir)\n\n# === Prepare county centroids and civic type ===\ncounty_centroids &lt;- tx_counties %&gt;%\n  mutate(fips = sprintf(\"%03s\", COUNTYFP)) %&gt;%\n  left_join(civic_county, by = \"fips\") %&gt;%\n  st_centroid()\n\n# === Extract X/Y coordinates ===\ncoords &lt;- st_coordinates(county_centroids)\n\n# === Generate Voronoi diagram ===\nvoronoi_raw &lt;- deldir(coords[, \"X\"], coords[, \"Y\"])\n\n# === Convert to closed polygons ===\ntiles &lt;- tile.list(voronoi_raw)\nvoronoi_sf &lt;- lapply(seq_along(tiles), function(i) {\n  tile &lt;- tiles[[i]]\n  coords_tile &lt;- cbind(tile$x, tile$y)\n  coords_tile &lt;- rbind(coords_tile, coords_tile[1, ])  # close polygon\n  st_polygon(list(coords_tile))\n})\n\nvoronoi_sf &lt;- st_sfc(voronoi_sf, crs = st_crs(county_centroids)) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(civic_type = county_centroids$civic_type)\n\n# === Clip to Texas boundary ===\ntx_union &lt;- st_union(tx_counties)\nvoronoi_clipped &lt;- st_intersection(voronoi_sf, tx_union)\n\n# === Plot Voronoi Map ===\nggplot() +\n  geom_sf(data = voronoi_clipped, aes(fill = civic_type), color = \"white\", size = 0.1, alpha = 0.9) +\n  geom_sf(data = tx_counties, fill = NA, color = \"black\", size = 0.2) +\n  scale_fill_manual(values = c(\"Civic Garden\" = \"#1a9850\", \"Civic Desert\" = \"#d73027\", \"Mixed\" = \"grey70\")) +\n  labs(\n    title = \"Voronoi Diagram of Civic Types by County Centroid\",\n    subtitle = \"Civic Gardens, Deserts, and Mixed Areas\",\n    fill = \"Civic Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(deldir)\n\n# === Rebuild county centroids with civic_type ===\ncounty_centroids &lt;- tx_counties %&gt;%\n  mutate(fips = sprintf(\"%03s\", COUNTYFP)) %&gt;%\n  left_join(civic_county, by = \"fips\") %&gt;%\n  st_centroid() %&gt;%\n  mutate(X = st_coordinates(.)[, 1], Y = st_coordinates(.)[, 2])  # add X/Y explicitly\n\n# === Create Voronoi diagram from coordinates ===\nvoronoi_raw &lt;- deldir(county_centroids$X, county_centroids$Y)\n\n# === Convert tiles to closed polygons ===\ntiles &lt;- tile.list(voronoi_raw)\nvoronoi_polys &lt;- lapply(seq_along(tiles), function(i) {\n  tile &lt;- tiles[[i]]\n  coords_tile &lt;- cbind(tile$x, tile$y)\n  coords_tile &lt;- rbind(coords_tile, coords_tile[1, ])  # close polygon\n  st_polygon(list(coords_tile))\n})\n\n# === Wrap into sf object\nvoronoi_sf &lt;- st_sfc(voronoi_polys, crs = st_crs(county_centroids)) %&gt;%\n  st_as_sf() %&gt;%\n  bind_cols(st_drop_geometry(county_centroids)[, c(\"fips\", \"civic_type\")])  # explicit bind\n\n# === Clip to Texas shape\ntx_union &lt;- st_union(tx_counties)\nvoronoi_clipped &lt;- st_intersection(voronoi_sf, tx_union)\n\n# === Plot with precincts and Voronoi overlay\nggplot() +\n  # Precinct base layer\n  geom_sf(data = precincts, aes(fill = civic_type), color = NA, alpha = 0.8) +\n\n  # Voronoi semi-transparent overlay\n  geom_sf(data = voronoi_clipped, aes(fill = civic_type), color = \"white\", alpha = 0.25) +\n\n  # County outlines\n  geom_sf(data = tx_counties, fill = NA, color = \"black\", size = 0.2) +\n\n  # Freeways\n  geom_sf(data = tx_roads, color = \"white\", size = 0.3, alpha = 0.7) +\n\n  scale_fill_manual(\n    values = c(\"Civic Garden\" = \"#1a9850\", \"Civic Desert\" = \"#d73027\", \"Mixed\" = \"grey70\"),\n    na.value = \"lightgrey\"\n  ) +\n  labs(\n    title = \"Texas Civic Zones: Precinct Detail + Voronoi Overlay\",\n    subtitle = \"Civic type from education, health, and turnout ‚Äî zones derived from county centroids\",\n    fill = \"Civic Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(treemapify)\n\n# === Base data: one row per county ===\ntreemap_data &lt;- civic_county %&gt;%\n  mutate(\n    county_label = paste0(COUNTY, \" Co.\"),\n    civic_type = factor(civic_type, levels = c(\"Civic Garden\", \"Mixed\", \"Civic Desert\"))\n  ) %&gt;%\n  filter(!is.na(civic_type), is.finite(avg_turnout))\n\n# === Treemap plot ===\nggplot(treemap_data, aes(\n  area = avg_turnout,\n  fill = civic_type,\n  label = county_label,\n  subgroup = civic_type\n)) +\n  geom_treemap(color = \"white\", size = 0.5) +\n  geom_treemap_subgroup_border(color = \"black\", size = 1) +\n  geom_treemap_text(colour = \"white\", place = \"center\", grow = TRUE, reflow = TRUE, min.size = 3) +\n  scale_fill_manual(\n    values = c(\"Civic Garden\" = \"#1a9850\", \"Civic Desert\" = \"#d73027\", \"Mixed\" = \"grey70\"),\n    name = \"Civic Type\"\n  ) +\n  labs(\n    title = \"Texas Counties by Civic Type\",\n    subtitle = \"Sized by Average Turnout (2020)\",\n    caption = \"Civic Gardens = High Edu + Health + Turnout; Deserts = Low on all\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# === Normalize function (0‚Äì1 scaling)\nnormalize &lt;- function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n\n# === Prepare data: one row per county\npolar_data &lt;- civic_county %&gt;%\n  filter(is.finite(pct_bachelor), is.finite(uninsured), is.finite(avg_turnout)) %&gt;%\n  mutate(\n    edu_norm     = normalize(pct_bachelor),\n    health_norm  = normalize(1 - uninsured),  # invert: lower uninsured is better\n    turnout_norm = normalize(avg_turnout)\n  ) %&gt;%\n  select(COUNTY, civic_type, edu_norm, health_norm, turnout_norm) %&gt;%\n  pivot_longer(cols = ends_with(\"_norm\"), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = recode(metric,\n                         edu_norm = \"Education\",\n                         health_norm = \"Health\",\n                         turnout_norm = \"Turnout\"))\n\n# === Pick top counties (optional)\ntop_counties &lt;- civic_county %&gt;%\n  arrange(desc(avg_turnout)) %&gt;%\n  slice_head(n = 12) %&gt;%\n  pull(COUNTY)\n\npolar_subset &lt;- polar_data %&gt;%\n  filter(COUNTY %in% top_counties)\n\n# === Polar plot\nggplot(polar_subset, aes(x = metric, y = value, group = COUNTY, fill = civic_type)) +\n  geom_polygon(alpha = 0.3, color = \"black\", show.legend = FALSE) +\n  geom_line(color = \"black\", linewidth = 0.2) +\n  coord_polar() +\n  facet_wrap(~ COUNTY, ncol = 4) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"Civic Garden\" = \"#1a9850\", \"Civic Desert\" = \"#d73027\", \"Mixed\" = \"grey70\")) +\n  labs(\n    title = \"Civic Metrics by County (Polar Plot)\",\n    subtitle = \"Education, Health, and Turnout scaled 0‚Äì100%\",\n    x = NULL, y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(face = \"bold\", size = 10),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(knitr)\n\n# === Recalculate classification logic for clarity ===\ncivic_summary &lt;- civic_county %&gt;%\n  mutate(\n    `Pct Bachelor's` = round(100 * pct_bachelor, 1),\n    `Pct Uninsured` = round(100 * uninsured, 1),\n    `Avg Turnout (%)` = round(100 * avg_turnout, 1),\n    \n    `Education` = ifelse(pct_bachelor &gt; median(pct_bachelor, na.rm = TRUE), \"High\", \"Low\"),\n    `Health`    = ifelse(uninsured &lt; median(uninsured, na.rm = TRUE), \"High\", \"Low\"),  # lower = better\n    `Turnout`   = ifelse(avg_turnout &gt; median(avg_turnout, na.rm = TRUE), \"High\", \"Low\"),\n    \n    `Civic Type` = case_when(\n      Education == \"High\" & Health == \"High\" & Turnout == \"High\" ~ \"Civic Garden\",\n      Education == \"Low\"  & Health == \"Low\"  & Turnout == \"Low\"  ~ \"Civic Desert\",\n      TRUE ~ \"Mixed\"\n    )\n  ) %&gt;%\n  select(COUNTY, `Pct Bachelor's`, `Pct Uninsured`, `Avg Turnout (%)`,\n         Education, Health, Turnout, `Civic Type`) %&gt;%\n  arrange(`Civic Type`, COUNTY)\n\n# === Display in table\nkable(head(civic_summary, 15), caption = \"Sample Civic Metrics Table (first 15 counties)\")\n\n\nSample Civic Metrics Table (first 15 counties)\n\n\n\n\n\n\n\n\n\n\n\n\nCOUNTY\nPct Bachelor‚Äôs\nPct Uninsured\nAvg Turnout (%)\nEducation\nHealth\nTurnout\nCivic Type\n\n\n\n\nAnderson County\n0.7\n117\n66.2\nLow\nLow\nLow\nCivic Desert\n\n\nAransas County\n0.6\n114\n67.3\nLow\nLow\nLow\nCivic Desert\n\n\nAtascosa County\n0.8\n61\n64.5\nLow\nLow\nLow\nCivic Desert\n\n\nBell County\n0.5\n55\n59.8\nLow\nLow\nLow\nCivic Desert\n\n\nBowie County\n0.3\n97\n59.8\nLow\nLow\nLow\nCivic Desert\n\n\nChildress County\n0.4\n73\n62.5\nLow\nLow\nLow\nCivic Desert\n\n\nCoryell County\n0.5\n55\n60.3\nLow\nLow\nLow\nCivic Desert\n\n\nCrockett County\n0.0\n87\n63.6\nLow\nLow\nLow\nCivic Desert\n\n\nGregg County\n0.5\n105\n65.5\nLow\nLow\nLow\nCivic Desert\n\n\nHarrison County\n0.6\n55\n66.4\nLow\nLow\nLow\nCivic Desert\n\n\nHunt County\n0.5\n132\n65.8\nLow\nLow\nLow\nCivic Desert\n\n\nLubbock County\n0.8\n60\n65.2\nLow\nLow\nLow\nCivic Desert\n\n\nMcLennan County\n0.6\n80\n61.6\nLow\nLow\nLow\nCivic Desert\n\n\nMitchell County\n0.4\n73\n58.0\nLow\nLow\nLow\nCivic Desert\n\n\nNewton County\n0.2\n51\n65.5\nLow\nLow\nLow\nCivic Desert\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nHow It‚Äôs Calculated\n\n\n\n\nPct Bachelor‚Äôs\npct_bachelor = AMRYE003 / AMRYE001 from NHGIS education CSV\n\n\nPct Uninsured\nuninsured = ANB0M169 / 100 from NHGIS health CSV (inverted)\n\n\nAvg Turnout\nWeighted average of precinct turnout per county:\n\n\n\n                  `turnout = (votes cast) / (G20VR)`  \n                  and aggregated to county level |\nHigh/Low status is determined relative to the median for Texas counties for each variable.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# === Prepare analysis dataset ===\nanova_data &lt;- civic_county %&gt;%\n  mutate(\n    civic_type = case_when(\n      pct_bachelor &gt; median(pct_bachelor, na.rm = TRUE) &\n      uninsured &lt; median(uninsured, na.rm = TRUE) &\n      avg_turnout &gt; median(avg_turnout, na.rm = TRUE) ~ \"Civic Garden\",\n      \n      pct_bachelor &lt;= median(pct_bachelor, na.rm = TRUE) &\n      uninsured &gt;= median(uninsured, na.rm = TRUE) &\n      avg_turnout &lt;= median(avg_turnout, na.rm = TRUE) ~ \"Civic Desert\",\n      \n      TRUE ~ \"Mixed\"\n    )\n  ) %&gt;%\n  filter(\n    is.finite(pct_bachelor),\n    is.finite(uninsured),\n    is.finite(avg_turnout)\n  ) %&gt;%\n  select(civic_type, pct_bachelor, uninsured, avg_turnout) %&gt;%\n  filter(complete.cases(.))  # drop NAs\n\n# === Run ANOVA tests ===\naov_edu     &lt;- aov(pct_bachelor ~ civic_type, data = anova_data)\naov_health  &lt;- aov(uninsured ~ civic_type, data = anova_data)\naov_turnout &lt;- aov(avg_turnout ~ civic_type, data = anova_data)\n\n# === Summary output\nsummary(aov_edu)\n\n             Df   Sum Sq   Mean Sq F value   Pr(&gt;F)    \ncivic_type    2 0.001297 0.0006484   10.63 3.77e-05 ***\nResiduals   240 0.014639 0.0000610                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov_health)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)   \ncivic_type    2  1.288  0.6441   5.952  0.003 **\nResiduals   240 25.971  0.1082                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov_turnout)\n\n             Df Sum Sq  Mean Sq F value  Pr(&gt;F)   \ncivic_type    2 0.0626 0.031278   5.834 0.00336 **\nResiduals   240 1.2866 0.005361                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# === Post-hoc comparison: Which groups differ?\nTukeyHSD(aov_edu)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = pct_bachelor ~ civic_type, data = anova_data)\n\n$civic_type\n                                  diff           lwr          upr     p adj\nCivic Garden-Civic Desert  0.013447811  0.0065153136  0.020380308 0.0000227\nMixed-Civic Desert         0.004787816  0.0008181401  0.008757492 0.0133608\nMixed-Civic Garden        -0.008659995 -0.0146221703 -0.002697819 0.0020782\n\nTukeyHSD(aov_health)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = uninsured ~ civic_type, data = anova_data)\n\n$civic_type\n                                diff         lwr         upr     p adj\nCivic Garden-Civic Desert -0.3967500 -0.68874863 -0.10475137 0.0043698\nMixed-Civic Desert        -0.1945156 -0.36171936 -0.02731174 0.0178904\nMixed-Civic Garden         0.2022344 -0.04889398  0.45336288 0.1411156\n\nTukeyHSD(aov_turnout)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = avg_turnout ~ civic_type, data = anova_data)\n\n$civic_type\n                                 diff          lwr         upr     p adj\nCivic Garden-Civic Desert  0.09412180  0.029130121  0.15911349 0.0021518\nMixed-Civic Desert         0.02854876 -0.008666681  0.06576419 0.1687482\nMixed-Civic Garden        -0.06557305 -0.121468033 -0.00967806 0.0167459"
  },
  {
    "objectID": "test.html#summary-of-calculations",
    "href": "test.html#summary-of-calculations",
    "title": "Test",
    "section": "",
    "text": "Metric\nHow It‚Äôs Calculated\n\n\n\n\nPct Bachelor‚Äôs\npct_bachelor = AMRYE003 / AMRYE001 from NHGIS education CSV\n\n\nPct Uninsured\nuninsured = ANB0M169 / 100 from NHGIS health CSV (inverted)\n\n\nAvg Turnout\nWeighted average of precinct turnout per county:\n\n\n\n                  `turnout = (votes cast) / (G20VR)`  \n                  and aggregated to county level |\nHigh/Low status is determined relative to the median for Texas counties for each variable.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# === Prepare analysis dataset ===\nanova_data &lt;- civic_county %&gt;%\n  mutate(\n    civic_type = case_when(\n      pct_bachelor &gt; median(pct_bachelor, na.rm = TRUE) &\n      uninsured &lt; median(uninsured, na.rm = TRUE) &\n      avg_turnout &gt; median(avg_turnout, na.rm = TRUE) ~ \"Civic Garden\",\n      \n      pct_bachelor &lt;= median(pct_bachelor, na.rm = TRUE) &\n      uninsured &gt;= median(uninsured, na.rm = TRUE) &\n      avg_turnout &lt;= median(avg_turnout, na.rm = TRUE) ~ \"Civic Desert\",\n      \n      TRUE ~ \"Mixed\"\n    )\n  ) %&gt;%\n  filter(\n    is.finite(pct_bachelor),\n    is.finite(uninsured),\n    is.finite(avg_turnout)\n  ) %&gt;%\n  select(civic_type, pct_bachelor, uninsured, avg_turnout) %&gt;%\n  filter(complete.cases(.))  # drop NAs\n\n# === Run ANOVA tests ===\naov_edu     &lt;- aov(pct_bachelor ~ civic_type, data = anova_data)\naov_health  &lt;- aov(uninsured ~ civic_type, data = anova_data)\naov_turnout &lt;- aov(avg_turnout ~ civic_type, data = anova_data)\n\n# === Summary output\nsummary(aov_edu)\n\n             Df   Sum Sq   Mean Sq F value   Pr(&gt;F)    \ncivic_type    2 0.001297 0.0006484   10.63 3.77e-05 ***\nResiduals   240 0.014639 0.0000610                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov_health)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)   \ncivic_type    2  1.288  0.6441   5.952  0.003 **\nResiduals   240 25.971  0.1082                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov_turnout)\n\n             Df Sum Sq  Mean Sq F value  Pr(&gt;F)   \ncivic_type    2 0.0626 0.031278   5.834 0.00336 **\nResiduals   240 1.2866 0.005361                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# === Post-hoc comparison: Which groups differ?\nTukeyHSD(aov_edu)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = pct_bachelor ~ civic_type, data = anova_data)\n\n$civic_type\n                                  diff           lwr          upr     p adj\nCivic Garden-Civic Desert  0.013447811  0.0065153136  0.020380308 0.0000227\nMixed-Civic Desert         0.004787816  0.0008181401  0.008757492 0.0133608\nMixed-Civic Garden        -0.008659995 -0.0146221703 -0.002697819 0.0020782\n\nTukeyHSD(aov_health)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = uninsured ~ civic_type, data = anova_data)\n\n$civic_type\n                                diff         lwr         upr     p adj\nCivic Garden-Civic Desert -0.3967500 -0.68874863 -0.10475137 0.0043698\nMixed-Civic Desert        -0.1945156 -0.36171936 -0.02731174 0.0178904\nMixed-Civic Garden         0.2022344 -0.04889398  0.45336288 0.1411156\n\nTukeyHSD(aov_turnout)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = avg_turnout ~ civic_type, data = anova_data)\n\n$civic_type\n                                 diff          lwr         upr     p adj\nCivic Garden-Civic Desert  0.09412180  0.029130121  0.15911349 0.0021518\nMixed-Civic Desert         0.02854876 -0.008666681  0.06576419 0.1687482\nMixed-Civic Garden        -0.06557305 -0.121468033 -0.00967806 0.0167459"
  },
  {
    "objectID": "project02.html",
    "href": "project02.html",
    "title": "Project - Proposal",
    "section": "",
    "text": "\\(\\textbf{Differentiation}\\\\\\)\n\\\n\\ $\nIn-line \\[\\begin{align*}\nP(\\text{not } A) &= 1 - P(A) \\\\\nP(A \\text{ and } B) &= P(A) \\times P(B)\n\\end{align*}\\]\nSide-by-side\n\\[\\begin{array}{ll}\nP(\\text{not } A) = 1 - P(A) &\nP(A \\text{ and } B) = P(A) \\times P(B)\n\\end{array}\\]\n\\(\\textbf{Statistics}\\)\n\nThis is a simple LaTeX document that demonstrates how to write mathematical equations.\nHere is an inline equation: \\(E = mc^2\\), which shows the relationship between energy, mass, and the speed of light. $$ Here is the same equation as a displayed equation:\nE = mc^2\n$$\n\n\nTo make the visualizations sophisticated, artistic, and attractive: - Use custom color palettes (viridis, wesanderson, scientific palettes).\n- Explore non-traditional layouts (circular, radial, layered).\n- Add interactive storytelling: clicking on a region brings up linked health/education/voting histories."
  },
  {
    "objectID": "project02.html#additions",
    "href": "project02.html#additions",
    "title": "Project - Proposal",
    "section": "",
    "text": "To make the visualizations sophisticated, artistic, and attractive: - Use custom color palettes (viridis, wesanderson, scientific palettes).\n- Explore non-traditional layouts (circular, radial, layered).\n- Add interactive storytelling: clicking on a region brings up linked health/education/voting histories."
  }
]