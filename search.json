[
  {
    "objectID": "project01.html",
    "href": "project01.html",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Background\nMoses Finley’s The Ancient Economy (1973) challenged traditional views of ancient economies, particularly those of Greece and Rome. Finley argued that ancient economies were fundamentally different from modern capitalist systems and should not be analyzed using modern economic concepts.\nSince publication, there have been some shifting thoughts on the inequalities of ancient Rome. This project will be designed to see if there has been a shift in scholarly publications on inequalities in the Roman economic system.\n\n\nHypothesis\n\\[\n\\begin{flalign}\n\\scriptsize\nH_0&: \\scriptsize  \\text{Scholarship about inequality in ancient Rome has not changed.}\\\\\n\\scriptsize\nH_1&: \\scriptsize  \\text{Scholarship has changed.}\n\\end{flalign}\n\\]\n\n\nData Collection\nInitially, data collection will start with gathering access to scholarly journals related to the ancient Roman Empire. We will start with selected articles from JSTOR from publications such as The Journal of Roman Studies.\n\n\nMethods\n\nJournal Acquisition\n\nAcquire a limited dataset of publications\nPreprocess the data including OCR, tokenization, stemming, etc.\n\nConversion\n\nConvert text to numerical representations\n\nModel Selection & Training\nPost-processing\n\n\n\nStart\nThe first step in the process is to verify that the data can actually be collected. Given the size and time constraints, it may be necessary to limit the scope of data collection to a reasonable, but not exhaustive amount.\nThe book The Ancient Economy has been obtained and OCRed.\nOne paper has been downloaded for examination (Brock, Motta, and Terrenato 2021).\n\n\nAlternative Projects to Consider\nSome other projects that could be considered:\n\nPredictive Modeling of Archaeological Site Locations\nAutomated Artifact Classification and Typology\nTemporal and Spatial Analysis of Settlement Patterns\nText Mining and NLP for Archaeological Literature\nReconstructing Ancient Trade Routes\n\n\n\nBibliography\nBrock, Andrea L., Lorenzo Motta, and Nicola Terrenato. 2021. “On the Banks of the Tiber: Opportunity and Transformation in Early Rome.” The Journal of Roman Studies 111: 1–30. https://www.jstor.org/stable/27128849.\nFinley, Moses I. 1973. The Ancient Economy. Berkeley: University of California Press."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Master’s student at the University of Texas at Dallas studying GIS.\nI am interested in applying GIS tools and techniques to historical questions. Projects that I have worked on include:\n\nRoman villa site location analysis and predictive modeling\nA Python implementation of Geomorphons\nApplication of low-cost drones to disaster efforts\nPython-based DEM-based spatial statistics via WCS data-acquisition\n\nI am currently working on analyzing Roman coin hoards.\nI have a strong interest in remote sensing and tools.\nCurriculum vitae"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Glendenning",
    "section": "",
    "text": "I am a masters student studying geographical information systems.\n\nContact\njmg220005@utdallas.edu"
  },
  {
    "objectID": "assignment02.html",
    "href": "assignment02.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Lab01, Lab02 in R (Download the .qmd files in Teams)\n\n\nRender and upload your files to GitHub website\nBe sure to uploaded associated folders (e.g. Lab01.html, Lab01_files, Lab02.html, Lab02_files)\n\n\n\nCompleted. In navigation bar under ‘Assignments.’\n\n\n\nReview Chapters 3 to 7 in R4DS, Section 3.5 in Data programming. Run an exploratory data analysis with R using the TEDS2016 dataset (sample codes as follows):\n\n\n\nCompleted. See below.\n\n\nlibrary(haven, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(knitr, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(dplyr, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(car, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(vcd, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(viridis, quietly = TRUE, warn.conflicts = FALSE)\n\n# Read Data\nTEDS_2016 &lt;-\nread_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nhead(TEDS_2016)\n\n# A tibble: 6 × 54\n  District      Sex     Age     Edu     Arear   Career  Career8 Ethnic  Party   \n  &lt;dbl+lbl&gt;     &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n1 201 [Yi Lan … 2 [Fem… 4 [50-… 4 [Col… 1 [Tai… 1 [Hig… 1 [Civ… 1 [Tai… 25 [Neu…\n2 201 [Yi Lan … 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n3 201 [Yi Lan … 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n4 201 [Yi Lan … 1 [Mal… 4 [50-… 2 [Jun… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n5 201 [Yi Lan … 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 9 [Nor… 25 [Neu…\n6 201 [Yi Lan … 2 [Fem… 5 [Abo… 2 [Jun… 1 [Tai… 2 [Low… 7 [Hou… 1 [Tai…  6 [Som…\n# ℹ 45 more variables: PartyID &lt;dbl+lbl&gt;, Tondu &lt;dbl+lbl&gt;, Tondu3 &lt;dbl+lbl&gt;,\n#   nI2 &lt;dbl+lbl&gt;, votetsai &lt;dbl&gt;, green &lt;dbl&gt;, votetsai_nm &lt;dbl&gt;,\n#   votetsai_all &lt;dbl&gt;, Independence &lt;dbl&gt;, Unification &lt;dbl&gt;, sq &lt;dbl&gt;,\n#   Taiwanese &lt;dbl&gt;, edu &lt;dbl&gt;, female &lt;dbl&gt;, whitecollar &lt;dbl&gt;,\n#   lowincome &lt;dbl&gt;, income &lt;dbl&gt;, income_nm &lt;dbl&gt;, age &lt;dbl&gt;, KMT &lt;dbl&gt;,\n#   DPP &lt;dbl&gt;, npp &lt;dbl&gt;, noparty &lt;dbl&gt;, pfp &lt;dbl&gt;, South &lt;dbl&gt;, north &lt;dbl&gt;,\n#   Minnan_father &lt;dbl&gt;, Mainland_father &lt;dbl&gt;, Econ_worse &lt;dbl&gt;, …\n\nstr(TEDS_2016)    # Check structure of data\n\ntibble [1,690 × 54] (S3: tbl_df/tbl/data.frame)\n $ District       : dbl+lbl [1:1690] 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201...\n   ..@ label       : chr \"District\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:73] 201 401 501 502 701 702 703 704 801 802 ...\n   .. ..- attr(*, \"names\")= chr [1:73] \"Yi Lan County Single District\" \"Hsinchu County Single District\" \"Miaoli County 1st District\" \"Miaoli County 2nd District\" ...\n $ Sex            : dbl+lbl [1:1690] 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1,...\n   ..@ label       : chr \"Sex\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:2] 1 2\n   .. ..- attr(*, \"names\")= chr [1:2] \"Male\" \"Female\"\n $ Age            : dbl+lbl [1:1690] 4, 2, 5, 4, 5, 5, 5, 4, 5, 4, 5, 1, 5, 3, 4, 5, 4, 5,...\n   ..@ label       : chr \"Age\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"20-29\" \"30-39\" \"40-49\" \"50-59\" ...\n $ Edu            : dbl+lbl [1:1690] 4, 5, 5, 2, 1, 2, 1, 5, 1, 1, 1, 2, 1, 5, 5, 1, 3, 4,...\n   ..@ label       : chr \"Education\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:6] 1 2 3 4 5 9\n   .. ..- attr(*, \"names\")= chr [1:6] \"Below elementary school\" \"Junior high school\" \"Senior high school\" \"College\" ...\n $ Arear          : dbl+lbl [1:1690] 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n   ..@ label       : chr \"Area\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:6] 1 2 3 4 5 6\n   .. ..- attr(*, \"names\")= chr [1:6] \"Taipei, New Taipei, Keelung and Yi Lan\" \"Taoyuan, Hsinchu and Miaoli\" \"Taichung, Changhua and Nantou\" \"Yunlin, Chiayi and Tainan\" ...\n $ Career         : dbl+lbl [1:1690] 1, 2, 1, 4, 3, 2, 4, 1, 4, 3, 3, 5, 5, 4, 1, 5, 2, 2,...\n   ..@ label       : chr \"Occupations5\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"Hight-class WHITE COLLAR\" \"Low-class WHITE COLLAR\" \"FARMER\" \"WORKER\" ...\n $ Career8        : dbl+lbl [1:1690] 1, 3, 1, 4, 5, 7, 4, 2, 4, 5, 5, 7, 7, 7, 2, 7, 3, 1,...\n   ..@ label       : chr \"Occupation8\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:8] 1 2 3 4 5 6 7 8\n   .. ..- attr(*, \"names\")= chr [1:8] \"Civil servants\" \"Managers and  Professionals (priv.)\" \"CLERKS (priv.)\" \"Labor (priv.)\" ...\n $ Ethnic         : dbl+lbl [1:1690] 1, 2, 2, 1, 9, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 9, 2, 2,...\n   ..@ label       : chr \"Ethnic\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:4] 1 2 3 9\n   .. ..- attr(*, \"names\")= chr [1:4] \"Taiwanese\" \"Both\" \"Chinese\" \"Noresponse\"\n $ Party          : dbl+lbl [1:1690] 25, 25,  3, 25, 25,  6, 25, 24, 25, 25,  6,  5, 25,  ...\n   ..@ label       : chr \"Party Preference\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:26] 1 2 3 4 5 6 7 8 9 10 ...\n   .. ..- attr(*, \"names\")= chr [1:26] \"Strongly support KMT\" \"Somewhat support KMT\" \"Lean to KMT\" \"Somewhat lean to KMT\" ...\n $ PartyID        : dbl+lbl [1:1690] 9, 9, 1, 9, 9, 2, 9, 6, 9, 9, 2, 2, 9, 1, 1, 9, 9, 9,...\n   ..@ label       : chr \"Party Identification\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"KMT\" \"DPP\" \"NP\" \"PFP\" ...\n $ Tondu          : dbl+lbl [1:1690] 3, 5, 3, 5, 9, 4, 9, 6, 9, 9, 5, 5, 9, 5, 4, 9, 9, 4,...\n   ..@ label       : chr \"Position on unification and independence\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"Immediate unification\" \"Maintain the status quo,move toward unification\" \"Maintain the status quo, decide either unification or independence\" \"Maintain the status quo forever\" ...\n $ Tondu3         : dbl+lbl [1:1690] 2, 3, 2, 3, 9, 2, 9, 3, 9, 9, 3, 3, 9, 3, 2, 9, 9, 2,...\n   ..@ label       : chr \"3 categories of TONDU\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:4] 1 2 3 9\n   .. ..- attr(*, \"names\")= chr [1:4] \"Unification\" \"Maintain the status quo\" \"Independence\" \"Nonresponse\"\n $ nI2            : dbl+lbl [1:1690]  3, 98, 98,  3, 98, 98, 98,  3, 98,  1,  2, 98, 98,  ...\n   ..@ label       : chr \"Who is the current the premier of our country?\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 95 98\n   .. ..- attr(*, \"names\")= chr [1:5] \"Correct\" \"Incorrect\" \"I know but can't remember the name\" \"Refuse to answer\" ...\n $ votetsai       : num [1:1690] NA 1 0 NA NA 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ green          : num [1:1690] 0 0 0 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votetsai_nm    : num [1:1690] NA 1 0 NA NA 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votetsai_all   : num [1:1690] 0 1 0 0 0 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Independence   : num [1:1690] 0 1 0 1 0 0 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Unification    : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ sq             : num [1:1690] 1 0 1 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Taiwanese      : num [1:1690] 1 0 0 1 0 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ edu            : num [1:1690] 4 5 5 2 1 2 1 5 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ female         : num [1:1690] 1 1 0 0 1 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ whitecollar    : num [1:1690] 1 1 1 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ lowincome      : num [1:1690] 4 4 5 4 3 5 2 5 5 5 ...\n  ..- attr(*, \"label\")= chr \"How serious do you think low income of salaryman?\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income         : num [1:1690] 8 7 8 5 5.5 9 1 10 2 5.5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income_nm      : num [1:1690] 8 7 8 5 NA 9 1 10 2 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ age            : num [1:1690] 59 39 63 55 76 64 75 54 64 59 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ KMT            : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ DPP            : num [1:1690] 0 0 0 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ npp            : num [1:1690] 0 0 0 0 0 0 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ noparty        : num [1:1690] 1 1 0 1 1 0 1 0 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pfp            : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ South          : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ north          : num [1:1690] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Minnan_father  : num [1:1690] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Mainland_father: num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Econ_worse     : num [1:1690] 0 0 1 1 0 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Inequality     : num [1:1690] 1 1 1 1 0 1 0 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ inequality5    : num [1:1690] 4 5 5 5 3 5 3 5 5 5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ econworse5     : num [1:1690] 3 3 4 5 3 4 4 5 5 5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Govt_for_public: num [1:1690] 1 1 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pubwelf5       : num [1:1690] 5 5 4 1 3 2 2 1 3 2 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Govt_dont_care : num [1:1690] 0 0 1 1 0 1 1 1 0 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ highincome     : num [1:1690] 1 1 1 1 NA 1 0 1 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt        : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt_nm     : num [1:1690] NA 0 1 NA NA 0 0 0 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Blue           : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Green          : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ No_Party       : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ voteblue       : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ voteblue_nm    : num [1:1690] NA 0 1 NA NA 0 0 0 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votedpp_1      : num [1:1690] NA 1 0 NA NA 1 1 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt_1      : num [1:1690] NA 0 1 NA NA 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n\ndim(TEDS_2016)    # Get dimensions (rows, columns)\n\n[1] 1690   54\n\nnames(TEDS_2016)  # Column names\n\n [1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\"      \n\n# Are there any missing values?\n# Total number of NA in dataset\nsum(is.na(TEDS_2016))\n\n[1] 3008\n\ncolSums(is.na(TEDS_2016))\n\n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2        votetsai           green \n              0               0               0             429               0 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n            429             248               0               0               0 \n      Taiwanese             edu          female     whitecollar       lowincome \n              0              10               0               0               0 \n         income       income_nm             age             KMT             DPP \n              0             330               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n              0               0               0               0             330 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n              0             429               0               0               0 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n              0             429             187             187 \n\n\n\n\nWhat problems do you encounter when working with the dataset?\n\n\n\nFor this dataset, there are many ‘NA’ in the data. Primarily the NA are in the ‘vote…’ data.\nThere are also factor variables coded as numeric variables.\n\n\n# Total number of NA in dataset\nsum(is.na(TEDS_2016))\n\n[1] 3008\n\nsort(colSums(is.na(TEDS_2016)), decreasing = TRUE)\n\n       votetsai     votetsai_nm      votekmt_nm     voteblue_nm       income_nm \n            429             429             429             429             330 \n     highincome    votetsai_all       votedpp_1       votekmt_1             edu \n            330             248             187             187              10 \n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2           green    Independence \n              0               0               0               0               0 \n    Unification              sq       Taiwanese          female     whitecollar \n              0               0               0               0               0 \n      lowincome          income             age             KMT             DPP \n              0               0               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care         votekmt \n              0               0               0               0               0 \n           Blue           Green        No_Party        voteblue \n              0               0               0               0 \n\n# Visualize the missing data\nlibrary(ggplot2)\nlibrary(naniar)  # install.packages(\"naniar\")\ngg_miss_var(TEDS_2016) +  theme(axis.text.y = element_text(size = 6, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nHow to deal with missing values?\n\n\n\nHere are two methods for handling missing values:\n\nIgnore\n\nCompletely remove all data points with missing values. Not ideal with this data set.\nJust ignore the missing values in calculations.\n\nReplace Fill in the missing value with a substitute value, such as\n\nFill in with a median value.\nFill in an obvious, but fitting value.\n\n\nIn this case, probably best to ignore the NA in calculations. This is usually with a flag like rm.NA=TRUE or in glm() it is na.action=na.omit or na.action=na.exclude. According to The Companion to Applied Regression: “We encourage the use of na.exclude () for modeling and other functions that work with it.”\n\n\n\n(Next step) Explore the relationship between Tondu and other variables including female, DPP, age, income, edu, Taiwanese and Econ_worse. What methods would you use?\n\n\n\nTo see the relationship between the variables, I will:\n\nReview the summary statistics\nPlot a scatterplot matrix\nRun a generalized linear model on the data\nBox plots for categorical versus quantitative variables\nMosaic plots to compare categorical variables\n\nNone of the variables below have NA as is shown above.\nOf course, this is only the beginning, and not every variable has had a pairwise comparison.\n\n\nsummary(TEDS_2016[,c(\"Tondu\",\"female\",\"DPP\",\"age\",\"income\",\"edu\",\"Taiwanese\",\"Econ_worse\")])\n\n     Tondu           female            DPP              age        \n Min.   :1.000   Min.   :0.0000   Min.   :0.0000   Min.   : 20.00  \n 1st Qu.:3.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 35.00  \n Median :4.000   Median :0.0000   Median :0.0000   Median : 49.00  \n Mean   :4.127   Mean   :0.4864   Mean   :0.3497   Mean   : 49.11  \n 3rd Qu.:5.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 61.00  \n Max.   :9.000   Max.   :1.0000   Max.   :1.0000   Max.   :100.00  \n                                                                   \n     income            edu          Taiwanese        Econ_worse    \n Min.   : 1.000   Min.   :1.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 5.500   Median :3.000   Median :1.0000   Median :1.0000  \n Mean   : 5.324   Mean   :3.301   Mean   :0.6272   Mean   :0.5544  \n 3rd Qu.: 7.000   3rd Qu.:5.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :5.000   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :10                                       \n\n# Scatterplor Matrix\ncar::scatterplotMatrix(~Tondu + female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016)\n\n\n\n\n\n\n\ntondu_glm &lt;- glm(Tondu ~ female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016)\n\n# Summary of GLM\nsummary(tondu_glm)\n\n\nCall:\nglm(formula = Tondu ~ female + DPP + age + income + edu + Taiwanese + \n    Econ_worse, data = TEDS_2016)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.204138   0.274188  15.333  &lt; 2e-16 ***\nfemale       0.368598   0.082680   4.458 8.82e-06 ***\nDPP          0.121223   0.091598   1.323    0.186    \nage          0.001668   0.003143   0.531    0.596    \nincome      -0.025558   0.015862  -1.611    0.107    \nedu         -0.181096   0.036824  -4.918 9.61e-07 ***\nTaiwanese    0.768943   0.091437   8.410  &lt; 2e-16 ***\nEcon_worse  -0.248545   0.083640  -2.972    0.003 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.780522)\n\n    Null deviance: 5208.2  on 1679  degrees of freedom\nResidual deviance: 4649.0  on 1672  degrees of freedom\n  (10 observations deleted due to missingness)\nAIC: 6495.6\n\nNumber of Fisher Scoring iterations: 2\n\n# Relationship between 'Tondu' and 'DPP'\n# Example of Comparison of categorical variables\n\ntable(TEDS_2016$Tondu, TEDS_2016$DPP)\n\n   \n      0   1\n  1  26   1\n  2 147  33\n  3 378 168\n  4 256  72\n  5 144 236\n  6  38  70\n  9 110  11\n\nchisq.test(table(TEDS_2016$Tondu, TEDS_2016$DPP))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(TEDS_2016$Tondu, TEDS_2016$DPP)\nX-squared = 263.17, df = 6, p-value &lt; 2.2e-16\n\n# Boxplot for quantitative vs categorical data example\nboxplot(income ~ Tondu, data = TEDS_2016,\n        main = \"Income by Tondu Preference\",\n        xlab = \"Tondu\",\n        ylab = \"Income\",\n        col = viridis::viridis(length(unique(TEDS_2016$Tondu))),\n        las = 2)  # Rotate labels for readability\n\n\n\n\n\n\n\n# Another Boxplot for quantitative vs categorical data example\nboxplot(age ~ Tondu, data = TEDS_2016,\n        main = \"Age by Tondu Preference\",\n        xlab = \"Tondu\",\n        ylab = \"Age\",\n        col = viridis::viridis(length(unique(TEDS_2016$Tondu))),\n        las = 2)  # Rotate labels for readability\n\n\n\n\n\n\n\n# A mosaic plot to compare categorical variables\nmosaic(~ Tondu + DPP, data = TEDS_2016, shade = TRUE)\n\n\n\n\n\n\n\n\n\n\n(Next step) How about the votetsai variable (vote for DPP candidate Tsai Ing-wen)?\n\n\n\nTo see the relationship between the variables, I will perform the same analysis as above, but control for all the NA:\n\nReview the summary statistics\nPlot a scatterplot matrix\nRun a generalized linear model on the data\nBox plots for categorical versus quantitative variables\nMosaic plots to compare categorical variables\n\nOf course, this is only the beginning, and not every variable has had a pairwise comparison.\n\n\nsummary(TEDS_2016[,c(\"votetsai\",\"female\",\"DPP\",\"age\",\"income\",\"edu\",\"Taiwanese\",\"Econ_worse\")])\n\n    votetsai          female            DPP              age        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   : 20.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 35.00  \n Median :1.0000   Median :0.0000   Median :0.0000   Median : 49.00  \n Mean   :0.6265   Mean   :0.4864   Mean   :0.3497   Mean   : 49.11  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 61.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :100.00  \n NA's   :429                                                        \n     income            edu          Taiwanese        Econ_worse    \n Min.   : 1.000   Min.   :1.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 5.500   Median :3.000   Median :1.0000   Median :1.0000  \n Mean   : 5.324   Mean   :3.301   Mean   :0.6272   Mean   :0.5544  \n 3rd Qu.: 7.000   3rd Qu.:5.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :5.000   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :10                                       \n\ncar::scatterplotMatrix(~votetsai + female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016)\n\n\n\n\n\n\n\nvotetsai_glm &lt;- glm(votetsai ~ female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016, na.action=na.exclude)\n\nsummary(votetsai_glm)\n\n\nCall:\nglm(formula = votetsai ~ female + DPP + age + income + edu + \n    Taiwanese + Econ_worse, data = TEDS_2016, na.action = na.exclude)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.4408344  0.0727487   6.060  1.8e-09 ***\nfemale      -0.0256167  0.0209882  -1.221  0.22250    \nDPP          0.4798515  0.0228743  20.978  &lt; 2e-16 ***\nage         -0.0021676  0.0008307  -2.609  0.00918 ** \nincome      -0.0035722  0.0039547  -0.903  0.36655    \nedu         -0.0232000  0.0095903  -2.419  0.01570 *  \nTaiwanese    0.2531388  0.0239856  10.554  &lt; 2e-16 ***\nEcon_worse   0.0636800  0.0214794   2.965  0.00309 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.134158)\n\n    Null deviance: 294.26  on 1256  degrees of freedom\nResidual deviance: 167.56  on 1249  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 1052.2\n\nNumber of Fisher Scoring iterations: 2\n\ntable(TEDS_2016$votetsai, TEDS_2016$DPP)\n\n   \n      0   1\n  0 454  17\n  1 276 514\n\nchisq.test(table(TEDS_2016$votetsai, TEDS_2016$DPP))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(TEDS_2016$votetsai, TEDS_2016$DPP)\nX-squared = 454.62, df = 1, p-value &lt; 2.2e-16\n\n# Boxplot for quantitative vs categorical data example\nboxplot(income ~ votetsai, data = TEDS_2016,\n        main = \"Income by votetsai Preference\",\n        xlab = \"votetsai\",\n        ylab = \"Income\",\n        col = viridis::viridis(length(unique(TEDS_2016$votetsai))),\n        las = 2)  # Rotate labels for readability\n\n\n\n\n\n\n\n# Another Boxplot for quantitative vs categorical data example\nboxplot(age ~ votetsai, data = TEDS_2016,\n        main = \"Age by votetsai Preference\",\n        xlab = \"votetsai\",\n        ylab = \"Age\",\n        col = viridis::viridis(length(unique(TEDS_2016$votetsai))),\n        las = 2)  # Rotate labels for readability\n\n\n\n\n\n\n\n# A mosaic plot to compare categorical variables\nmosaic(~ votetsai + DPP, data = TEDS_2016, shade = TRUE)\n\n\n\n\n\n\n\n\n\n\n(Next step) Generate frequency table and barchart of the Tondu variable. Assign labels to the variable using the following:\n\n\nTEDS_2016$Tondu&lt;-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”,\n“Status quo, unif. in future”, “Status quo, decide later\", \"Status quo\nforever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\nNote that the dataset seems to have changed so that adding the labels is no longer necessary. The labels could be added with:\nTEDS_2016$Tondu &lt;- labelled(TEDS_2016$Tondu, \n                            c(\"Unification now\" = 1, \n                              \"Status quo, unif. in future\" = 2, \n                              \"Status quo, decide later\" = 3, \n                              \"Status quo forever\" = 4, \n                              \"Status quo, indep. in future\" = 5, \n                              \"Independence now\" = 6, \n                              \"No response\" = 9))\n\n\n\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6   9 \n 27 180 546 328 380 108 121 \n\n\n\n\nFrequency Table\n\n\n#head(TEDS_2016$Tondu)\n#tail(TEDS_2016$Tondu)\n\nTEDS_2016 %&gt;%\n  count(Tondu) %&gt;%\n  mutate(Percent = n / sum(n) * 100)\n\n# A tibble: 7 × 3\n  Tondu                                                                n Percent\n  &lt;dbl+lbl&gt;                                                        &lt;int&gt;   &lt;dbl&gt;\n1 1 [Immediate unification]                                           27    1.60\n2 2 [Maintain the status quo,move toward unification]                180   10.7 \n3 3 [Maintain the status quo, decide either unification or indepe…   546   32.3 \n4 4 [Maintain the status quo forever]                                328   19.4 \n5 5 [Maintain the status quo,move toward independence]               380   22.5 \n6 6 [Immediate independence]                                         108    6.39\n7 9 [Nonresponse]                                                    121    7.16\n\n\n\n# Compute frequencies\n\nTondu_freq &lt;- TEDS_2016 %&gt;%\n  count(Tondu) %&gt;%\n  mutate(Percent = n / sum(n) * 100)\n\n\n# Convert Tondu to factor (preserve labels)\nTEDS_2016$Tonduf &lt;- as_factor(TEDS_2016$Tondu)\n\n# Compute frequencies\nTondu_freq &lt;- table(TEDS_2016$Tonduf)  # Count occurrences\nPercent &lt;- prop.table(Tondu_freq) * 100  # Convert to percentages\nQuantity &lt;- as.vector(Tondu_freq)  # Extract counts as a numeric vector\n\n# Percentage bar chart\nbarplot(Percent,\n        names.arg = names(Tondu_freq),  # Use category labels\n        main = \"Distribution of Tondu Preferences Percentages\",\n        ylab = \"Percentage\",\n        col = viridis::viridis(length(Percent)),  # Assign colors\n        las = 2,  # Rotate x-axis labels for readability\n        cex.names = 0.5)  # Adjust text \n\n\n\n\n\n\n\n# Count bar chart\nbarplot(Quantity,\n        names.arg = names(Tondu_freq),  # Use category labels\n        main = \"Distribution of Tondu Preferences Count\",\n        ylab = \"Count\",\n        col = viridis::viridis(length(Percent)),  # Assign colors\n        las = 2,  # Rotate x-axis labels for readability\n        cex.names = 0.5)  # Adjust text"
  },
  {
    "objectID": "Work.html",
    "href": "Work.html",
    "title": "Work",
    "section": "",
    "text": "Work samples will go here.\n\n\n\nPrincess Donut"
  },
  {
    "objectID": "comparison01.html",
    "href": "comparison01.html",
    "title": "Comparison of Shmueli & Breiman",
    "section": "",
    "text": "Comparison of “To Explain or to Predict?” by Galit Shmueli and “Statistical Modeling: The Two Cultures” by Leo Breiman\n\nBackground\nThe two papers from Breiman and Shmueli are both critiques of traditional modeling approaches. Breiman advocates for shifting from traditional parametric modeling to algorithmic modeling. Schmueli advocates differentiating between explanatory modeling and predictive modeling.\n\n\nBreiman\nThe paper by Breiman (Breiman, 2001) critiques the “data modeling culture” in the field of traditional statistics. He argues that there has been an overreliance on statistical models and interpretability at the expense of algorithmic models, which favor predictability. “Usually, simple parametric models imposed on data generated by complex systems…result in a loss of accuracy and information as compared to algorithmic models.” This has led to models that may not capture real-world data complexities and assume an underlying data generation process that may not exist.\nThrough several examples, he examines the use of traditional modeling as compared to algorithmic modeling. His conclusion is that the “emphasis needs to be on the problem and not the data.”\nAt the end of this paper, there are comments and follow-up by some noted statisticians, which generally agree (except for Cox) with Breiman but bring about some interesting caveats. For instance, Hoadley warns that when creating algorithmic models, care should be taken not to overfit models.\n\n\nShmueli\nThe paper by Shmueli (Shmueli, 2010) looks at the difference between explanatory and predictive modeling, outlining that each supports different goals. Explanatory modeling tests causal hypotheses to describe the underlying mechanisms. Predictive modeling, according to Shmueli, “predictive modeling [is] the process of applying a statistical model or data mining algorithm to data for the purpose of predicting new or future observations.”\nThe paper emphasizes that understanding the difference between the two models is critical to proper model selection during the process. Understanding this difference can impact the study design, data collection, data processing, variable choices, methodologies, and nearly everything else in the modeling process.\nIn the end, it is suggested that it is important to be aware of how models are used and to acknowledge the difference between explanatory, predictive, and descriptive modeling.\n\n\nComparison\nBoth of these papers point out that it is important to distinguish between the different modeling methods while highlighting the strengths and weaknesses of each. They also both acknowledge the growing acceptance and usefulness of black-box predictive modeling for data science.\nWhile both support the role of modeling, Breiman advocates primarily for predictive modeling, and Shmueli advocates for making a conscious choice between the two. Shmueli also cautions against assuming that a strong explanatory model means it will have good predictive performance. Breiman states that the historical modeling focus has kept statisticians from effectively modeling real problems.\n\n\nConclusion\nThese papers discuss the evolving landscape of statistical modeling, urging researchers to be more deliberate in their methodological choices and recognize the trade-offs between explanation and prediction. Although both of these papers are decades old, the conclusions remain relevant.\n\n\nBibliography\nBreiman, Leo. 2001. “Statistical modeling: The two cultures (with comments and a rejoinder by the author)”. Statistical Science, 16(3), pp.199-231.\nShmueli, Galit. 2010. “To explain or to predict?.” Statistical Science 25, no. 3: 289-310."
  },
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/n9/9b7__npd72z18bl4l57tvj4m0000gn/T//Rtmpb55ac2/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#indexing-data-using",
    "href": "Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "Lab02.html#loading-data-from-github-remote",
    "href": "Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "Lab02.html#load-data-from-islr-website",
    "href": "Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "Lab02.html#linear-regression",
    "href": "Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/n9/9b7__npd72z18bl4l57tvj4m0000gn/T//Rtmpb55ac2/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Lab02.html#multiple-linear-regression",
    "href": "Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#qualitative-predictors",
    "href": "Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9944501\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9944501\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "EPPS6323.html",
    "href": "EPPS6323.html",
    "title": "EPPS6323 Files",
    "section": "",
    "text": "library(ggplot2)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]"
  },
  {
    "objectID": "EPPS6323.html#assignment-1-more-penguins",
    "href": "EPPS6323.html#assignment-1-more-penguins",
    "title": "EPPS6323 Files",
    "section": "",
    "text": "library(ggplot2)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]"
  },
  {
    "objectID": "assignment01.html",
    "href": "assignment01.html",
    "title": "Assignment 01",
    "section": "",
    "text": "Review/refresh R programming:"
  },
  {
    "objectID": "assignment01.html#exercise-1",
    "href": "assignment01.html#exercise-1",
    "title": "Assignment 01",
    "section": "Exercise 1",
    "text": "Exercise 1\nAfter installation, type R.version. What version of R did you install? What is the nickname of that particular software build?\n\nR.version\n\n               _                           \nplatform       aarch64-apple-darwin20      \narch           aarch64                     \nos             darwin20                    \nsystem         aarch64, darwin20           \nstatus                                     \nmajor          4                           \nminor          4.2                         \nyear           2024                        \nmonth          10                          \nday            31                          \nsvn rev        87279                       \nlanguage       R                           \nversion.string R version 4.4.2 (2024-10-31)\nnickname       Pile of Leaves"
  },
  {
    "objectID": "assignment01.html#exercise-2",
    "href": "assignment01.html#exercise-2",
    "title": "Assignment 01",
    "section": "Exercise 2",
    "text": "Exercise 2\nOpen RStudio. In the console pane (usually at the bottom-left), type 1 + 1 and press Enter. What result do you get?\n\n1+1\n\n[1] 2"
  },
  {
    "objectID": "assignment01.html#exercise-3",
    "href": "assignment01.html#exercise-3",
    "title": "Assignment 01",
    "section": "Exercise 3",
    "text": "Exercise 3\nCreate a new R script (File &gt; New File &gt; R Script). Type print(“Hello, Data Science!”) and run the code. What output do you see in the console?\n\nprint(\"Hello, Data Science!\")\n\n[1] \"Hello, Data Science!\""
  },
  {
    "objectID": "assignment01.html#exercise-4",
    "href": "assignment01.html#exercise-4",
    "title": "Assignment 01",
    "section": "Exercise 4",
    "text": "Exercise 4\nUse pacman to install and load the tidyr package. Then, use p_functions() to list all functions in the tidyr package.\n\nlibrary(pacman)\np_functions(tidyr)\n\n [1] \"%&gt;%\"                      \"all_of\"                  \n [3] \"any_of\"                   \"as_tibble\"               \n [5] \"build_longer_spec\"        \"build_wider_spec\"        \n [7] \"check_pivot_spec\"         \"chop\"                    \n [9] \"complete\"                 \"complete_\"               \n[11] \"contains\"                 \"crossing\"                \n[13] \"crossing_\"                \"drop_na\"                 \n[15] \"drop_na_\"                 \"ends_with\"               \n[17] \"everything\"               \"expand\"                  \n[19] \"expand_\"                  \"expand_grid\"             \n[21] \"extract\"                  \"extract_\"                \n[23] \"extract_numeric\"          \"fill\"                    \n[25] \"fill_\"                    \"full_seq\"                \n[27] \"gather\"                   \"gather_\"                 \n[29] \"hoist\"                    \"last_col\"                \n[31] \"matches\"                  \"nest\"                    \n[33] \"nest_\"                    \"nest_legacy\"             \n[35] \"nesting\"                  \"nesting_\"                \n[37] \"num_range\"                \"one_of\"                  \n[39] \"pack\"                     \"pivot_longer\"            \n[41] \"pivot_longer_spec\"        \"pivot_wider\"             \n[43] \"pivot_wider_spec\"         \"replace_na\"              \n[45] \"separate\"                 \"separate_\"               \n[47] \"separate_longer_delim\"    \"separate_longer_position\"\n[49] \"separate_rows\"            \"separate_rows_\"          \n[51] \"separate_wider_delim\"     \"separate_wider_position\" \n[53] \"separate_wider_regex\"     \"spread\"                  \n[55] \"spread_\"                  \"starts_with\"             \n[57] \"tibble\"                   \"tidyr_legacy\"            \n[59] \"tribble\"                  \"unchop\"                  \n[61] \"uncount\"                  \"unite\"                   \n[63] \"unite_\"                   \"unnest\"                  \n[65] \"unnest_\"                  \"unnest_auto\"             \n[67] \"unnest_legacy\"            \"unnest_longer\"           \n[69] \"unnest_wider\"             \"unpack\""
  },
  {
    "objectID": "assignment01.html#exercise-5",
    "href": "assignment01.html#exercise-5",
    "title": "Assignment 01",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate a new folder on your computer called “DataScience”. Set this as your working directory in RStudio. Then, use getwd() to confirm it’s set correctly.\nNote: This was done, but running this in Quarto is problematic, so this output is copied from the Console.\n&gt; getwd()\n[1] \"/Users/john/DataScience\""
  },
  {
    "objectID": "assignment01.html#exercise-1-1",
    "href": "assignment01.html#exercise-1-1",
    "title": "Assignment 01",
    "section": "Exercise 1",
    "text": "Exercise 1\nCreate two variables c and d with values of your choice. Perform all the above operations on these variables and print the results.\n\nc &lt;- 11\nd &lt;- 13\n\nsum &lt;- c + d\ndifference &lt;- c - d\nproduct &lt;- c * d\nquotient &lt;- c / d\npower &lt;- c ^ d\nmodulus &lt;- c %% d\n\nprint(sum)\n\n[1] 24\n\nprint(difference)\n\n[1] -2\n\nprint(product)\n\n[1] 143\n\nprint(quotient)\n\n[1] 0.8461538\n\nprint(power)\n\n[1] 3.452271e+13\n\nprint(modulus)\n\n[1] 11"
  },
  {
    "objectID": "assignment01.html#exercise-2-1",
    "href": "assignment01.html#exercise-2-1",
    "title": "Assignment 01",
    "section": "Exercise 2",
    "text": "Exercise 2\nCreate variables of each data type we’ve discussed so far (numeric, character, logical). Use the class() function to verify their types.\n\nnumeric_vector &lt;- rnorm(5)\ncharacter_vector &lt;- c(\"carbon\", \"hydrogen\", \"oxygen\")\nlogical_vector &lt;- c(FALSE, FALSE, TRUE, TRUE)\n\nprint(numeric_vector)\n\n[1] -0.4330667 -1.2942505 -1.5775342 -0.4266627  0.8155104\n\nprint(character_vector)\n\n[1] \"carbon\"   \"hydrogen\" \"oxygen\"  \n\nprint(logical_vector)\n\n[1] FALSE FALSE  TRUE  TRUE\n\ncolors &lt;- factor(c(\"red\", \"cornflowerblue\", \"green\", \"red3\", \"pink\"))\nprint(colors)\n\n[1] red            cornflowerblue green          red3           pink          \nLevels: cornflowerblue green pink red red3\n\nlevels(colors)\n\n[1] \"cornflowerblue\" \"green\"          \"pink\"           \"red\"           \n[5] \"red3\"          \n\nclass(numeric_vector)\n\n[1] \"numeric\"\n\nclass(character_vector)\n\n[1] \"character\"\n\nclass(logical_vector)\n\n[1] \"logical\"\n\nclass(colors)\n\n[1] \"factor\""
  },
  {
    "objectID": "assignment01.html#exercise-3-1",
    "href": "assignment01.html#exercise-3-1",
    "title": "Assignment 01",
    "section": "Exercise 3",
    "text": "Exercise 3\nThere is no Exercise 3."
  },
  {
    "objectID": "assignment01.html#exercise-4-1",
    "href": "assignment01.html#exercise-4-1",
    "title": "Assignment 01",
    "section": "Exercise 4",
    "text": "Exercise 4\nCreate a vector of numbers from 1 to 10. Then, use indexing to:\n\nExtract the 5th element\nExtract all elements except the 3rd\nExtract the 2nd, 4th, and 6th elements\n\n\n# Create a vector of numbers from 1 to 10\nnumbers &lt;- 1:10\n\n# Extract the 5th element\nfifth &lt;- numbers[5]\n\n# Extract all elements except the 3rd\nnot_third &lt;- numbers[-3]\n\n# Extract the 2nd, 4th, and 6th elements\nevens &lt;- numbers[c(2, 4, 6)]\n\n# Print results\nprint(numbers)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nprint(fifth)\n\n[1] 5\n\nprint(not_third)\n\n[1]  1  2  4  5  6  7  8  9 10\n\nprint(evens)\n\n[1] 2 4 6"
  },
  {
    "objectID": "assignment01.html#exercise-5-1",
    "href": "assignment01.html#exercise-5-1",
    "title": "Assignment 01",
    "section": "Exercise 5",
    "text": "Exercise 5\nWrite a conditional statement that checks if a number is positive, negative, or zero, and prints an appropriate message for each case.\n\n# Define a number\nnum &lt;- sample(-10:10, 1)  # generate number\nprint(num)\n\n[1] 3\n\n# Conditional statement to check if the number is positive, negative, or zero\nif (num &gt; 0) {\n  print(\"The number is positive.\")\n} else if (num &lt; 0) {\n  print(\"The number is negative.\")\n} else {\n  print(\"The number is zero.\")\n}\n\n[1] \"The number is positive.\""
  },
  {
    "objectID": "assignment01.html#exercise-6",
    "href": "assignment01.html#exercise-6",
    "title": "Assignment 01",
    "section": "Exercise 6",
    "text": "Exercise 6\nCreate a vector of 5 numbers and a vector of 5 names. Combine them into a data frame where each number corresponds to an age and each name corresponds to a person. Then, calculate the mean age and display a summary of the data frame.\n\n# Create a vector of 5 ages\nages &lt;- c(25, 30, 11, 28, 55)\n\n# Create a vector of 5 names\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\")\n\n# Combine into a data frame\npeople_df &lt;- data.frame(Name = names, Age = ages)\n\n# Calculate the mean age\nmean_age &lt;- mean(people_df$Age)\n\n# Display the summary of the data frame\nsummary(people_df)\n\n     Name                Age      \n Length:5           Min.   :11.0  \n Class :character   1st Qu.:25.0  \n Mode  :character   Median :28.0  \n                    Mean   :29.8  \n                    3rd Qu.:30.0  \n                    Max.   :55.0  \n\n# Print the mean age\nprint(paste(\"The mean age is:\", mean_age))\n\n[1] \"The mean age is: 29.8\""
  },
  {
    "objectID": "assignment01.html#exercise-6---advanced",
    "href": "assignment01.html#exercise-6---advanced",
    "title": "Assignment 01",
    "section": "Exercise 6 - Advanced",
    "text": "Exercise 6 - Advanced\nCreate a custom function that takes a vector of numbers as input and returns a list containing the following: 1. The square of each number in the vector. 2. A count of how many numbers in the vector are greater than a specified threshold. 3. The mean of the numbers in the vector, but only include numbers greater than a specified threshold in the calculation.\nTest your function with a vector of random numbers, using a threshold of your choice.\n\n# Define the custom function\nvector_out &lt;- function(numbers, target) {\n  squared_num &lt;- numbers^2\n  target_count &lt;- sum(numbers &gt; target)\n  target_mean &lt;- mean(numbers[numbers &gt; target])\n  \n  # Return a list with results\n  return(list(\n    Squared_Numbers = squared_num,\n    Count_Above_Target = target_count,\n    Mean_Above_Target = target_mean\n  ))\n}\n\n# Generate a vector of 10 random numbers between 1 and 50\nset.seed(111)  # Set seed for reproducibility\nrandom_numbers &lt;- sample(1:100, 10, replace = TRUE)\nprint(random_numbers)\n\n [1] 78 84 83 47 25 59 69 35 72 26\n\n# Define a threshold value\ntarget_value &lt;- median(random_numbers)\n\n# Call the function with the generated vector and threshold\nresult &lt;- vector_out(random_numbers, target_value)\n\n# Print the results\nprint(result)\n\n$Squared_Numbers\n [1] 6084 7056 6889 2209  625 3481 4761 1225 5184  676\n\n$Count_Above_Target\n[1] 5\n\n$Mean_Above_Target\n[1] 77.2"
  },
  {
    "objectID": "assignment01.html#part-1",
    "href": "assignment01.html#part-1",
    "title": "Assignment 01",
    "section": "Part 1",
    "text": "Part 1\n\nExercise 1\nVector Manipulation\n\nCreate a vector of 5 superhero ages.\nIncrease all ages by 2 years.\nFind which heroes are older than 40 after the increase.\nCreate a logical vector indicating if each hero is from DC (assume the first 3 are from DC).\n\n\n# Ages\nsuper_ages &lt;- c(40, 21, 132, 55, 33)\nplus_two &lt;- super_ages + 2\nprint(plus_two)\n\n[1]  42  23 134  57  35\n\n# Over 40\nis_old &lt;- plus_two &gt; 40\nprint(is_old)\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE\n\n# Publisher\n\npub_vector &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE)\nprint(pub_vector)\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\nclass(pub_vector)\n\n[1] \"logical\"\n\n\n\n\nExercise 2\nMatrix Manipulation\n\nCreate a 3x3 matrix of hero power levels (strength, speed, intelligence) for three new heroes.\nCalculate the average power level for each hero.\nFind which hero has the highest strength.\nScale all power levels by 1.5 and round to the nearest integer.\n\n\nsuper_df &lt;- data.frame(\n  Strength = c(110, 20, 180),\n  Speed = c(25, 95, 130),\n  Intelligence = c(19, 150, 91)\n)\n\nprint(super_df)\n\n  Strength Speed Intelligence\n1      110    25           19\n2       20    95          150\n3      180   130           91\n\n# Average for individuals\nprint(\"The average for individual superheros are:\")\n\n[1] \"The average for individual superheros are:\"\n\nprint(rowMeans(super_df[, 1:3]))\n\n[1]  51.33333  88.33333 133.66667\n\n# Average for type of power\nprint(\"The average for the categories are:\")\n\n[1] \"The average for the categories are:\"\n\nprint(paste(\"Strength:  \", mean(super_df$Strength))) \n\n[1] \"Strength:   103.333333333333\"\n\nprint(paste(\"Speed:  \",mean(super_df$Speed)))\n\n[1] \"Speed:   83.3333333333333\"\n\nprint(paste(\"Intelligence:  \",mean(super_df$Intelligence)))\n\n[1] \"Intelligence:   86.6666666666667\"\n\n# High Strength\nsuper_strength &lt;- super_df[which.max(super_df$Strength), ]\nprint(super_strength)\n\n  Strength Speed Intelligence\n3      180   130           91\n\n# Scale and round\nsuper_round &lt;- round(1.5*super_df)\nprint(super_round)\n\n  Strength Speed Intelligence\n1      165    38           28\n2       30   142          225\n3      270   195          136\n\n\n\n\nExercise 3\nData Frame Manipulation\n\nAdd a “PowerLevel” column that’s the average of Strength, Intelligence, and Speed.\nFilter the data frame to show only heroes with a PowerLevel above 85.\nSort the heroes by PowerLevel in descending order.\nCreate a new data frame with only the Name and PowerLevel columns for non-Marvel heroes.\n\n\n# Add a column for average power\nsuper_df$PowerLevel &lt;- rowMeans(super_df[, 1:3])\nprint(super_df)\n\n  Strength Speed Intelligence PowerLevel\n1      110    25           19   51.33333\n2       20    95          150   88.33333\n3      180   130           91  133.66667\n\n# Filter data for PowerLevel &gt; 85\ntemp_df &lt;- super_df[super_df$PowerLevel &gt; 85,]\nprint(temp_df)\n\n  Strength Speed Intelligence PowerLevel\n2       20    95          150   88.33333\n3      180   130           91  133.66667\n\n# Sort by PowerLevel (descending)\ntemp_df &lt;- super_df[order(-super_df$PowerLevel), ]\nprint(temp_df)\n\n  Strength Speed Intelligence PowerLevel\n3      180   130           91  133.66667\n2       20    95          150   88.33333\n1      110    25           19   51.33333\n\n# Add character type to df\npublisher &lt;- c(\"DC\", \"Dark Horse\", \"Marvel\")\nsuper_df$Publisher &lt;- publisher\nprint (super_df)\n\n  Strength Speed Intelligence PowerLevel  Publisher\n1      110    25           19   51.33333         DC\n2       20    95          150   88.33333 Dark Horse\n3      180   130           91  133.66667     Marvel\n\n# No Marvel df\ntemp_df &lt;- subset(super_df, Publisher != \"Marvel\")\nsuper_df_nomarvel &lt;- data.frame(Name = c(\"Batman\", \"Hellboy\"), \n                                PowerLevel = temp_df$PowerLevel)\nprint(super_df_nomarvel)\n\n     Name PowerLevel\n1  Batman   51.33333\n2 Hellboy   88.33333"
  },
  {
    "objectID": "assignment01.html#part-2",
    "href": "assignment01.html#part-2",
    "title": "Assignment 01",
    "section": "Part 2",
    "text": "Part 2\n\nExercise 1\n\nCreate a numeric vector containing the numbers 10, 20, 30, 40, and 50.\nCreate a character vector containing the names “Alice”, “Bob”, “Charlie”, “David”, and “Eve”.\nUse indexing to retrieve the third element from each of these vectors.\nModify the second element in the numeric vector to be 25.\nCalculate the sum of all elements in the numeric vector.\n\n\n# Numeric vector\nnum_vector &lt;- c(10, 20, 30, 40, 50)\n\n# Character vector\nchar_vector &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\")\n\n# Indexing\nnum_vector[3]\n\n[1] 30\n\nchar_vector[3]\n\n[1] \"Charlie\"\n\n# Second Element\nnum_vector[2] &lt;- 25\nprint(num_vector)\n\n[1] 10 25 30 40 50\n\n# Sum\nsum(num_vector)\n\n[1] 155\n\n\n\n\nExercise 2\nWorking with Factors\nObjective: Understand how to create and manipulate factors in R.\nInstructions:\n\nCreate a factor variable from the following vector: c(“low”, “medium”, “high”, “low”, “medium”, “high”).\nDisplay the levels of the factor variable.\nConvert the factor levels to an ordered factor where “low” &lt; “medium” &lt; “high”.\nCreate a bar plot to visualize the frequency of each level.\n\n\n# Create factor variable\nfactor_var &lt;- factor(c(\"low\", \"medium\", \"high\", \"low\", \"medium\", \"high\"))\n\nlevels(factor_var)\n\n[1] \"high\"   \"low\"    \"medium\"\n\n# Ordered factor\nord_factor &lt;- factor(factor_var, \n                     levels = c(\"low\", \"medium\", \"high\"), \n                     ordered = TRUE)\n\n# Create a bar plot\nbarplot(table(ord_factor))\n\n\n\n\n\n\n\n\n\n\nExercise 3\nData Frame Operations Objective: Learn how to create, access, and manipulate data frames.\nInstructions:\n\nCreate a data frame with the following columns: ID (1, 2, 3), Name (“Alice”, “Bob”, “Charlie”), and Score (85, 90, 88).\nAccess the Name column and print it.\nAdd a new column Pass that indicates whether the Score is greater than or equal to 90.\nCalculate the average Score for all students.\n\n\n# Create a data frame\ndf &lt;- data.frame(\n  ID = 1:3,\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Score = c(85, 90, 88)\n)\n\n# Access the Name column\ndf$Name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n# New column\ndf$Pass &lt;- df$Score &gt;= 90\nprint(df)\n\n  ID    Name Score  Pass\n1  1   Alice    85 FALSE\n2  2     Bob    90  TRUE\n3  3 Charlie    88 FALSE\n\n# Calculate the average score\nmean(df$Score)\n\n[1] 87.66667\n\n\n\n\nExercise 4\nText Data Manipulation with stringr Objective: Practice manipulating text data using the stringr package.\nInstructions: 1. Load the stringr package. 2. Create a character string: “The quick brown fox jumps over the lazy dog”. 3. Count the number of words in the string. 4. Extract the word “quick” from the string. 5. Replace the word “lazy” with “energetic”.\n\n# Load the stringr package\nlibrary(stringr)\n\n# Create a string\ntext_string &lt;- \"The quick brown fox jumps over the lazy dog\"\n\n# Count words\nstr_count(text_string, \"\\\\w+\")\n\n[1] 9\n\n# Extract \"quick\"\nstr_extract(text_string, \"quick\")\n\n[1] \"quick\"\n\n# Replace \"lazy\" with \"crazy\"\nstr_replace(text_string, \"lazy\", \"energetic\")\n\n[1] \"The quick brown fox jumps over the energetic dog\"\n\n# Try the following and count again?\ntext_string &lt;- \"The quick brown fox jumps over the lazy dog #\"\nstr_count(text_string, \"\\\\w+\")\n\n[1] 9\n\n\n\n\nExercise 5\nCreating and Analyzing a Document-Term Matrix (DTM) Objective: Learn how to create and analyze a Document-Term Matrix using text data.\nInstructions:\n\nLoad the tm package.\nCreate a small corpus using the following text documents:\n\n“R is a programming language for data analysis.”\n“Data analysis in R is powerful and flexible.”\n“Learning R can be fun and rewarding.”\n\nCreate a Document-Term Matrix (DTM) from the corpus.\nInspect the DTM to see the term frequency matrix.\nIdentify the term with the highest frequency across all documents.\n\n\n# Load tm package\nlibrary(tm)\n\nLoading required package: NLP\n\n# Create a corpus\ndocuments &lt;- c(\"R is a programming language for data analysis.\",\n          \"Data analysis in R is powerful and flexible.\",\n          \"Learning R can be fun and rewarding.\")\ncorpus &lt;- Corpus(VectorSource(documents))\n\n# Create a Document-Term Matrix\ndtm &lt;- DocumentTermMatrix(corpus)\n\n# Inspect the DTM\ninspect(dtm)\n\n&lt;&lt;DocumentTermMatrix (documents: 3, terms: 13)&gt;&gt;\nNon-/sparse entries: 15/24\nSparsity           : 62%\nMaximal term length: 11\nWeighting          : term frequency (tf)\nSample             :\n    Terms\nDocs analysis analysis. and can data flexible. for language powerful\n   1        0         1   0   0    1         0   1        1        0\n   2        1         0   1   0    1         1   0        0        1\n   3        0         0   1   1    0         0   0        0        0\n    Terms\nDocs programming\n   1           1\n   2           0\n   3           0\n\n# Find highest frequency\nterm_frequencies &lt;- colSums(as.matrix(dtm))\nmost_frequent_term &lt;- names(term_frequencies[which.max(term_frequencies)])\nmost_frequent_term\n\n[1] \"data\""
  },
  {
    "objectID": "assignment01.html#basic-statistical-functions---q1",
    "href": "assignment01.html#basic-statistical-functions---q1",
    "title": "Assignment 01",
    "section": "2.1 Basic Statistical Functions - Q1",
    "text": "2.1 Basic Statistical Functions - Q1\n\nCreate a vector data with the following values: c(12, 17, 14, 22, 15, 19, 16).\nCalculate the mean, median, standard deviation, and variance of data.\nDetermine the minimum, maximum, and range of the data.\nGenerate a summary of data.\n\n\n# Create vector\ndata &lt;- c(12, 17, 14, 22, 15, 19, 16)\n\n# Statistics\nmean_data &lt;- mean(data)\nmedian_data &lt;- median(data)\nsd_data &lt;- sd(data)\nvar_data &lt;- var(data)\n\nstat_data &lt;- c(Mean = mean_data, \n               Median = median_data, \n               SD = sd_data, \n               Var = var_data)\n\nprint(stat_data)\n\n     Mean    Median        SD       Var \n16.428571 16.000000  3.309438 10.952381 \n\n# Min, Max, Range\nmin_data &lt;- min(data)\nmax_data &lt;- max(data)\nrange_data &lt;- range(data)\n\nrange_data &lt;- c(Min = min_data, \n               Max = max_data, \n               Range = range_data \n               )\nprint(range_data)\n\n   Min    Max Range1 Range2 \n    12     22     12     22 \n\n# Summary statistics\nsummary_data &lt;- summary(data)\nprint(summary_data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   14.50   16.00   16.43   18.00   22.00"
  },
  {
    "objectID": "assignment01.html#matrix-operations---q2",
    "href": "assignment01.html#matrix-operations---q2",
    "title": "Assignment 01",
    "section": "2.2 Matrix Operations - Q2",
    "text": "2.2 Matrix Operations - Q2\n\nCreate a 2x3 matrix matrix_A with the values c(1, 2, 3, 4, 5, 6).\nCreate another 2x3 matrix matrix_B with the values c(6, 5, 4, 3, 2, 1).\nPerform element-wise addition and subtraction on matrix_A and matrix_B.\nTranspose matrix_A to create matrix_A_T.\nMultiply matrix_A_T by matrix_B (after ensuring their dimensions match).\n\n\n# Creating matrices\nmatrix_A &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\nprint(matrix_A)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nmatrix_B &lt;- matrix(c(6, 5, 4, 3, 2, 1), nrow = 2, ncol = 3)\nprint(matrix_B)\n\n     [,1] [,2] [,3]\n[1,]    6    4    2\n[2,]    5    3    1\n\n# Operations\nmatrix_add &lt;- matrix_A + matrix_B\nprint(matrix_add)\n\n     [,1] [,2] [,3]\n[1,]    7    7    7\n[2,]    7    7    7\n\nmatrix_sub &lt;- matrix_A - matrix_B\nprint(matrix_sub)\n\n     [,1] [,2] [,3]\n[1,]   -5   -1    3\n[2,]   -3    1    5\n\n# Transpose\nmatrix_A_T &lt;- t(matrix_A)\nprint(matrix_A_T)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n# Multiplying AT X BT\nmatrix_mul &lt;- matrix_A_T %*% matrix_B\nprint(matrix_mul)\n\n     [,1] [,2] [,3]\n[1,]   16   10    4\n[2,]   38   24   10\n[3,]   60   38   16"
  },
  {
    "objectID": "assignment01.html#creating-functions---q3",
    "href": "assignment01.html#creating-functions---q3",
    "title": "Assignment 01",
    "section": "2.3 Creating Functions - Q3",
    "text": "2.3 Creating Functions - Q3\n\nWrite a function cube() that calculates the cube of a number.\nCreate a vector numbers with the values c(2, 3, 4), and use the cube() function to find the cube of each element in numbers.\nWrite a function is_positive() that checks if a number is positive, negative, or zero, and returns a corresponding message.\nTest the is_positive() function with the numbers 5, -3, and 0.\n\n\n# Cube Function\ncube &lt;- function(x) {\n  return(x^3)\n}\n\n# Cube a Vector\nnumbers &lt;- c(2, 3, 4)\ncubes &lt;- sapply(numbers, cube)\nprint(cubes)\n\n[1]  8 27 64\n\n# Sign Test\nis_positive &lt;- function(num) {\n  if (num &gt; 0) {\n    return(\"Positive\")\n  } else if (num &lt; 0) {\n    return(\"Negative\")\n  } else {\n    return(\"Zero\")\n  }\n}\n\n# Test is_positive function\nresult_1 &lt;- is_positive(5)  # Should return \"Positive\"\nprint(result_1)\n\n[1] \"Positive\"\n\nresult_2 &lt;- is_positive(-3) # Should return \"Negative\"\nprint(result_2)\n\n[1] \"Negative\"\n\nresult_3 &lt;- is_positive(0)  # Should return \"Zero\"\nprint(result_3)\n\n[1] \"Zero\""
  },
  {
    "objectID": "assignment01.html#q1",
    "href": "assignment01.html#q1",
    "title": "Assignment 01",
    "section": "Q1",
    "text": "Q1\n\n1. Import the Air_Quality data and view the column names.\n\n# 1.1.1 Direct import If the data is in the same folder as the current working directory\n\nair_data &lt;- read.csv(\"Air_Quality.csv\")\n\n# 1.1.2 Setting working directory before direct import\n\n#setwd('D:/Summer Coding Camp/Jeong_CodingCamp_draft_code/Jeong_CodingCamp_draft_code')\n#air_data &lt;- read.csv(\"Air_Quality.csv\")\n\n# 1.1.3 Reading by giving the absolute address\n\n#air_data &lt;- read.csv('D:/Summer Coding Camp/Jeong_CodingCamp_draft_code/Jeong_CodingCamp_draft_code/Air_Quality.csv')\n\n\n\n2. Calculate the mean of a numeric column of your choice.\n\nmean_air &lt;- mean(air_data$Year)\nprint(mean_air)\n\n[1] 2014.657\n\n\n\n\n3. Identify the number of unique values in a categorical column.\n\nnum_unique_values &lt;- length(unique(air_data$Name))\nprint(num_unique_values)\n\n[1] 18"
  },
  {
    "objectID": "assignment01.html#q2",
    "href": "assignment01.html#q2",
    "title": "Assignment 01",
    "section": "Q2",
    "text": "Q2\n\n1. Import the Air_Quality data and check the number of rows and columns.\n\ndimensions &lt;- dim(air_data)\nprint(\"Rows    Columns\")\n\n[1] \"Rows    Columns\"\n\nprint(dimensions)\n\n[1] 14077     9\n\n\n\n\n2. Calculate the median of a numeric column.\n\nmedian_air &lt;- median(air_data$Year)\nprint(median_air)\n\n[1] 2015\n\n\n\n\n3. Count the number of missing values in the Electric Vehicle data.\n\nlibrary(readxl)\nev_data &lt;- read_excel(\"Electric_Vehicle_Population_Data.xlsx\")\nnum_missing_values &lt;- sum(is.na(ev_data))\nprint(paste(\"Number of missing values in the entire data frame:\", num_missing_values))\n\n[1] \"Number of missing values in the entire data frame: 494\""
  },
  {
    "objectID": "assignment01.html#q3",
    "href": "assignment01.html#q3",
    "title": "Assignment 01",
    "section": "Q3",
    "text": "Q3\n\nExport the Electric Vehicle data to a new CSV file.\n\n\nwrite.csv(ev_data, \"electric_vehicle_data.csv\", row.names = FALSE)\n\n\nModify a column (e.g., replace missing values) and save the modified data.\n\n\nev_data$Postal_Code[is.na(ev_data$Postal_Code)] &lt;- \"XXXXX\"\nprint(ev_data)\n\n# A tibble: 194,232 × 17\n   `VIN (1-10)` County    City         State Postal_Code Model_Year Make   Model\n   &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 1C4RJXN66R   Snohomish Everett      WA    98204             2024 JEEP   WRAN…\n 2 KNDJX3AEXG   King      Renton       WA    98058             2016 KIA    SOUL \n 3 5YJ3E1EA3L   King      Seattle      WA    98125             2020 TESLA  MODE…\n 4 1G1RC6S5XH   Kitsap    Port Orchard WA    98367             2017 CHEVR… VOLT \n 5 5UXTA6C09P   Snohomish Monroe       WA    98272             2023 BMW    X5   \n 6 1FMCU0EZXN   Yakima    Moxee        WA    98936             2022 FORD   ESCA…\n 7 5YJSA1DNXD   Thurston  Olympia      WA    98506             2013 TESLA  MODE…\n 8 1N4AZ0CP8F   Snohomish Monroe       WA    98272             2015 NISSAN LEAF \n 9 WP1AE2A21J   Snohomish Everett      WA    98208             2018 PORSC… CAYE…\n10 1N4BZ1BV4N   Snohomish Everett      WA    98208             2022 NISSAN LEAF \n# ℹ 194,222 more rows\n# ℹ 9 more variables: `Electric Vehicle Type` &lt;chr&gt;,\n#   `Clean Alternative Fuel Vehicle (CAFV) Eligibility` &lt;chr&gt;,\n#   Electric_Range &lt;dbl&gt;, `Base MSRP` &lt;dbl&gt;, `Legislative District` &lt;dbl&gt;,\n#   `DOL Vehicle ID` &lt;dbl&gt;, `Vehicle Location` &lt;chr&gt;, `Electric Utility` &lt;chr&gt;,\n#   `2020 Census Tract` &lt;dbl&gt;"
  },
  {
    "objectID": "assignment01.html#creating-visualizations-for-age-adjusted-death-rate",
    "href": "assignment01.html#creating-visualizations-for-age-adjusted-death-rate",
    "title": "Assignment 01",
    "section": "2.1 Creating Visualizations for Age-adjusted Death Rate",
    "text": "2.1 Creating Visualizations for Age-adjusted Death Rate\n\n1. Create a basic histogram of Age_adjusted_Death_Rate with mean and median lines.\n\nlife_data &lt;- read.csv(\"US_Life_expectancy.csv\")\n# Replotting with mean and median\nmean_le &lt;- mean(life_data$Average_Life_Expectancy, na.rm = TRUE)\nmedian_le &lt;- median(life_data$Average_Life_Expectancy, na.rm = TRUE)\nhist(life_data$Average_Life_Expectancy,\n     main = \"Histogram of Average Life Expectancy with Mean and Median\",\n     xlab = \"Average Life Expectancy\",\n     col = \"lightblue\", border = \"black\")\nabline(v = mean_le, col = \"grey\", lwd = 2)\nabline(v = median_le, col = \"pink\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Mean\", \"Median\"), col = c(\"grey\", \"pink\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n2. Create a boxplot of Age_adjusted_Death_Rate by gender with colors.\n\nboxplot(Average_Life_Expectancy ~ Gender, data = life_data,\n        main = \"Boxplot of Life Expectancy by Gender\",\n        xlab = \"Gender\", ylab = \"Average Life Expectancy\",\n        col = c(\"lightblue\", \"lightgreen\"))\n\n\n\n\n\n\n\n\n\n\n3. Create a scatter plot of Year vs. Age_adjusted_Death_Rate with points colored by gender.\n\nplot(life_data$Year, life_data$Average_Life_Expectancy,\n     xlab = \"Year\", ylab = \"Average Life Expectancy\",\n     main = \"Scatter Plot of Life Expectancy Over Time by Gender\",\n     pch = 19, col = ifelse(life_data$Gender == \"Male\", \"grey\", \"cornflowerblue\"))\nlegend(\"topleft\", legend = c(\"Male\", \"Female\"), col = c(\"grey\", \"cornflowerblue\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n4. Create a line plot of Year vs. Age_adjusted_Death_Rate with lines for each gender.\n\nmale_data &lt;- subset(life_data, Gender == \"Male\")\nfemale_data &lt;- subset(life_data, Gender == \"Female\")\nplot(male_data$Year, male_data$Average_Life_Expectancy,\n     type = \"l\", col = \"lightblue\", lwd = 2,\n     xlab = \"Year\", ylab = \"Average Life Expectancy (years)\",\n     main = \"Life Expectancy by Gender Over Time\",\n     lty = 1,\n     xlim = c(min(life_data$Year), max(life_data$Year)), \n     ylim = c(min(life_data$Average_Life_Expectancy, na.rm = TRUE), \n              max(life_data$Average_Life_Expectancy, na.rm = TRUE)))\nlines(female_data$Year, female_data$Average_Life_Expectancy, col = \"red4\", lwd = 2, lty = 2)\n\nlegend(\"bottomright\", legend = c(\"Male\", \"Female\"), col = c(\"lightblue\", \"red4\"), lwd = 2, lty = c(1, 2))\ntext(1918, 75, \"End of World War I\", col = \"black\")\nabline(v = 1918, col = \"black\", lwd = 1, lty = 3)"
  },
  {
    "objectID": "assignment01.html#introduction",
    "href": "assignment01.html#introduction",
    "title": "Assignment 01",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThere were no exercises in this section."
  },
  {
    "objectID": "assignment01.html#loading-the-data",
    "href": "assignment01.html#loading-the-data",
    "title": "Assignment 01",
    "section": "1.2 Loading the Data",
    "text": "1.2 Loading the Data\nThere were no exercises in this section."
  },
  {
    "objectID": "assignment01.html#grouping-and-summarizing-data",
    "href": "assignment01.html#grouping-and-summarizing-data",
    "title": "Assignment 01",
    "section": "1.3 Grouping and Summarizing Data",
    "text": "1.3 Grouping and Summarizing Data\nGroup the data by Year, Aggregate by sum the Age_adjusted_Death_Rate\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nyear_data &lt;- life_data %&gt;%\n  group_by(Year) %&gt;%\n  summarize(Total_Death_Rate = sum(Age_adjusted_Death_Rate, na.rm = TRUE))\nhead(year_data)\n\n# A tibble: 6 × 2\n   Year Total_Death_Rate\n  &lt;int&gt;            &lt;dbl&gt;\n1  1900            5041.\n2  1901            4951 \n3  1902            4611.\n4  1903            4764.\n5  1904            5012.\n6  1905            4853."
  },
  {
    "objectID": "assignment01.html#joining-data-frames",
    "href": "assignment01.html#joining-data-frames",
    "title": "Assignment 01",
    "section": "1.4 Joining Data Frames",
    "text": "1.4 Joining Data Frames\nCreate a new dataset with a column specifying if the year is before or after 1945, and join it ON the original dataset, keeping the original dataset intact and without na values. Hint: which side of join you should use?\n\nadditional_data_war &lt;- data.frame(\n  Year = 1900:2020,\n  Year_Category = ifelse(1900:2020 &lt;1945, \"Before WW2\", \"After WW2\")\n)\n\n# Joining data frames\njoined_data_ww2 &lt;- left_join(life_data, additional_data_war, by = 'Year')\n\n# Viewing the joined data\nhead(joined_data_ww2)\n\n  Year Gender Average_Life_Expectancy Age_adjusted_Death_Rate Year_Category\n1 1900 Female                    48.3                  2410.4    Before WW2\n2 1901 Female                    50.6                  2350.5    Before WW2\n3 1902 Female                    53.4                  2162.8    Before WW2\n4 1903 Female                    52.0                  2250.6    Before WW2\n5 1904 Female                    49.1                  2358.8    Before WW2\n6 1905 Female                    50.2                  2287.7    Before WW2"
  },
  {
    "objectID": "assignment01.html#adding-data-using-function-with-mutate",
    "href": "assignment01.html#adding-data-using-function-with-mutate",
    "title": "Assignment 01",
    "section": "1.5 Adding data using function with mutate()",
    "text": "1.5 Adding data using function with mutate()\n\nlife_data_norm &lt;- mutate(\n  life_data, \n  life_normalized = (Average_Life_Expectancy - min(Average_Life_Expectancy)) / \n                               (max(Average_Life_Expectancy) - min(Average_Life_Expectancy))\n)\n\nhead(life_data_norm[,c(1,5)] )\n\n  Year life_normalized\n1 1900       0.2617450\n2 1901       0.3131991\n3 1902       0.3758389\n4 1903       0.3445190\n5 1904       0.2796421\n6 1905       0.3042506"
  },
  {
    "objectID": "assignment01.html#exercise-1-descriptive-statistics",
    "href": "assignment01.html#exercise-1-descriptive-statistics",
    "title": "Assignment 01",
    "section": "Exercise 1: Descriptive Statistics",
    "text": "Exercise 1: Descriptive Statistics\n\nQ1-1.\nCalculate the mean and standard deviation of the carat, price, and depth variables.\n\nlibrary(\"ggplot2\")\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:NLP':\n\n    annotate\n\ndata(\"diamonds\")\nmean_carat &lt;- mean(diamonds$carat, na.rm = TRUE)\nprint(mean_carat)\n\n[1] 0.7979397\n\nsd_carat &lt;- sd(diamonds$carat, na.rm = TRUE)\nprint(sd_carat)\n\n[1] 0.4740112\n\nmean_price &lt;- mean(diamonds$price, na.rm = TRUE)\nprint(mean_price)\n\n[1] 3932.8\n\nsd_price &lt;- sd(diamonds$price, na.rm = TRUE)\nprint(sd_price)\n\n[1] 3989.44\n\nmean_depth &lt;- mean(diamonds$depth, na.rm = TRUE)\nprint(mean_depth)\n\n[1] 61.7494\n\nsd_depth &lt;- sd(diamonds$depth, na.rm = TRUE)\nprint(sd_depth)\n\n[1] 1.432621\n\n\n\n\nQ1-2.\nUse the summary() function to get a detailed summary of the price variable.\n\nprint(summary(diamonds$price))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    326     950    2401    3933    5324   18823 \n\n\n\n\nQ1-3.\nDetermine the number of diamonds within each cut category using the table() function.\n\nprint(table(diamonds$cut))\n\n\n     Fair      Good Very Good   Premium     Ideal \n     1610      4906     12082     13791     21551"
  },
  {
    "objectID": "assignment01.html#exercise-2-t-test",
    "href": "assignment01.html#exercise-2-t-test",
    "title": "Assignment 01",
    "section": "Exercise 2: t-test",
    "text": "Exercise 2: t-test\n\nQ2\n\nSubset the diamonds dataset to include only diamonds with an “Ideal” or “Fair” cut.\nPerform a t-test to compare the mean prices between these two cut categories.\nInterpret the p-value to determine if the difference in mean prices is statistically significant.\n\n\ndiamonds_subset &lt;- subset(diamonds, cut %in% c(\"Ideal\", \"Fair\"))\nt_test_result &lt;- t.test(price ~ cut, data = diamonds_subset)\n\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\nInterpretation:\n\nt = 9.7484, df = 1894.8\nThe alternative hypothesis is that the difference in means between the “Fair” and “Ideal” groups is not equal to 0. Since the p-value is near zero, we reject the null hypothesis. The means of the two groups are different.\nThe 95% CI is between 719.9065 and 1082.5251.\nThe mean for Fair is 4358.758.\nThe mean for Ideal is 3457.542."
  },
  {
    "objectID": "assignment01.html#exercise-3-anova",
    "href": "assignment01.html#exercise-3-anova",
    "title": "Assignment 01",
    "section": "Exercise 3 ANOVA",
    "text": "Exercise 3 ANOVA\n\nQ3-1\n\nPerform an ANOVA test to assess the effect of clarity on diamond price.\nIf significant differences are found, conduct a Tukey’s HSD post-hoc test to identify which clarity levels differ from each other.\n\n\nsummary(aov(price ~ clarity, data = diamonds))\n\n               Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \nclarity         7 2.331e+10 3.330e+09     215 &lt;2e-16 ***\nResiduals   53932 8.352e+11 1.549e+07                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClarity has a significant effect on price.\n\n\nQ3-2\n\nIf significant differences are found, conduct a Tukey’s HSD post-hoc test to identify which clarity levels differ from each other.\n\n\nTukeyHSD(aov(price ~ clarity, data = diamonds))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = price ~ clarity, data = diamonds)\n\n$clarity\n                   diff          lwr         upr     p adj\nSI2-I1     1138.8599147   683.395891  1594.32394 0.0000000\nSI1-I1       71.8324571  -378.570901   522.23582 0.9997320\nVS2-I1        0.8207037  -450.377702   452.01911 1.0000000\nVS1-I1      -84.7132999  -542.298929   372.87233 0.9992819\nVVS2-I1    -640.4316203 -1109.531923  -171.33132 0.0009165\nVVS1-I1   -1401.0540535 -1881.569711  -920.53840 0.0000000\nIF-I1     -1059.3295848 -1580.334655  -538.32451 0.0000000\nSI1-SI2   -1067.0274575 -1229.386830  -904.66808 0.0000000\nVS2-SI2   -1138.0392109 -1302.591274  -973.48715 0.0000000\nVS1-SI2   -1223.5732146 -1404.907129 -1042.23930 0.0000000\nVVS2-SI2  -1779.2915349 -1987.983831 -1570.59924 0.0000000\nVVS1-SI2  -2539.9139681 -2773.136347 -2306.69159 0.0000000\nIF-SI2    -2198.1894995 -2506.318797 -1890.06020 0.0000000\nVS2-SI1     -71.0117534  -220.988718    78.96521 0.8410824\nVS1-SI1    -156.5457571  -324.764949    11.67343 0.0899007\nVVS2-SI1   -712.2640774  -909.667681  -514.86047 0.0000000\nVVS1-SI1  -1472.8865106 -1696.064436 -1249.70859 0.0000000\nIF-SI1    -1131.1620420 -1431.760399  -830.56369 0.0000000\nVS1-VS2     -85.5340037  -255.870471    84.80246 0.7958312\nVVS2-VS2   -641.2523240  -840.463263  -442.04138 0.0000000\nVVS1-VS2  -1401.8747572 -1626.652874 -1177.09664 0.0000000\nIF-VS2    -1060.1502885 -1361.938605  -758.36197 0.0000000\nVVS2-VS1   -555.7183203  -769.001243  -342.43540 0.0000000\nVVS1-VS1  -1316.3407535 -1553.679770 -1079.00174 0.0000000\nIF-VS1     -974.6162849 -1285.873083  -663.35949 0.0000000\nVVS1-VVS2  -760.6224332 -1019.466585  -501.77828 0.0000000\nIF-VVS2    -418.8979645  -746.848084   -90.94785 0.0027364\nIF-VVS1     341.7244687    -2.356168   685.80510 0.0531204"
  },
  {
    "objectID": "assignment01.html#exercise-4-simple-linear-regression",
    "href": "assignment01.html#exercise-4-simple-linear-regression",
    "title": "Assignment 01",
    "section": "Exercise 4: Simple Linear Regression",
    "text": "Exercise 4: Simple Linear Regression\n\nQ4\n\nFit a simple linear regression model with carat as the predictor and price as the response variable.\nDisplay the summary of the regression model, including coefficients, R-squared, and p-value.\nCreate a scatter plot of price versus carat, and add the regression line to visualize the relationship.\n\n\ndiamond_lm &lt;- lm(price ~ carat, data = diamonds)\nsummary(diamond_lm)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Create a scatter plot and add the regression line\nplot(diamonds$carat, diamonds$price, main = \"Linear Regression of Price on Carat\",\n     xlab = \"Carat\", ylab = \"Price\",pch = 19, col = \"pink\")\nabline(diamond_lm, col = \"cornflowerblue\")"
  },
  {
    "objectID": "assignment01.html#exercise-1-fitting-a-multiple-linear-regression-model",
    "href": "assignment01.html#exercise-1-fitting-a-multiple-linear-regression-model",
    "title": "Assignment 01",
    "section": "Exercise 1: Fitting a Multiple Linear Regression Model",
    "text": "Exercise 1: Fitting a Multiple Linear Regression Model\n\nQ1-1\nUse the lm() function to create a multiple linear regression model with your selected variables.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndata(\"Boston\")\nstr(Boston)\n\n'data.frame':   506 obs. of  14 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n\n# Fitting the multiple linear regression model\nlm1 &lt;- lm(crim ~ indus + lstat + nox, data = Boston)\n\n\n\nQ1-2\nDisplay the summary of the model to interpret the coefficients, R-squared, and p-values.\n\n# Display the model summary\nsummary(lm1)\n\n\nCall:\nlm(formula = crim ~ indus + lstat + nox, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.330  -2.667  -0.557   1.169  81.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.35640    1.94271  -4.816 1.94e-06 ***\nindus        0.12046    0.07869   1.531  0.12644    \nlstat        0.35573    0.06050   5.880 7.49e-09 ***\nnox         12.84903    4.60314   2.791  0.00545 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.489 on 502 degrees of freedom\nMultiple R-squared:  0.2465,    Adjusted R-squared:  0.242 \nF-statistic: 54.74 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nQ1-3\nFor each independent variable, interpret the coefficient in terms of its impact on the dependent variable.\ncrim will be -9.36 indus, lstat, and nox are zero. This is the intercept and is significant.\nindus is 0.12046. For every 1 increase in indus, crim will increase 0.12. This is not significant.\nlstat is 0.35573. For every 1 increase in lstat, crim will increase 0.36. This has a strong significance.\nnox is 12.84903. For every 1 increase in nox, crim will increase 12.85. This is significant."
  },
  {
    "objectID": "assignment01.html#exercise-2-refining-the-model-with-stepwise-regression",
    "href": "assignment01.html#exercise-2-refining-the-model-with-stepwise-regression",
    "title": "Assignment 01",
    "section": "Exercise 2: Refining the Model with Stepwise Regression",
    "text": "Exercise 2: Refining the Model with Stepwise Regression\n\nQ2-1\nApply forward selection, backward elimination, or bidirectional elimination to your model using the step() function.\nForward\n\nforward_model &lt;- step(lm(crim ~ 1, data = Boston), \n                      scope = list(lower = lm(crim ~ 1, data = Boston), \n                                   upper = lm(crim ~ indus + lstat + nox, data = Boston)), \n                      direction = \"forward\")\n\nStart:  AIC=2178.76\ncrim ~ 1\n\n        Df Sum of Sq   RSS    AIC\n+ lstat  1    7756.3 29607 2063.0\n+ nox    1    6621.4 30742 2082.1\n+ indus  1    6176.5 31187 2089.3\n&lt;none&gt;               37363 2178.8\n\nStep:  AIC=2063.03\ncrim ~ lstat\n\n        Df Sum of Sq   RSS    AIC\n+ nox    1    1322.0 28285 2041.9\n+ indus  1    1016.5 28590 2047.3\n&lt;none&gt;               29607 2063.0\n\nStep:  AIC=2041.92\ncrim ~ lstat + nox\n\n        Df Sum of Sq   RSS    AIC\n+ indus  1    131.43 28154 2041.6\n&lt;none&gt;               28285 2041.9\n\nStep:  AIC=2041.56\ncrim ~ lstat + nox + indus\n\nsummary(forward_model)\n\n\nCall:\nlm(formula = crim ~ lstat + nox + indus, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.330  -2.667  -0.557   1.169  81.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.35640    1.94271  -4.816 1.94e-06 ***\nlstat        0.35573    0.06050   5.880 7.49e-09 ***\nnox         12.84903    4.60314   2.791  0.00545 ** \nindus        0.12046    0.07869   1.531  0.12644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.489 on 502 degrees of freedom\nMultiple R-squared:  0.2465,    Adjusted R-squared:  0.242 \nF-statistic: 54.74 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\nBackward\n\nbackward_model &lt;- step(lm(crim ~ indus + lstat + nox, data = Boston), \n                       direction = \"backward\")\n\nStart:  AIC=2041.56\ncrim ~ indus + lstat + nox\n\n        Df Sum of Sq   RSS    AIC\n&lt;none&gt;               28154 2041.6\n- indus  1    131.43 28285 2041.9\n- nox    1    436.98 28590 2047.3\n- lstat  1   1939.05 30093 2073.3\n\nsummary(backward_model)\n\n\nCall:\nlm(formula = crim ~ indus + lstat + nox, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.330  -2.667  -0.557   1.169  81.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.35640    1.94271  -4.816 1.94e-06 ***\nindus        0.12046    0.07869   1.531  0.12644    \nlstat        0.35573    0.06050   5.880 7.49e-09 ***\nnox         12.84903    4.60314   2.791  0.00545 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.489 on 502 degrees of freedom\nMultiple R-squared:  0.2465,    Adjusted R-squared:  0.242 \nF-statistic: 54.74 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\nBoth\n\nboth_model &lt;- step(lm(crim ~ 1, data = Boston), \n                   scope = list(lower = lm(crim ~ 1, data = Boston), \n                                upper = lm(crim ~ indus + lstat + nox, data = Boston)), \n                   direction = \"both\")\n\nStart:  AIC=2178.76\ncrim ~ 1\n\n        Df Sum of Sq   RSS    AIC\n+ lstat  1    7756.3 29607 2063.0\n+ nox    1    6621.4 30742 2082.1\n+ indus  1    6176.5 31187 2089.3\n&lt;none&gt;               37363 2178.8\n\nStep:  AIC=2063.03\ncrim ~ lstat\n\n        Df Sum of Sq   RSS    AIC\n+ nox    1    1322.0 28285 2041.9\n+ indus  1    1016.5 28590 2047.3\n&lt;none&gt;               29607 2063.0\n- lstat  1    7756.3 37363 2178.8\n\nStep:  AIC=2041.92\ncrim ~ lstat + nox\n\n        Df Sum of Sq   RSS    AIC\n+ indus  1    131.43 28154 2041.6\n&lt;none&gt;               28285 2041.9\n- nox    1   1322.02 29607 2063.0\n- lstat  1   2456.88 30742 2082.1\n\nStep:  AIC=2041.56\ncrim ~ lstat + nox + indus\n\n        Df Sum of Sq   RSS    AIC\n&lt;none&gt;               28154 2041.6\n- indus  1    131.43 28285 2041.9\n- nox    1    436.98 28590 2047.3\n- lstat  1   1939.05 30093 2073.3\n\nsummary(both_model)\n\n\nCall:\nlm(formula = crim ~ lstat + nox + indus, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.330  -2.667  -0.557   1.169  81.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.35640    1.94271  -4.816 1.94e-06 ***\nlstat        0.35573    0.06050   5.880 7.49e-09 ***\nnox         12.84903    4.60314   2.791  0.00545 ** \nindus        0.12046    0.07869   1.531  0.12644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.489 on 502 degrees of freedom\nMultiple R-squared:  0.2465,    Adjusted R-squared:  0.242 \nF-statistic: 54.74 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nQ2-2\nCompare the new model with your initial model and discuss any differences in the selected variables and model performance.\nlstat and nox are the most significant predictors of crime rate. Adding indus only slightly improves the model."
  },
  {
    "objectID": "assignment01.html#selecting-variables-and-fitting-a-regression-model",
    "href": "assignment01.html#selecting-variables-and-fitting-a-regression-model",
    "title": "Assignment 01",
    "section": "Selecting Variables and Fitting a Regression Model",
    "text": "Selecting Variables and Fitting a Regression Model\n\nQ1\n\nUse the lm() function to create a multiple linear regression model.\nDisplay the summary of the model to interpret the coefficients, R-squared, and p-values.\n\n\nlm1 &lt;- lm(price ~ color + depth + table, data = diamonds)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ color + depth + table, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6738  -2593  -1326   1302  15936 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -12295.321    992.830 -12.384  &lt; 2e-16 ***\ncolor.L       2051.961     56.700  36.190  &lt; 2e-16 ***\ncolor.Q        146.868     53.863   2.727   0.0064 ** \ncolor.C       -273.608     50.687  -5.398 6.77e-08 ***\ncolor^4         60.871     46.552   1.308   0.1910    \ncolor^5       -241.934     44.009  -5.497 3.87e-08 ***\ncolor^6         62.399     39.908   1.564   0.1179    \ndepth           52.593     12.282   4.282 1.85e-05 ***\ntable          229.054      7.876  29.083  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3896 on 53931 degrees of freedom\nMultiple R-squared:  0.0466,    Adjusted R-squared:  0.04646 \nF-statistic: 329.5 on 8 and 53931 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignment01.html#residual-analysis",
    "href": "assignment01.html#residual-analysis",
    "title": "Assignment 01",
    "section": "Residual Analysis",
    "text": "Residual Analysis\n\nQ2\n\nCreate residual plots to check for linearity, homoscedasticity, and normality.\nUse the par() function to display multiple plots at once.\n\n\npar(mfrow = c(2, 2))\nplot(lm1)"
  },
  {
    "objectID": "assignment01.html#checking-for-multicollinearity-and-heteroscedasticity",
    "href": "assignment01.html#checking-for-multicollinearity-and-heteroscedasticity",
    "title": "Assignment 01",
    "section": "Checking for Multicollinearity and Heteroscedasticity",
    "text": "Checking for Multicollinearity and Heteroscedasticity\n\nQ3\n\nCalculate VIF for each independent variable in your model.\nUse the bptest() function from the lmtest package to test for heteroscedasticity.\nInterpret the results of the test.\n\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nvif_values &lt;- vif(lm1)\nvif_values\n\n          GVIF Df GVIF^(1/(2*Df))\ncolor 1.007114  6        1.000591\ndepth 1.100311  1        1.048957\ntable 1.100780  1        1.049181\n\n\n\n# Install the lmtest package if not already installed\n# install.packages(\"lmtest\")\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# Breusch-Pagan test for heteroscedasticity\nbp_test &lt;- bptest(lm1)\nbp_test\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm1\nBP = 785.22, df = 8, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "assignment01.html#model-comparison-and-evaluation",
    "href": "assignment01.html#model-comparison-and-evaluation",
    "title": "Assignment 01",
    "section": "Model Comparison and Evaluation",
    "text": "Model Comparison and Evaluation\n\nQ4\n\nFit an alternative model with a different set of independent variables.\nCompare the models based on Adjusted R-squared and AIC to determine which model is better.\n\n\n# Alternative model with different independent variables\nlm2 &lt;- lm(price ~ carat + cut + color, data = diamonds)"
  },
  {
    "objectID": "SPARQL.html",
    "href": "SPARQL.html",
    "title": "SPARQL",
    "section": "",
    "text": "Nomisma SPARQL Endpoint"
  },
  {
    "objectID": "SPARQL.html#example-1",
    "href": "SPARQL.html#example-1",
    "title": "SPARQL",
    "section": "Example 1",
    "text": "Example 1\nModified from R networks analysis / works\nSELECT DISTINCT ?hoard ?mint ?mintlat ?mintlong WHERE {\n{\n  ?hoard void:inDataset &lt;http://numismatics.org/chrr/&gt; ;\n  dcterms:tableOfContents/nmo:hasTypeSeriesItem/nmo:hasMint ?mint .\n}\nUNION\n{ ?hoard a nmo:Hoard ;\ndcterms:tableOfContents [ nmo:hasTypeSeriesItem ?tsi ] .\n?tsi nmo:hasMint ?mint .\n}\nOPTIONAL { ?hoard nmo:hasFindspot [\ngeo:lat ?hoardlat ;\ngeo:long ?hoardlong ] }\nOPTIONAL { ?mint geo:location [\ngeo:lat ?mintlat ;\ngeo:long ?mintlong ] }\n}"
  }
]