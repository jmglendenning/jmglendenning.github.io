[
  {
    "objectID": "project01.html",
    "href": "project01.html",
    "title": "Project - Brainstorming",
    "section": "",
    "text": "Background\nMoses Finley’s The Ancient Economy (1973) challenged traditional views of ancient economies, particularly those of Greece and Rome. Finley argued that ancient economies were fundamentally different from modern capitalist systems and should not be analyzed using modern economic concepts.\nSince publication, there have been some shifting thoughts on the inequalities of ancient Rome. This project will be designed to see if there has been a shift in scholarly publications on inequalities in the Roman economic system.\n\n\nHypothesis\n\\[\n\\begin{flalign}\n\\scriptsize\nH_0&: \\scriptsize  \\text{Scholarship about inequality in ancient Rome has not changed.}\\\\\n\\scriptsize\nH_1&: \\scriptsize  \\text{Scholarship has changed.}\n\\end{flalign}\n\\]\n\n\nData Collection\nInitially, data collection will start with gathering access to scholarly journals related to the ancient Roman Empire. We will start with selected articles from JSTOR from publications such as The Journal of Roman Studies.\n\n\nMethods\n\nJournal Acquisition\n\nAcquire a limited dataset of publications\nPreprocess the data including OCR, tokenization, stemming, etc.\n\nConversion\n\nConvert text to numerical representations\n\nModel Selection & Training\nPost-processing\n\n\n\nStart\nThe first step in the process is to verify that the data can actually be collected. Given the size and time constraints, it may be necessary to limit the scope of data collection to a reasonable, but not exhaustive amount.\nThe book The Ancient Economy has been obtained and OCRed.\nOne paper has been downloaded for examination (Brock, Motta, and Terrenato 2021).\n\n\n\n\n\ngraph TD\n  A[Start] --&gt; B[Load Text Data]\n  B --&gt; C[Preprocessing]\n  C --&gt;|Convert to Lowercase| C1[Lowercasing]\n  C --&gt;|Remove Punctuation| C2[Remove Punctuation]\n  C --&gt;|Remove Stopwords| C3[Stopword Removal]\n  C --&gt;|Stemming/Lemmatization| C4[Stemming/Lemmatization]\n  C --&gt;|Tokenization| C5[Tokenization]\n  C1 --&gt; D[Create Corpus]\n  C2 --&gt; D\n  C3 --&gt; D\n  C4 --&gt; D\n  C5 --&gt; D\n  D --&gt; E[Feature Extraction]\n  E --&gt;|Bag-of-Words| E1[BoW Model]\n  E --&gt;|TF-IDF| E2[TF-IDF Model]\n  E --&gt;|Word Embeddings| E3[Word2Vec, FastText, GloVe]\n  E1 --&gt; F[Train NLP Model]\n  E2 --&gt; F\n  E3 --&gt; F\n  F --&gt;|Classification| G1[Sentiment Analysis, Spam Detection]\n  F --&gt;|Clustering| G2[Topic Modeling, Named Entity Recognition]\n  F --&gt;|Text Generation| G3[Chatbots, Machine Translation]\n  G1 --&gt; H[Evaluate Model]\n  G2 --&gt; H\n  G3 --&gt; H\n  H --&gt; I[Deploy Model]\n  I --&gt; J[End]\n\n\n\n\n\n\n\n\nAlternative Projects to Consider\nSome other projects that could be considered:\n\nPredictive Modeling of Archaeological Site Locations\nAutomated Artifact Classification and Typology\nTemporal and Spatial Analysis of Settlement Patterns\nText Mining and NLP for Archaeological Literature\nReconstructing Ancient Trade Routes\n\n\n\nBibliography\nBrock, Andrea L., Lorenzo Motta, and Nicola Terrenato. 2021. “On the Banks of the Tiber: Opportunity and Transformation in Early Rome.” The Journal of Roman Studies 111: 1–30. https://www.jstor.org/stable/27128849.\nFinley, Moses I. 1973. The Ancient Economy. Berkeley: University of California Press."
  },
  {
    "objectID": "project04.html",
    "href": "project04.html",
    "title": "Project Interim Report EPPS6323",
    "section": "",
    "text": "Interim Project Update\nView Presentation"
  },
  {
    "objectID": "Prepare06.html",
    "href": "Prepare06.html",
    "title": "Prepare for Class06",
    "section": "",
    "text": "Proposal presentation:\n\n\nFive minutes for presentation\nEach member will be assigned to discuss another member’s presentation\nOne minute for discussion\n\n\nAssignment 3\n\nCompleted\n\nAssigned reading\n\n\nAttewell, Paul, and David Monaghan. 2015. Data mining for the social sciences: An introduction. Univ of California Press. (AM) Chapters 1 - 4\n\nCompleted\n\n\n\nResearch\n\n\nWhat is Association Rules?\n\nAssociation Rules are a set of if-then rules used in data mining to discover interesting relationships between variables in large datasets.\nFor instance, in toy store, when a kid buys a teddy bear, they often also buy a blanket. This is like a special rule: “If a kid buys a teddy bear, they will probably buy a blanket too!”\n\nWhat is the difference between Data Mining and Unsupervised Learning?\n\nData mining is a broad process that involves discovering patterns, correlations, or anomalies in large datasets to extract meaningful information.\nUnsupervised learning is a specific type of machine learning where the algorithm is trained on data without labeled responses. Unsupervised learning can be a component of data mining."
  },
  {
    "objectID": "Prepare06.html#prepare-for-class",
    "href": "Prepare06.html#prepare-for-class",
    "title": "Prepare for Class06",
    "section": "",
    "text": "Proposal presentation:\n\n\nFive minutes for presentation\nEach member will be assigned to discuss another member’s presentation\nOne minute for discussion\n\n\nAssignment 3\n\nCompleted\n\nAssigned reading\n\n\nAttewell, Paul, and David Monaghan. 2015. Data mining for the social sciences: An introduction. Univ of California Press. (AM) Chapters 1 - 4\n\nCompleted\n\n\n\nResearch\n\n\nWhat is Association Rules?\n\nAssociation Rules are a set of if-then rules used in data mining to discover interesting relationships between variables in large datasets.\nFor instance, in toy store, when a kid buys a teddy bear, they often also buy a blanket. This is like a special rule: “If a kid buys a teddy bear, they will probably buy a blanket too!”\n\nWhat is the difference between Data Mining and Unsupervised Learning?\n\nData mining is a broad process that involves discovering patterns, correlations, or anomalies in large datasets to extract meaningful information.\nUnsupervised learning is a specific type of machine learning where the algorithm is trained on data without labeled responses. Unsupervised learning can be a component of data mining."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Master’s student at the University of Texas at Dallas studying GIS.\nI am interested in applying GIS tools and techniques to historical questions. Projects that I have worked on include:\n\nRoman villa site location analysis and predictive modeling\nA Python implementation of Geomorphons\nApplication of low-cost drones to disaster efforts\nPython-based DEM-based spatial statistics via WCS data-acquisition\n\nI am currently working on analyzing Roman coin hoards.\nI have a strong interest in remote sensing and tools.\nCurriculum vitae"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John Glendenning",
    "section": "",
    "text": "I am a masters student studying geographical information systems.\n\nContact\njmg220005@utdallas.edu"
  },
  {
    "objectID": "assignment03.html",
    "href": "assignment03.html",
    "title": "Assignment 03",
    "section": "",
    "text": "Objective: Design prompts to conduct a structured systematic literature review on data mining and machine learning using multiple AI models (e.g. ChatGPT, Gemini, and Llama) and analyze each model’s contribution and refining prompts for academic precision and depth.\nExercise Steps:\n\n\n\nTask: Write a baseline prompt to request a structured systematic literature review on data mining and machine learning applications.\nExample Prompt: “Conduct a 2,000-word structured systematic literature review on the applications of data mining and machine learning in real-world domains. Include a methodology section, synthesize key findings, identify trends and gaps, and propose one testable hypothesis. Use an academic tone and emulate systematic review standards.”\nStudents submit this prompt to ChatGPT, Copilot, and Grok 3, collecting the raw outputs.\n\n\nPlease note that the rest of this document is a rainbow of colors to help keep inputs and responses, separate from the original questions and my answers to them.\n\n\nGrey - Original Assignment Text\n\n\n\nGreen - Answers to Original Assignment\n\n\n\nBlue - Document Headings\n\n\n\nLight Purple - Prompts\n\n\n\nDark Purple - Model Output\n\n\nHopfully, this will make this document easier to digest.\nNote: Some of the raw outputs have been adjusted to remove some Markdown formatting because it conflicts with the formatting of this document.\n\n\nThis is the original prompt\n\nCreate a structured, systematic literature review on the applications of data mining and machine learning.\nInclude: Methodology Trends Gap analysis Supporting quotes Tables, Figures, Charts Future Outlook Conclusion One Testable Hypothesis\nRequirements: All statements must be real and true A comprehensive list of sources Organize by themes APSA format and academic tone\n\n\n\nMethodology\nThis systematic literature review examines the applications of data mining and machine learning across various domains. The methodology involved a comprehensive search of academic databases, including PubMed, MEDLINE, Google Scholar, and SpringerLink, to identify relevant studies. Keywords such as “data mining applications,” “machine learning applications,” and “systematic literature review” were utilized. Inclusion criteria encompassed peer-reviewed articles published in English that discussed practical applications of data mining and machine learning. Studies focusing solely on theoretical aspects without empirical validation were excluded. The selected articles were categorized based on application domains, methodologies employed, and outcomes achieved.\nTrends\nThe analysis revealed several prominent trends in the application of data mining and machine learning: 1. Healthcare and Medicine: There is a significant increase in utilizing machine learning for predictive analytics in patient diagnosis, treatment planning, and personalized medicine. For instance, machine learning algorithms have been applied to predict patient outcomes and assist in clinical decision-making. 2. Environmental Science: Data mining techniques are increasingly used to analyze environmental data, such as air quality monitoring and pollution prediction. A systematic review highlighted the application of machine learning methods in air pollution epidemiology, aiding in forecasting and source apportionment. ￼ 3. Education: Educational data mining has emerged as a tool to analyze student performance and improve educational outcomes. A review discussed the use of data mining and machine learning algorithms to predict student success and identify areas needing intervention. ￼ 4. Finance: Predictive analytics and machine learning models are employed for credit scoring, fraud detection, and risk assessment, enhancing decision-making processes in financial institutions. ￼\nGap Analysis\nDespite the advancements, certain gaps persist: 1. Data Quality and Integration: Many studies highlight challenges related to data quality, including missing values and inconsistencies, which hinder the effectiveness of machine learning models. Integrating heterogeneous data sources remains a significant obstacle. 2. Model Interpretability: Complex models, such as deep neural networks, often lack interpretability, making it difficult for practitioners to trust and adopt these models in critical applications like healthcare. 3. Ethical Considerations: There is a growing concern about the ethical implications of data mining and machine learning, particularly regarding privacy, bias, and fairness. Few studies address these issues comprehensively.\nTestable Hypothesis\nImplementing explainable AI models in healthcare diagnostics will lead to higher adoption rates among medical professionals compared to traditional black-box models.\nSupporting Quotes • “Appropriate application of Data Mining procedures can provide multiple indicators of student performance.” ￼ • “Data mining is becoming an increasingly common tool in environmental health.” ￼\nTables, Figures, Charts\nTable 1: Applications of Data Mining and Machine Learning by Domain\nDomain Applications References Healthcare Patient diagnosis, treatment planning\nEnvironmental Science Air quality monitoring, pollution prediction\nEducation Student performance analysis\nFinance Credit scoring, fraud detection\nFigure 1: Trends in Machine Learning Applications Across Domains\nChart 1: Challenges in Data Mining and Machine Learning Implementations\nFuture Outlook\nThe future of data mining and machine learning applications is promising, with anticipated advancements in the following areas: 1. Explainable AI: Developing models that provide transparent and interpretable results to address the trust and adoption issues in critical fields like healthcare. 2. Automated Machine Learning (AutoML): Simplifying the model development process to make machine learning accessible to non-experts, thereby broadening its application scope. 3. Integration with Big Data Technologies: Combining machine learning with big data platforms to efficiently process and analyze vast amounts of data, leading to more accurate and timely insights. 4. Ethical Frameworks: Establishing robust ethical guidelines to ensure the responsible use of data mining and machine learning, focusing on privacy, bias mitigation, and fairness.\nConclusion\nData mining and machine learning have become integral tools across various sectors, offering significant improvements in decision-making and efficiency. However, challenges related to data quality, model interpretability, and ethical considerations must be addressed to fully harness their potential. Future research should focus on developing transparent models, improving data integration techniques, and establishing ethical standards to guide the responsible application of these technologies.\nComprehensive List of Sources • Tawde, Pallavi Devendra, and Yogesh Kumar Sharma. “Systematic Review on Data Mining and Machine Learning Algorithms for Analysing Student’s Performance.” Utkal Historical Research Journal 34, no. III (2021): 108-117. • Chen, Hsinchun, et al. “A Systematic Review of Data Mining and Machine Learning for Air Pollution Epidemiology.” BMC Public Health 17, no. 1 (2017): 1-19. • Katuwal, Gopal J., and Prasanna K. Kolhe. “Systematic Literature Review of Machine Learning Methods Used in Respiratory Sound Analysis.” BMC Medical Informatics and Decision Making 21, no. 1 (2021): 1-16. • Bellazzi, Riccardo, and Blaz Zupan. “Predictive Data Mining in Clinical Medicine: Current Issues and Guidelines.” International Journal of Medical Informatics 77, no. 2 (2008): 81-97. • Azeem, Ibraheem. *On the Evolution of Data Science and Machine Learning\n\n\nStructured Systematic Literature Review: Applications of Data Mining and Machine Learning\n\n1. Introduction Data mining and machine learning (ML) have emerged as transformative technologies across various domains, including healthcare, finance, education, and cybersecurity. This review systematically examines the applications, trends, and gaps in the literature, providing a structured analysis of the current state of research. The methodology, trends, and future outlook are discussed, supported by evidence from peer-reviewed studies.\n\n2. Methodology This review follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) framework to ensure a rigorous and transparent process.\n2.1 Data Collection - Databases Searched: IEEE Xplore, PubMed, SpringerLink, ACM Digital Library, and Google Scholar. - Keywords: “data mining applications,” “machine learning applications,” “trends in data mining,” “machine learning in healthcare,” “data mining in finance.” - Inclusion Criteria: Peer-reviewed articles published between 2015 and 2023, focusing on real-world applications of data mining and ML. - Exclusion Criteria: Articles without empirical evidence, non-English publications, and theoretical papers without practical applications.\n2.2 Data Analysis - Thematic analysis was conducted to identify recurring themes and trends. - Gap analysis was performed by comparing the stated objectives of studies with their outcomes. - Quantitative data were visualized using tables and charts.\n\n3. Trends in Data Mining and Machine Learning Applications\n3.1 Healthcare - Applications: Predictive diagnostics, personalized medicine, and patient outcome prediction. - Trends: Increased use of deep learning for medical image analysis and natural language processing (NLP) for electronic health record (EHR) analysis. - Supporting Quote: “Machine learning models, particularly convolutional neural networks (CNNs), have achieved diagnostic accuracy comparable to human experts in radiology” (Esteva et al., 2017).\n3.2 Finance - Applications: Fraud detection, algorithmic trading, and credit scoring. - Trends: Adoption of ensemble learning methods like XGBoost and Random Forests for improved predictive accuracy. - Supporting Quote: “Ensemble methods have reduced false positives in fraud detection by 20% compared to traditional statistical methods” (Dal Pozzolo et al., 2015).\n3.3 Education - Applications: Learning analytics, student performance prediction, and adaptive learning systems. - Trends: Growth in the use of clustering algorithms to identify at-risk students and recommend interventions. - Supporting Quote: “K-means clustering has been effective in segmenting students based on learning behaviors, enabling targeted interventions” (Romero & Ventura, 2020).\n3.4 Cybersecurity - Applications: Intrusion detection, malware classification, and anomaly detection. - Trends: Shift towards unsupervised learning for detecting zero-day attacks and adversarial ML for robust systems. - Supporting Quote: “Unsupervised learning techniques like autoencoders have shown promise in identifying novel attack patterns” (Chandola et al., 2018).\n\n4. Gap Analysis Despite advancements, several gaps persist in the literature:\n4.1 Interpretability - Many ML models, especially deep learning, lack interpretability, limiting their adoption in critical fields like healthcare. - Supporting Quote: “The ‘black-box’ nature of deep learning models remains a significant barrier to their deployment in clinical settings” (Samek et al., 2019).\n4.2 Data Quality - Poor data quality and lack of labeled datasets hinder the performance of supervised learning models. - Supporting Quote: “Insufficient and noisy data are the primary challenges in applying ML to real-world problems” (Provost & Fawcett, 2013).\n4.3 Scalability - Many algorithms struggle with scalability when applied to large datasets. - Supporting Quote: “Scalability issues in traditional ML algorithms have led to the development of distributed computing frameworks like Apache Spark” (Zaharia et al., 2016).\n\n5. Visual Representation\nTable 1: Applications of Data Mining and Machine Learning by Domain | Domain | Applications | Common Algorithms | |——————|————————————–|——————————–| | Healthcare | Predictive diagnostics, EHR analysis | CNNs, SVM, NLP | | Finance | Fraud detection, credit scoring | XGBoost, Random Forests | | Education | Learning analytics, adaptive systems | K-means, Decision Trees | | Cybersecurity | Intrusion detection, malware analysis| Autoencoders, GANs |\nFigure 1: Trends in ML Applications (2015-2023) \nCaption: Growth in deep learning and ensemble methods across domains.\n\n6. Future Outlook - Explainable AI (XAI): Development of interpretable models to enhance trust and adoption. - Federated Learning: Addressing data privacy concerns by enabling decentralized model training. - Quantum Machine Learning: Leveraging quantum computing for solving complex optimization problems. - Supporting Quote: “Federated learning is poised to revolutionize data privacy in ML applications” (Yang et al., 2019).\n\n7. Conclusion This review highlights the transformative potential of data mining and ML across various domains while identifying critical gaps in interpretability, data quality, and scalability. Future research should focus on developing robust, scalable, and interpretable models to address these challenges.\n\n8. References - Chandola, V., Banerjee, A., & Kumar, V. (2018). Anomaly detection: A survey. ACM Computing Surveys, 41(3), 1-58. - Dal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2015). Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications, 41(10), 4915-4928. - Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115-118. - Provost, F., & Fawcett, T. (2013). Data science for business: What you need to know about data mining and data-analytic thinking. O’Reilly Media. - Romero, C., & Ventura, S. (2020). Educational data mining and learning analytics: An updated survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 10(3), e1355. - Samek, W., Montavon, G., Vedaldi, A., Hansen, L. K., & Müller, K. R. (2019). Explainable AI: Interpreting, explaining and visualizing deep learning. Springer. - Yang, Q., Liu, Y., Chen, T., & Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 1-19. - Zaharia, M., Xin, R. S., Wendell, P., Das, T., Armbrust, M., Dave, A., … & Stoica, I. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65.\n\nThis review adheres to APSA formatting and maintains an academic tone, ensuring credibility and relevance for scholarly audiences.\n\n\n\nApplications of Data Mining and Machine Learning: A Systematic Literature Review Abstract:\nThis review systematically examines the applications of data mining and machine learning (DM/ML) across diverse domains. Employing a structured methodology, it identifies prevailing trends, conducts a gap analysis, and outlines future research directions. By synthesizing findings from a comprehensive literature search, this review provides a nuanced understanding of the transformative impact of DM/ML.\n\nIntroduction:\n\nData mining and machine learning have evolved into pivotal tools for extracting valuable insights and automating decision-making from vast datasets. Their application spans various sectors, including healthcare, finance, marketing, and environmental science. This review aims to systematically analyze the existing literature to identify key applications, trends, and research gaps.\n\nMethodology:\n\nThis review adheres to a systematic approach, encompassing the following stages:\nSearch Strategy: A comprehensive search was conducted using databases such as IEEE Xplore, ACM Digital Library, ScienceDirect, PubMed, and Google Scholar. Keywords included “data mining,” “machine learning,” “applications,” “healthcare,” “finance,” “marketing,” “environmental science,” and related terms. Inclusion/Exclusion Criteria: Studies were included if they: Presented empirical applications of DM/ML. Were published in peer-reviewed journals or conference proceedings. Were written in English. Were published within the last decade (with exceptions for foundational works). Studies were excluded if they: Were theoretical papers without empirical applications. Were non-peer-reviewed or grey literature. Data Extraction: Relevant information was extracted from each study, including: Application domain. DM/ML techniques used. Dataset characteristics. Performance metrics. Key findings. Synthesis and Analysis: Extracted data were synthesized and analyzed to identify prevailing trends, research gaps, and future directions. 3. Themes and Trends:\n3.1. Healthcare:\nDM/ML algorithms are widely used for disease prediction, diagnosis, and treatment. Applications include medical image analysis, genomics, and electronic health record analysis. Trend: Increased use of deep learning for complex medical imaging tasks. “Machine learning algorithms have demonstrated significant potential in predicting disease progression and improving patient outcomes” (Obermeyer & Emanuel, 2016). Table 1: Healthcare Applications of DM/ML | Application | DM/ML Technique | Example | | :——————- | :——————— | :————————————— | | Disease Prediction | Logistic Regression, SVM | Predicting cardiovascular disease risk | | Medical Image Analysis | Convolutional Neural Networks | Detecting tumors in MRI scans | | Genomics | Clustering, Classification | Identifying genetic markers for diseases | 3.2. Finance:\nDM/ML is employed for fraud detection, risk management, and algorithmic trading. Applications include credit scoring, stock market prediction, and customer churn analysis. Trend: Growing adoption of reinforcement learning for algorithmic trading. “The use of machine learning in financial fraud detection has significantly improved accuracy and reduced false positives” (Bolton & Hand, 2002). Figure 1: Financial Applications of DM/ML (A pie chart could be used to show the distribution of DM/ML applications in finance, such as fraud detection, risk management, and algorithmic trading.) 3.3. Marketing:\nDM/ML enables personalized marketing, customer segmentation, and recommendation systems. Applications include sentiment analysis, social media analytics, and targeted advertising. Trend: Increasing focus on ethical considerations in personalized marketing. “Machine learning algorithms are increasingly used to personalize customer experiences and improve marketing campaign effectiveness” (Wedel & Kannan, 2016). 3.4. Environmental Science:\nDM/ML is used for climate modeling, air quality prediction, and resource management. Applications include remote sensing data analysis and ecological modeling. Trend: Development of AI-powered solutions for environmental sustainability. “Data mining techniques are crucial for analyzing environmental data and extracting insights to support sustainable resource management” (Recknagel et al., 2001). 4. Gap Analysis:\nData Quality and Availability: Many applications suffer from limited data availability or poor data quality. Interpretability and Explainability: Complex DM/ML models, particularly deep learning, often lack interpretability, hindering trust and adoption. Ethical Considerations: Bias in algorithms, privacy concerns, and the potential for misuse require careful attention. “The ‘black box’ nature of many machine learning models poses challenges for understanding and trusting their predictions” (Doshi-Velez & Kim, 2017). Generalizability: Models trained on specific datasets may not generalize well to other populations or contexts. Integration: Integration of DM/ML into existing workflows and systems remains a significant challenge. 5. Future Outlook:\nExplainable AI (XAI): Research will focus on developing methods to make DM/ML models more transparent and interpretable. Federated Learning: This approach allows training models on decentralized data, addressing privacy concerns. Automated Machine Learning (AutoML): AutoML tools will simplify the process of developing and deploying DM/ML models. AI for Social Good: Increased focus on using DM/ML to address societal challenges, such as poverty, inequality, and climate change. Hybrid AI: Combining different AI methods, such as symbolic AI and machine learning, to create more robust and adaptable systems. Edge AI: Implementing machine learning directly on edge devices, enabling faster and more efficient data processing. 6. Conclusion:\nData mining and machine learning have revolutionized numerous domains, offering powerful tools for extracting insights and automating decision-making. However, challenges related to data quality, interpretability, and ethical considerations remain. Future research should focus on addressing these gaps and developing more robust, transparent, and ethical DM/ML solutions. The integration of explainable AI, federated learning, and AutoML will play a crucial role in expanding the reach and impact of these technologies.\n\nTestable Hypothesis:\n\nHypothesis: Deep learning models trained on multimodal medical imaging data (e.g., combining MRI and PET scans) will demonstrate significantly higher accuracy in early-stage Alzheimer’s disease detection compared to models trained on single-modality data.\n\nReferences:\n\nBolton, R. J., & Hand, D. J. (2002). Statistical fraud detection: A review. Statistical science, 17(3), 235-255. Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608. Obermeyer, Z., & Emanuel, E. J. (2016). Predicting the future—big data, machine learning, and clinical medicine. New England Journal of Medicine, 375(13), 1216-1219. 1   1. rsdjournal.org rsdjournal.org Recknagel, F., French, M., Harker, P., & Yabunaka, K. I. (2001). Artificial neural network approach for modelling and prediction of algal blooms. Ecological modelling, 146(1-3), 81-91. Wedel, M., & Kannan, P. K. (2016). Marketing analytics for data-rich environments. Journal of Marketing, 80(6), 97-121. Note: This is a structured outline. To create a fully realized review, further research and data extraction would be required. The tables and figures would be created based on the data collected.\n\n\n\nTitle: Unlocking Insights: A Systematic Review of Data Mining and Machine Learning Applications Abstract: This systematic literature review explores the applications of data mining and machine learning across various domains. A comprehensive analysis of 150 studies reveals emerging trends, gaps, and future directions. The review highlights the transformative potential of these technologies in driving decision-making, improving outcomes, and enhancing efficiency. Methodology: Database selection: Scopus, Web of Science, and IEEE Xplore. Search query: (“data mining” OR “machine learning”) AND (“application” OR “implementation”). Inclusion criteria: Peer-reviewed articles, conference papers, and book chapters published between 2015 and 2022. Exclusion criteria: Studies focusing solely on theoretical aspects, surveys, or reviews. Data extraction: Study characteristics, application domains, techniques used, and outcomes. Trends: Increasing adoption: Data mining and machine learning applications have grown exponentially across industries. Domain expansion: Emerging applications in healthcare, finance, and education. Technique diversification: Integration of deep learning, natural language processing, and transfer learning. Domain Number of Studies Prominent Techniques Healthcare 30 Random Forest, Support Vector Machines Finance 20 Deep Learning, Gradient Boosting Education 15 Clustering, Decision Trees Gap Analysis: Limited interpretability: Many studies lack transparent explanations of complex models. Data quality issues: Insufficient attention to data preprocessing and quality assessment. Scalability concerns: Few studies address the challenges of deploying models in large-scale environments. Supporting Quotes: “The application of machine learning in healthcare has the potential to revolutionize patient care and outcomes.” (Rajkomar et al., 2019) “Data mining techniques can help financial institutions detect fraudulent activities and improve risk management.” (Srivastava et al., 2018) Figures and Charts: Figure 1: Distribution of studies across domains Chart 1: Techniques used in data mining and machine learning applications Future Outlook: Explainable AI: Developing techniques for transparent and interpretable models. Edge AI: Deploying models on edge devices for real-time processing. Human-AI collaboration: Designing systems that facilitate human-AI interaction. Conclusion: This systematic review highlights the vast potential of data mining and machine learning applications across industries. While challenges persist, emerging trends and techniques offer promising solutions. Future research should focus on addressing gaps, improving interpretability, and developing human-AI collaborative systems.\nTitle: Unlocking Insights: A Systematic Review of Data Mining and Machine Learning Applications Abstract: This systematic literature review explores the applications of data mining and machine learning across various domains. A comprehensive analysis of 150 studies reveals emerging trends, gaps, and future directions. The review highlights the transformative potential of these technologies in driving decision-making, improving outcomes, and enhancing efficiency. Methodology: Database selection: Scopus, Web of Science, and IEEE Xplore. Search query: (“data mining” OR “machine learning”) AND (“application” OR “implementation”). Inclusion criteria: Peer-reviewed articles, conference papers, and book chapters published between 2015 and 2022. Exclusion criteria: Studies focusing solely on theoretical aspects, surveys, or reviews. Data extraction: Study characteristics, application domains, techniques used, and outcomes. Trends: Increasing adoption: Data mining and machine learning applications have grown exponentially across industries (Kumar et al., 2020). Domain expansion: Emerging applications in healthcare, finance, and education (Sharma et al., 2019). Technique diversification: Integration of deep learning, natural language processing, and transfer learning (Rajkomar et al., 2019). Domain Number of Studies Prominent Techniques Healthcare 30 Random Forest, Support Vector Machines Finance 20 Deep Learning, Gradient Boosting Education 15 Clustering, Decision Trees Gap Analysis: Limited interpretability: Many studies lack transparent explanations of complex models (Gunning, 2017). Data quality issues: Insufficient attention to data preprocessing and quality assessment (Srivastava et al., 2018). Scalability concerns: Few studies address the challenges of deploying models in large-scale environments (Chen et al., 2019). Supporting Quotes: “The application of machine learning in healthcare has the potential to revolutionize patient care and outcomes.” (Rajkomar et al., 2019) “Data mining techniques can help financial institutions detect fraudulent activities and improve risk management.” (Srivastava et al., 2018) Figures and Charts: Figure 1: Distribution of studies across domains Chart 1: Techniques used in data mining and machine learning applications Future Outlook: Explainable AI: Developing techniques for transparent and interpretable models. Edge AI: Deploying models on edge devices for real-time processing. Human-AI collaboration: Designing systems that facilitate human-AI interaction. Testable Hypothesis: H1: The integration of data mining and machine learning techniques will lead to significant improvements in predictive accuracy and decision-making across various industries. References: Rajkomar, A., Oren, E., Chen, K., et al. (2019). Scalable and accurate deep learning for electronic health records. NPJ Digital Medicine, 2(1), 1-10. Srivastava, A., Singh, S. K., Tanwar, S., et al. (2018). Credit card fraud detection using data mining techniques. Journal of Intelligent Information Systems, 53(2), 257-275. Appendix: Comprehensive list of sources (150 studies) Note: The references provided are a selection of the studies reviewed and are not an exhaustive list.\n\n\n\nSystematic Literature Review on the Applications of Data Mining and Machine Learning Methodology The methodology for this literature review follows a systematic approach to identify, evaluate, and synthesize relevant research on the applications of data mining and machine learning. The steps include: Search for Relevant Literature: Conducted comprehensive searches in academic databases such as IEEE Xplore, SpringerLink, and Google Scholar using keywords like “data mining applications,” “machine learning applications,” “data mining trends,” and “machine learning trends.” Evaluate Sources: Selected peer-reviewed articles, conference papers, and reputable industry reports published between 2015 and 2025. Identify Themes, Debates, and Gaps: Analyzed the literature to identify common themes, ongoing debates, and gaps in the research. Outline Structure: Organized the review by themes, including methodology, trends, gap analysis, supporting quotes, tables, figures, charts, future outlook, conclusion, and a testable hypothesis. Write the Literature Review: Synthesized the findings and presented them in a structured format. Trends Several key trends have emerged in the applications of data mining and machine learning: AI and Machine Learning Integration: AI and machine learning are increasingly integrated into data mining processes, enhancing predictive accuracy and automating complex data analysis tasks. Real-Time Data Processing: The growth of IoT devices has driven the need for real-time data processing, enabling organizations to make instant decisions based on streaming data. Graph Analytics: Graph analytics is gaining traction for its ability to uncover complex relationships in networks, such as social media connections and supply chains. Privacy-Preserving Techniques: With stricter regulations like GDPR and CCPA, privacy-preserving techniques in data mining are becoming essential. Gap Analysis Despite significant advancements, several gaps remain in the research on data mining and machine learning: Data Quality and Preprocessing: Many studies highlight the challenges of dealing with noisy and incomplete data, which can affect the accuracy of machine learning models. Scalability: There is a need for more scalable algorithms that can handle the ever-increasing volume of data generated by modern applications. Interdisciplinary Approaches: Integrating insights from other scientific areas, such as statistics and domain-specific knowledge, remains underexplored. Supporting Quotes “The impact of AI and machine learning on data mining is transformative, enabling more accurate predictions and efficient data analysis” (Rey Ladaran, 2024). “Real-time data processing is now a must-have, especially with the explosive growth of IoT devices” (Gartner, 2023). Tables, Figures, Charts Below is a summary table of key trends in data mining and machine learning: Trend Description AI and Machine Learning Integration of AI and machine learning to enhance predictive accuracy Real-Time Data Processing Need for instant data analysis driven by IoT devices Graph Analytics Uncovering complex relationships in networks Privacy-Preserving Techniques Ensuring data privacy in compliance with regulations like GDPR and CCPA Future Outlook The future of data mining and machine learning is promising, with several emerging trends expected to shape the field: Increased Automation: Automation of data preprocessing and model building will allow data scientists to focus on higher-value tasks. Advanced AI Integration: AI algorithms will continue to evolve, providing more sophisticated tools for data analysis. Ethical Considerations: As data mining and machine learning become more pervasive, ethical considerations will play a crucial role in shaping their applications. Conclusion In conclusion, data mining and machine learning have revolutionized various industries by providing powerful tools for data analysis and decision-making. While significant progress has been made, challenges such as data quality, scalability, and interdisciplinary integration remain. Addressing these gaps will be crucial for the continued advancement of the field. One Testable Hypothesis Hypothesis: The integration of AI-driven automation in data preprocessing will significantly improve the accuracy and efficiency of machine learning models in real-time data analysis. Comprehensive List of Sources Intellspot Wiley Simplilearn Scribbr Unriddle Purdue OWL SPLACE MIT Sloan Management SpringerLink SRK DEI Essay Freelance Writers\n\n\n\n\nTask: Assess each model’s output based on:\n\nStructure: Did it include a methodology section and follow a systematic review format?\nSynthesis: Were key findings from data mining and machine learning applications well-summarized?\nTrends and Gaps: Did it identify meaningful trends and research gaps?\nHypothesis: Was the proposed hypothesis testable and relevant?\nReferences: Are the citations accurate (check using Google Scholar or Semantic Scholar)\nStudents document strengths and weaknesses. For example, ChatGPT might provide broad coverage, Copilot might excel at concise structuring, and I (Grok 3) might offer a unique perspective on emerging trends.\n\n\n\n\n\nNo length requirement was give for the response, so each of the models must have used an internally defined length limit.\nEach of the models provided a general structure for their responses. ChatGPTs structure was weak and poorly implemented. Copilot was also weak in its structure. It seemed to be a bit more dumbed down than necessary. Meta AI was more professional and better structured. Gemini was produced in an outline form. It seemed more comprehensive than the average response. Llamas response was terse, and not really informative. Similar to Copilot. DeepSeek was also terse and written\nThe focus of this exercise is a literature review, but it seems that these are very poorly structured and written. They are also generally all similar.\nA quick note. The cut and paste responses above do not reflect the output of each of the models. In general, they looked better. DeepSeek output Markdown when copied, which was very good. This had to be changed for the information above because it interfered with the formatting of this quarto document. Gemini had no copy function, requiring output be sent to Google Docs.\n\n\n\nGiven the length of all the responses, they appeared trite. All of the responses, except Llama and Copilot were broken down by groups, such as healthcare. This was not helpful and meant that many areas were not covered.\n\n\n\nThe gap and trend analysis varied widely from model-to-model to the point that a more explicit request needs to be outlined for a new prompt to get the best results.\n\n\n\nAll of the hypotheses that were returned appeared testable, but all were rather specific. All required some domain specific knowledge, and many would have been very expensive to test.\n\n\n\nNot all the references were accurate or complete for any of the models, but the URL links were all live. Copilot only provided links, but no other sources.\n\n\n\n\nTask: Revise the prompt to address deficiencies in each model’s response, creating three tailored prompts—one for ChatGPT, one for Copilot, and one for Grok 3.\nExample Refined Prompt for Grok 3: “Imagine you’re a data scientist conducting a 2,000-word systematic literature review on how data mining and machine learning are applied in domains like healthcare, finance, and education. Outline a clear methodology, synthesize key findings with fresh insights, highlight trends and gaps, and propose one bold, testable hypothesis. Maintain a rigorous academic tone.”\nStudents test these refined prompts and compare the improved outputs.\n\n\nGetting to Grok was incovenient and DeepSeek has provided better responses than most models, so DeepSeek was substituted.\nBelow are the revised prompts for each of the models.\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper, less outlined, more rigorous\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper, less outlined, more rigorous\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\nIn general, the outputs were all improved. Even though the input prompts varied only a little from model-to-model the output was uniformily improved.\n\n\n\nTask: Integrate the best elements from each model’s output into a final systematic review. Write a new prompt for the student’s preferred model (e.g., Grok 3) to synthesize the results.\nExample Synthesis Prompt: “Using these drafts from three AI models [paste outputs], produce a 2,000-word structured systematic literature review on data mining and machine learning applications. Combine the strongest methodology, findings, trends, gaps, and hypothesis into a cohesive, academically sound document.”\nStudents submit their final review and justify their synthesis decisions.\n\n\nI took the output from DeepSeek, ChatGPT and Copilot to create a 2,000 word written report from ChatGPT with the following prompt. ChatGPT was chosen because I have a paid account, and it does not give me busy errors. The actual text input was not included for brevity. The prompt is short but useful.\n\nUsing the included information create a 2000 word written report.\n\nThis is the final output from the prompt above. I have found with the latest models from ChatGPT, simple guidance such as above have produced acceptable outputs. Counterintuitively, more detail has produced stranger results. One thought is that the effort the model puts into following specific request works against the output process.\n\nApplications of Data Mining and Machine Learning: A Systematic Literature Review\nAbstract\nThis systematic literature review explores the applications of data mining and machine learning across multiple domains, including healthcare, finance, education, and cybersecurity. By analyzing current trends and discussing future prospects, this review provides a comprehensive understanding of how these technologies are utilized and where opportunities for further research lie. Additionally, a testable hypothesis is proposed to guide future investigations in this rapidly evolving field.\nIntroduction\nData mining and machine learning (ML) have become indispensable tools in the modern era of big data, driving innovations across diverse fields. These technologies enable the extraction of meaningful patterns from large datasets, facilitating decision-making processes and predictive analytics. As the volume of data generated globally continues to grow exponentially, the applications of data mining and ML have expanded in scope and complexity. This paper presents a systematic literature review of the applications of data mining and ML, focusing on their methodologies, emerging trends, and future directions. The review also identifies gaps in the current research and proposes a testable hypothesis to guide future studies.\nMethodology\nThis review adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure methodological rigor and transparency. The process involved the following steps:\nData Collection\nA comprehensive search was conducted across several academic databases, including IEEE Xplore, PubMed, SpringerLink, ACM Digital Library, and Google Scholar. The search terms included “data mining applications,” “machine learning applications,” “trends in data mining,” “machine learning in healthcare,” and “data mining in finance.” The inclusion criteria were limited to peer-reviewed articles published between 2015 and 2023 that focused on real-world applications of data mining and ML. Articles without empirical evidence, non-English publications, and purely theoretical papers were excluded.\nData Analysis\nThe selected articles were analyzed thematically to identify recurring applications, methodologies, and trends. Quantitative data were extracted and synthesized to provide a comprehensive overview of the field. Gap analysis was performed by comparing the stated objectives of the studies with their outcomes, highlighting areas where further research is needed.\nTrends in Data Mining and Machine Learning Applications\nHealthcare\nHealthcare has been one of the most prominent domains for the application of data mining and ML. These technologies have been used for predictive diagnostics, personalized medicine, and patient outcome prediction. For instance, deep learning models, particularly convolutional neural networks (CNNs), have demonstrated diagnostic accuracy comparable to human experts in radiology. Similarly, natural language processing (NLP) techniques have been employed to analyze electronic health records (EHRs), enabling the extraction of valuable insights from unstructured text data.\nDespite these advancements, challenges remain. The interpretability of ML models, particularly deep learning, is a significant barrier to their widespread adoption in clinical settings. The “black-box” nature of deep learning models limits their utility in critical applications where transparency is essential.\nFinance\nIn the financial sector, data mining and ML have been widely adopted for fraud detection, algorithmic trading, and credit scoring. Ensemble learning methods, such as XGBoost and Random Forests, have shown superior performance in detecting fraudulent transactions compared to traditional statistical methods. These methods have reduced false positives significantly, improving the efficiency of fraud detection systems.\nHowever, the financial domain also faces challenges related to data quality and scalability. Noisy and incomplete datasets can hinder the performance of ML models, while the increasing volume of financial data necessitates the development of scalable algorithms.\nEducation\nEducational institutions have increasingly turned to data mining and ML to enhance learning outcomes and operational efficiency. Applications include learning analytics, student performance prediction, and adaptive learning systems. Clustering algorithms, such as K-means, have been used to segment students based on their learning behaviors, enabling targeted interventions.\nDespite these successes, the field of educational data mining faces challenges related to data privacy and ethical considerations. The use of student data for predictive analytics raises concerns about consent and the potential for bias in algorithmic decision-making.\nCybersecurity\nCybersecurity is another domain where data mining and ML have made significant contributions. These technologies have been used for intrusion detection, malware classification, and anomaly detection. Unsupervised learning techniques, such as autoencoders, have shown promise in identifying novel attack patterns that traditional rule-based systems might miss.\nHowever, the adversarial nature of cybersecurity presents unique challenges. Attackers can exploit vulnerabilities in ML models, leading to adversarial attacks that compromise system integrity. As a result, there is a growing need for robust and resilient ML systems in cybersecurity.\nFuture Outlook\nThe future of data mining and ML is shaped by several emerging trends and technologies. Explainable AI (XAI) is gaining traction as a means to address the interpretability challenges associated with deep learning models. XAI techniques aim to provide transparent and understandable explanations for model predictions, enhancing trust and adoption in critical applications.\nFederated learning is another promising area of research. This approach enables decentralized model training, allowing multiple parties to collaborate without sharing raw data. Federated learning has the potential to address data privacy concerns, particularly in domains such as healthcare and finance.\nQuantum machine learning is also on the horizon, leveraging the computational power of quantum computers to solve complex optimization problems. While still in its infancy, quantum machine learning holds the potential to revolutionize fields such as drug discovery and financial modeling.\nConclusion\nThis systematic literature review highlights the transformative impact of data mining and ML across various domains. While these technologies have enabled significant advancements, challenges related to interpretability, data quality, and scalability remain. Future research should focus on developing robust, scalable, and interpretable models to address these challenges. Additionally, ethical considerations, particularly in domains such as education and healthcare, must be carefully addressed to ensure the responsible use of data mining and ML.\nTestable Hypothesis\nOne easily testable hypothesis emerging from this review is:\n“Ensemble learning methods, such as XGBoost and Random Forests, will outperform traditional statistical methods in fraud detection tasks, achieving at least a 20% reduction in false positives.”\nThis hypothesis can be tested by comparing the performance of ensemble learning methods with traditional statistical methods on a standardized fraud detection dataset, such as the Credit Card Fraud Detection dataset available on Kaggle.\nReferences\nChandola, V., Banerjee, A., & Kumar, V. (2018). Anomaly detection: A survey. ACM Computing Surveys, 41(3), 1-58.\nDal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2015). Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications, 41(10), 4915-4928.\nEsteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115-118.\nYang, Q., Liu, Y., Chen, T., & Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 1-19.\n\n\n\nTask: Write a reflection answering:\n\nHow did each model approach the systematic review differently?\nWhich prompt refinements yielded the best results for each model?\nWhat did you learn about leveraging AI for structured academic reviews?\n\n\n\nThe response from each model was slightly different. It seems that each model has set different priorities on the types of responses included. Some of the responses are more structured and less written. Some the opposite.\nChatGPT: Perhaps a bit too structured when structure was requested. This might be great for outlining for a paper. The refinements required to remove the request for structure.\nCopilot seemed a bit dumbed-down. Although it included many words, it did not include much content. This required pushing the model for details.\nDeepSeek: This was my favorite model for this exercise. It had a good balance between structure and content. Model refinements were needed to get additional details. This model also seemed to produce valid sources.\nFor structured academic reviews, like most things, good, targeted questions lead to the best answers."
  },
  {
    "objectID": "assignment03.html#step-1-initial-prompt-creation",
    "href": "assignment03.html#step-1-initial-prompt-creation",
    "title": "Assignment 03",
    "section": "",
    "text": "Task: Write a baseline prompt to request a structured systematic literature review on data mining and machine learning applications.\nExample Prompt: “Conduct a 2,000-word structured systematic literature review on the applications of data mining and machine learning in real-world domains. Include a methodology section, synthesize key findings, identify trends and gaps, and propose one testable hypothesis. Use an academic tone and emulate systematic review standards.”\nStudents submit this prompt to ChatGPT, Copilot, and Grok 3, collecting the raw outputs.\n\n\nPlease note that the rest of this document is a rainbow of colors to help keep inputs and responses, separate from the original questions and my answers to them.\n\n\nGrey - Original Assignment Text\n\n\n\nGreen - Answers to Original Assignment\n\n\n\nBlue - Document Headings\n\n\n\nLight Purple - Prompts\n\n\n\nDark Purple - Model Output\n\n\nHopfully, this will make this document easier to digest.\nNote: Some of the raw outputs have been adjusted to remove some Markdown formatting because it conflicts with the formatting of this document.\n\n\nThis is the original prompt\n\nCreate a structured, systematic literature review on the applications of data mining and machine learning.\nInclude: Methodology Trends Gap analysis Supporting quotes Tables, Figures, Charts Future Outlook Conclusion One Testable Hypothesis\nRequirements: All statements must be real and true A comprehensive list of sources Organize by themes APSA format and academic tone\n\n\n\nMethodology\nThis systematic literature review examines the applications of data mining and machine learning across various domains. The methodology involved a comprehensive search of academic databases, including PubMed, MEDLINE, Google Scholar, and SpringerLink, to identify relevant studies. Keywords such as “data mining applications,” “machine learning applications,” and “systematic literature review” were utilized. Inclusion criteria encompassed peer-reviewed articles published in English that discussed practical applications of data mining and machine learning. Studies focusing solely on theoretical aspects without empirical validation were excluded. The selected articles were categorized based on application domains, methodologies employed, and outcomes achieved.\nTrends\nThe analysis revealed several prominent trends in the application of data mining and machine learning: 1. Healthcare and Medicine: There is a significant increase in utilizing machine learning for predictive analytics in patient diagnosis, treatment planning, and personalized medicine. For instance, machine learning algorithms have been applied to predict patient outcomes and assist in clinical decision-making. 2. Environmental Science: Data mining techniques are increasingly used to analyze environmental data, such as air quality monitoring and pollution prediction. A systematic review highlighted the application of machine learning methods in air pollution epidemiology, aiding in forecasting and source apportionment. ￼ 3. Education: Educational data mining has emerged as a tool to analyze student performance and improve educational outcomes. A review discussed the use of data mining and machine learning algorithms to predict student success and identify areas needing intervention. ￼ 4. Finance: Predictive analytics and machine learning models are employed for credit scoring, fraud detection, and risk assessment, enhancing decision-making processes in financial institutions. ￼\nGap Analysis\nDespite the advancements, certain gaps persist: 1. Data Quality and Integration: Many studies highlight challenges related to data quality, including missing values and inconsistencies, which hinder the effectiveness of machine learning models. Integrating heterogeneous data sources remains a significant obstacle. 2. Model Interpretability: Complex models, such as deep neural networks, often lack interpretability, making it difficult for practitioners to trust and adopt these models in critical applications like healthcare. 3. Ethical Considerations: There is a growing concern about the ethical implications of data mining and machine learning, particularly regarding privacy, bias, and fairness. Few studies address these issues comprehensively.\nTestable Hypothesis\nImplementing explainable AI models in healthcare diagnostics will lead to higher adoption rates among medical professionals compared to traditional black-box models.\nSupporting Quotes • “Appropriate application of Data Mining procedures can provide multiple indicators of student performance.” ￼ • “Data mining is becoming an increasingly common tool in environmental health.” ￼\nTables, Figures, Charts\nTable 1: Applications of Data Mining and Machine Learning by Domain\nDomain Applications References Healthcare Patient diagnosis, treatment planning\nEnvironmental Science Air quality monitoring, pollution prediction\nEducation Student performance analysis\nFinance Credit scoring, fraud detection\nFigure 1: Trends in Machine Learning Applications Across Domains\nChart 1: Challenges in Data Mining and Machine Learning Implementations\nFuture Outlook\nThe future of data mining and machine learning applications is promising, with anticipated advancements in the following areas: 1. Explainable AI: Developing models that provide transparent and interpretable results to address the trust and adoption issues in critical fields like healthcare. 2. Automated Machine Learning (AutoML): Simplifying the model development process to make machine learning accessible to non-experts, thereby broadening its application scope. 3. Integration with Big Data Technologies: Combining machine learning with big data platforms to efficiently process and analyze vast amounts of data, leading to more accurate and timely insights. 4. Ethical Frameworks: Establishing robust ethical guidelines to ensure the responsible use of data mining and machine learning, focusing on privacy, bias mitigation, and fairness.\nConclusion\nData mining and machine learning have become integral tools across various sectors, offering significant improvements in decision-making and efficiency. However, challenges related to data quality, model interpretability, and ethical considerations must be addressed to fully harness their potential. Future research should focus on developing transparent models, improving data integration techniques, and establishing ethical standards to guide the responsible application of these technologies.\nComprehensive List of Sources • Tawde, Pallavi Devendra, and Yogesh Kumar Sharma. “Systematic Review on Data Mining and Machine Learning Algorithms for Analysing Student’s Performance.” Utkal Historical Research Journal 34, no. III (2021): 108-117. • Chen, Hsinchun, et al. “A Systematic Review of Data Mining and Machine Learning for Air Pollution Epidemiology.” BMC Public Health 17, no. 1 (2017): 1-19. • Katuwal, Gopal J., and Prasanna K. Kolhe. “Systematic Literature Review of Machine Learning Methods Used in Respiratory Sound Analysis.” BMC Medical Informatics and Decision Making 21, no. 1 (2021): 1-16. • Bellazzi, Riccardo, and Blaz Zupan. “Predictive Data Mining in Clinical Medicine: Current Issues and Guidelines.” International Journal of Medical Informatics 77, no. 2 (2008): 81-97. • Azeem, Ibraheem. *On the Evolution of Data Science and Machine Learning\n\n\nStructured Systematic Literature Review: Applications of Data Mining and Machine Learning\n\n1. Introduction Data mining and machine learning (ML) have emerged as transformative technologies across various domains, including healthcare, finance, education, and cybersecurity. This review systematically examines the applications, trends, and gaps in the literature, providing a structured analysis of the current state of research. The methodology, trends, and future outlook are discussed, supported by evidence from peer-reviewed studies.\n\n2. Methodology This review follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) framework to ensure a rigorous and transparent process.\n2.1 Data Collection - Databases Searched: IEEE Xplore, PubMed, SpringerLink, ACM Digital Library, and Google Scholar. - Keywords: “data mining applications,” “machine learning applications,” “trends in data mining,” “machine learning in healthcare,” “data mining in finance.” - Inclusion Criteria: Peer-reviewed articles published between 2015 and 2023, focusing on real-world applications of data mining and ML. - Exclusion Criteria: Articles without empirical evidence, non-English publications, and theoretical papers without practical applications.\n2.2 Data Analysis - Thematic analysis was conducted to identify recurring themes and trends. - Gap analysis was performed by comparing the stated objectives of studies with their outcomes. - Quantitative data were visualized using tables and charts.\n\n3. Trends in Data Mining and Machine Learning Applications\n3.1 Healthcare - Applications: Predictive diagnostics, personalized medicine, and patient outcome prediction. - Trends: Increased use of deep learning for medical image analysis and natural language processing (NLP) for electronic health record (EHR) analysis. - Supporting Quote: “Machine learning models, particularly convolutional neural networks (CNNs), have achieved diagnostic accuracy comparable to human experts in radiology” (Esteva et al., 2017).\n3.2 Finance - Applications: Fraud detection, algorithmic trading, and credit scoring. - Trends: Adoption of ensemble learning methods like XGBoost and Random Forests for improved predictive accuracy. - Supporting Quote: “Ensemble methods have reduced false positives in fraud detection by 20% compared to traditional statistical methods” (Dal Pozzolo et al., 2015).\n3.3 Education - Applications: Learning analytics, student performance prediction, and adaptive learning systems. - Trends: Growth in the use of clustering algorithms to identify at-risk students and recommend interventions. - Supporting Quote: “K-means clustering has been effective in segmenting students based on learning behaviors, enabling targeted interventions” (Romero & Ventura, 2020).\n3.4 Cybersecurity - Applications: Intrusion detection, malware classification, and anomaly detection. - Trends: Shift towards unsupervised learning for detecting zero-day attacks and adversarial ML for robust systems. - Supporting Quote: “Unsupervised learning techniques like autoencoders have shown promise in identifying novel attack patterns” (Chandola et al., 2018).\n\n4. Gap Analysis Despite advancements, several gaps persist in the literature:\n4.1 Interpretability - Many ML models, especially deep learning, lack interpretability, limiting their adoption in critical fields like healthcare. - Supporting Quote: “The ‘black-box’ nature of deep learning models remains a significant barrier to their deployment in clinical settings” (Samek et al., 2019).\n4.2 Data Quality - Poor data quality and lack of labeled datasets hinder the performance of supervised learning models. - Supporting Quote: “Insufficient and noisy data are the primary challenges in applying ML to real-world problems” (Provost & Fawcett, 2013).\n4.3 Scalability - Many algorithms struggle with scalability when applied to large datasets. - Supporting Quote: “Scalability issues in traditional ML algorithms have led to the development of distributed computing frameworks like Apache Spark” (Zaharia et al., 2016).\n\n5. Visual Representation\nTable 1: Applications of Data Mining and Machine Learning by Domain | Domain | Applications | Common Algorithms | |——————|————————————–|——————————–| | Healthcare | Predictive diagnostics, EHR analysis | CNNs, SVM, NLP | | Finance | Fraud detection, credit scoring | XGBoost, Random Forests | | Education | Learning analytics, adaptive systems | K-means, Decision Trees | | Cybersecurity | Intrusion detection, malware analysis| Autoencoders, GANs |\nFigure 1: Trends in ML Applications (2015-2023) \nCaption: Growth in deep learning and ensemble methods across domains.\n\n6. Future Outlook - Explainable AI (XAI): Development of interpretable models to enhance trust and adoption. - Federated Learning: Addressing data privacy concerns by enabling decentralized model training. - Quantum Machine Learning: Leveraging quantum computing for solving complex optimization problems. - Supporting Quote: “Federated learning is poised to revolutionize data privacy in ML applications” (Yang et al., 2019).\n\n7. Conclusion This review highlights the transformative potential of data mining and ML across various domains while identifying critical gaps in interpretability, data quality, and scalability. Future research should focus on developing robust, scalable, and interpretable models to address these challenges.\n\n8. References - Chandola, V., Banerjee, A., & Kumar, V. (2018). Anomaly detection: A survey. ACM Computing Surveys, 41(3), 1-58. - Dal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2015). Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications, 41(10), 4915-4928. - Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115-118. - Provost, F., & Fawcett, T. (2013). Data science for business: What you need to know about data mining and data-analytic thinking. O’Reilly Media. - Romero, C., & Ventura, S. (2020). Educational data mining and learning analytics: An updated survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 10(3), e1355. - Samek, W., Montavon, G., Vedaldi, A., Hansen, L. K., & Müller, K. R. (2019). Explainable AI: Interpreting, explaining and visualizing deep learning. Springer. - Yang, Q., Liu, Y., Chen, T., & Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 1-19. - Zaharia, M., Xin, R. S., Wendell, P., Das, T., Armbrust, M., Dave, A., … & Stoica, I. (2016). Apache Spark: A unified engine for big data processing. Communications of the ACM, 59(11), 56-65.\n\nThis review adheres to APSA formatting and maintains an academic tone, ensuring credibility and relevance for scholarly audiences.\n\n\n\nApplications of Data Mining and Machine Learning: A Systematic Literature Review Abstract:\nThis review systematically examines the applications of data mining and machine learning (DM/ML) across diverse domains. Employing a structured methodology, it identifies prevailing trends, conducts a gap analysis, and outlines future research directions. By synthesizing findings from a comprehensive literature search, this review provides a nuanced understanding of the transformative impact of DM/ML.\n\nIntroduction:\n\nData mining and machine learning have evolved into pivotal tools for extracting valuable insights and automating decision-making from vast datasets. Their application spans various sectors, including healthcare, finance, marketing, and environmental science. This review aims to systematically analyze the existing literature to identify key applications, trends, and research gaps.\n\nMethodology:\n\nThis review adheres to a systematic approach, encompassing the following stages:\nSearch Strategy: A comprehensive search was conducted using databases such as IEEE Xplore, ACM Digital Library, ScienceDirect, PubMed, and Google Scholar. Keywords included “data mining,” “machine learning,” “applications,” “healthcare,” “finance,” “marketing,” “environmental science,” and related terms. Inclusion/Exclusion Criteria: Studies were included if they: Presented empirical applications of DM/ML. Were published in peer-reviewed journals or conference proceedings. Were written in English. Were published within the last decade (with exceptions for foundational works). Studies were excluded if they: Were theoretical papers without empirical applications. Were non-peer-reviewed or grey literature. Data Extraction: Relevant information was extracted from each study, including: Application domain. DM/ML techniques used. Dataset characteristics. Performance metrics. Key findings. Synthesis and Analysis: Extracted data were synthesized and analyzed to identify prevailing trends, research gaps, and future directions. 3. Themes and Trends:\n3.1. Healthcare:\nDM/ML algorithms are widely used for disease prediction, diagnosis, and treatment. Applications include medical image analysis, genomics, and electronic health record analysis. Trend: Increased use of deep learning for complex medical imaging tasks. “Machine learning algorithms have demonstrated significant potential in predicting disease progression and improving patient outcomes” (Obermeyer & Emanuel, 2016). Table 1: Healthcare Applications of DM/ML | Application | DM/ML Technique | Example | | :——————- | :——————— | :————————————— | | Disease Prediction | Logistic Regression, SVM | Predicting cardiovascular disease risk | | Medical Image Analysis | Convolutional Neural Networks | Detecting tumors in MRI scans | | Genomics | Clustering, Classification | Identifying genetic markers for diseases | 3.2. Finance:\nDM/ML is employed for fraud detection, risk management, and algorithmic trading. Applications include credit scoring, stock market prediction, and customer churn analysis. Trend: Growing adoption of reinforcement learning for algorithmic trading. “The use of machine learning in financial fraud detection has significantly improved accuracy and reduced false positives” (Bolton & Hand, 2002). Figure 1: Financial Applications of DM/ML (A pie chart could be used to show the distribution of DM/ML applications in finance, such as fraud detection, risk management, and algorithmic trading.) 3.3. Marketing:\nDM/ML enables personalized marketing, customer segmentation, and recommendation systems. Applications include sentiment analysis, social media analytics, and targeted advertising. Trend: Increasing focus on ethical considerations in personalized marketing. “Machine learning algorithms are increasingly used to personalize customer experiences and improve marketing campaign effectiveness” (Wedel & Kannan, 2016). 3.4. Environmental Science:\nDM/ML is used for climate modeling, air quality prediction, and resource management. Applications include remote sensing data analysis and ecological modeling. Trend: Development of AI-powered solutions for environmental sustainability. “Data mining techniques are crucial for analyzing environmental data and extracting insights to support sustainable resource management” (Recknagel et al., 2001). 4. Gap Analysis:\nData Quality and Availability: Many applications suffer from limited data availability or poor data quality. Interpretability and Explainability: Complex DM/ML models, particularly deep learning, often lack interpretability, hindering trust and adoption. Ethical Considerations: Bias in algorithms, privacy concerns, and the potential for misuse require careful attention. “The ‘black box’ nature of many machine learning models poses challenges for understanding and trusting their predictions” (Doshi-Velez & Kim, 2017). Generalizability: Models trained on specific datasets may not generalize well to other populations or contexts. Integration: Integration of DM/ML into existing workflows and systems remains a significant challenge. 5. Future Outlook:\nExplainable AI (XAI): Research will focus on developing methods to make DM/ML models more transparent and interpretable. Federated Learning: This approach allows training models on decentralized data, addressing privacy concerns. Automated Machine Learning (AutoML): AutoML tools will simplify the process of developing and deploying DM/ML models. AI for Social Good: Increased focus on using DM/ML to address societal challenges, such as poverty, inequality, and climate change. Hybrid AI: Combining different AI methods, such as symbolic AI and machine learning, to create more robust and adaptable systems. Edge AI: Implementing machine learning directly on edge devices, enabling faster and more efficient data processing. 6. Conclusion:\nData mining and machine learning have revolutionized numerous domains, offering powerful tools for extracting insights and automating decision-making. However, challenges related to data quality, interpretability, and ethical considerations remain. Future research should focus on addressing these gaps and developing more robust, transparent, and ethical DM/ML solutions. The integration of explainable AI, federated learning, and AutoML will play a crucial role in expanding the reach and impact of these technologies.\n\nTestable Hypothesis:\n\nHypothesis: Deep learning models trained on multimodal medical imaging data (e.g., combining MRI and PET scans) will demonstrate significantly higher accuracy in early-stage Alzheimer’s disease detection compared to models trained on single-modality data.\n\nReferences:\n\nBolton, R. J., & Hand, D. J. (2002). Statistical fraud detection: A review. Statistical science, 17(3), 235-255. Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608. Obermeyer, Z., & Emanuel, E. J. (2016). Predicting the future—big data, machine learning, and clinical medicine. New England Journal of Medicine, 375(13), 1216-1219. 1   1. rsdjournal.org rsdjournal.org Recknagel, F., French, M., Harker, P., & Yabunaka, K. I. (2001). Artificial neural network approach for modelling and prediction of algal blooms. Ecological modelling, 146(1-3), 81-91. Wedel, M., & Kannan, P. K. (2016). Marketing analytics for data-rich environments. Journal of Marketing, 80(6), 97-121. Note: This is a structured outline. To create a fully realized review, further research and data extraction would be required. The tables and figures would be created based on the data collected.\n\n\n\nTitle: Unlocking Insights: A Systematic Review of Data Mining and Machine Learning Applications Abstract: This systematic literature review explores the applications of data mining and machine learning across various domains. A comprehensive analysis of 150 studies reveals emerging trends, gaps, and future directions. The review highlights the transformative potential of these technologies in driving decision-making, improving outcomes, and enhancing efficiency. Methodology: Database selection: Scopus, Web of Science, and IEEE Xplore. Search query: (“data mining” OR “machine learning”) AND (“application” OR “implementation”). Inclusion criteria: Peer-reviewed articles, conference papers, and book chapters published between 2015 and 2022. Exclusion criteria: Studies focusing solely on theoretical aspects, surveys, or reviews. Data extraction: Study characteristics, application domains, techniques used, and outcomes. Trends: Increasing adoption: Data mining and machine learning applications have grown exponentially across industries. Domain expansion: Emerging applications in healthcare, finance, and education. Technique diversification: Integration of deep learning, natural language processing, and transfer learning. Domain Number of Studies Prominent Techniques Healthcare 30 Random Forest, Support Vector Machines Finance 20 Deep Learning, Gradient Boosting Education 15 Clustering, Decision Trees Gap Analysis: Limited interpretability: Many studies lack transparent explanations of complex models. Data quality issues: Insufficient attention to data preprocessing and quality assessment. Scalability concerns: Few studies address the challenges of deploying models in large-scale environments. Supporting Quotes: “The application of machine learning in healthcare has the potential to revolutionize patient care and outcomes.” (Rajkomar et al., 2019) “Data mining techniques can help financial institutions detect fraudulent activities and improve risk management.” (Srivastava et al., 2018) Figures and Charts: Figure 1: Distribution of studies across domains Chart 1: Techniques used in data mining and machine learning applications Future Outlook: Explainable AI: Developing techniques for transparent and interpretable models. Edge AI: Deploying models on edge devices for real-time processing. Human-AI collaboration: Designing systems that facilitate human-AI interaction. Conclusion: This systematic review highlights the vast potential of data mining and machine learning applications across industries. While challenges persist, emerging trends and techniques offer promising solutions. Future research should focus on addressing gaps, improving interpretability, and developing human-AI collaborative systems.\nTitle: Unlocking Insights: A Systematic Review of Data Mining and Machine Learning Applications Abstract: This systematic literature review explores the applications of data mining and machine learning across various domains. A comprehensive analysis of 150 studies reveals emerging trends, gaps, and future directions. The review highlights the transformative potential of these technologies in driving decision-making, improving outcomes, and enhancing efficiency. Methodology: Database selection: Scopus, Web of Science, and IEEE Xplore. Search query: (“data mining” OR “machine learning”) AND (“application” OR “implementation”). Inclusion criteria: Peer-reviewed articles, conference papers, and book chapters published between 2015 and 2022. Exclusion criteria: Studies focusing solely on theoretical aspects, surveys, or reviews. Data extraction: Study characteristics, application domains, techniques used, and outcomes. Trends: Increasing adoption: Data mining and machine learning applications have grown exponentially across industries (Kumar et al., 2020). Domain expansion: Emerging applications in healthcare, finance, and education (Sharma et al., 2019). Technique diversification: Integration of deep learning, natural language processing, and transfer learning (Rajkomar et al., 2019). Domain Number of Studies Prominent Techniques Healthcare 30 Random Forest, Support Vector Machines Finance 20 Deep Learning, Gradient Boosting Education 15 Clustering, Decision Trees Gap Analysis: Limited interpretability: Many studies lack transparent explanations of complex models (Gunning, 2017). Data quality issues: Insufficient attention to data preprocessing and quality assessment (Srivastava et al., 2018). Scalability concerns: Few studies address the challenges of deploying models in large-scale environments (Chen et al., 2019). Supporting Quotes: “The application of machine learning in healthcare has the potential to revolutionize patient care and outcomes.” (Rajkomar et al., 2019) “Data mining techniques can help financial institutions detect fraudulent activities and improve risk management.” (Srivastava et al., 2018) Figures and Charts: Figure 1: Distribution of studies across domains Chart 1: Techniques used in data mining and machine learning applications Future Outlook: Explainable AI: Developing techniques for transparent and interpretable models. Edge AI: Deploying models on edge devices for real-time processing. Human-AI collaboration: Designing systems that facilitate human-AI interaction. Testable Hypothesis: H1: The integration of data mining and machine learning techniques will lead to significant improvements in predictive accuracy and decision-making across various industries. References: Rajkomar, A., Oren, E., Chen, K., et al. (2019). Scalable and accurate deep learning for electronic health records. NPJ Digital Medicine, 2(1), 1-10. Srivastava, A., Singh, S. K., Tanwar, S., et al. (2018). Credit card fraud detection using data mining techniques. Journal of Intelligent Information Systems, 53(2), 257-275. Appendix: Comprehensive list of sources (150 studies) Note: The references provided are a selection of the studies reviewed and are not an exhaustive list.\n\n\n\nSystematic Literature Review on the Applications of Data Mining and Machine Learning Methodology The methodology for this literature review follows a systematic approach to identify, evaluate, and synthesize relevant research on the applications of data mining and machine learning. The steps include: Search for Relevant Literature: Conducted comprehensive searches in academic databases such as IEEE Xplore, SpringerLink, and Google Scholar using keywords like “data mining applications,” “machine learning applications,” “data mining trends,” and “machine learning trends.” Evaluate Sources: Selected peer-reviewed articles, conference papers, and reputable industry reports published between 2015 and 2025. Identify Themes, Debates, and Gaps: Analyzed the literature to identify common themes, ongoing debates, and gaps in the research. Outline Structure: Organized the review by themes, including methodology, trends, gap analysis, supporting quotes, tables, figures, charts, future outlook, conclusion, and a testable hypothesis. Write the Literature Review: Synthesized the findings and presented them in a structured format. Trends Several key trends have emerged in the applications of data mining and machine learning: AI and Machine Learning Integration: AI and machine learning are increasingly integrated into data mining processes, enhancing predictive accuracy and automating complex data analysis tasks. Real-Time Data Processing: The growth of IoT devices has driven the need for real-time data processing, enabling organizations to make instant decisions based on streaming data. Graph Analytics: Graph analytics is gaining traction for its ability to uncover complex relationships in networks, such as social media connections and supply chains. Privacy-Preserving Techniques: With stricter regulations like GDPR and CCPA, privacy-preserving techniques in data mining are becoming essential. Gap Analysis Despite significant advancements, several gaps remain in the research on data mining and machine learning: Data Quality and Preprocessing: Many studies highlight the challenges of dealing with noisy and incomplete data, which can affect the accuracy of machine learning models. Scalability: There is a need for more scalable algorithms that can handle the ever-increasing volume of data generated by modern applications. Interdisciplinary Approaches: Integrating insights from other scientific areas, such as statistics and domain-specific knowledge, remains underexplored. Supporting Quotes “The impact of AI and machine learning on data mining is transformative, enabling more accurate predictions and efficient data analysis” (Rey Ladaran, 2024). “Real-time data processing is now a must-have, especially with the explosive growth of IoT devices” (Gartner, 2023). Tables, Figures, Charts Below is a summary table of key trends in data mining and machine learning: Trend Description AI and Machine Learning Integration of AI and machine learning to enhance predictive accuracy Real-Time Data Processing Need for instant data analysis driven by IoT devices Graph Analytics Uncovering complex relationships in networks Privacy-Preserving Techniques Ensuring data privacy in compliance with regulations like GDPR and CCPA Future Outlook The future of data mining and machine learning is promising, with several emerging trends expected to shape the field: Increased Automation: Automation of data preprocessing and model building will allow data scientists to focus on higher-value tasks. Advanced AI Integration: AI algorithms will continue to evolve, providing more sophisticated tools for data analysis. Ethical Considerations: As data mining and machine learning become more pervasive, ethical considerations will play a crucial role in shaping their applications. Conclusion In conclusion, data mining and machine learning have revolutionized various industries by providing powerful tools for data analysis and decision-making. While significant progress has been made, challenges such as data quality, scalability, and interdisciplinary integration remain. Addressing these gaps will be crucial for the continued advancement of the field. One Testable Hypothesis Hypothesis: The integration of AI-driven automation in data preprocessing will significantly improve the accuracy and efficiency of machine learning models in real-time data analysis. Comprehensive List of Sources Intellspot Wiley Simplilearn Scribbr Unriddle Purdue OWL SPLACE MIT Sloan Management SpringerLink SRK DEI Essay Freelance Writers\n\n\n\n\nTask: Assess each model’s output based on:\n\nStructure: Did it include a methodology section and follow a systematic review format?\nSynthesis: Were key findings from data mining and machine learning applications well-summarized?\nTrends and Gaps: Did it identify meaningful trends and research gaps?\nHypothesis: Was the proposed hypothesis testable and relevant?\nReferences: Are the citations accurate (check using Google Scholar or Semantic Scholar)\nStudents document strengths and weaknesses. For example, ChatGPT might provide broad coverage, Copilot might excel at concise structuring, and I (Grok 3) might offer a unique perspective on emerging trends.\n\n\n\n\n\nNo length requirement was give for the response, so each of the models must have used an internally defined length limit.\nEach of the models provided a general structure for their responses. ChatGPTs structure was weak and poorly implemented. Copilot was also weak in its structure. It seemed to be a bit more dumbed down than necessary. Meta AI was more professional and better structured. Gemini was produced in an outline form. It seemed more comprehensive than the average response. Llamas response was terse, and not really informative. Similar to Copilot. DeepSeek was also terse and written\nThe focus of this exercise is a literature review, but it seems that these are very poorly structured and written. They are also generally all similar.\nA quick note. The cut and paste responses above do not reflect the output of each of the models. In general, they looked better. DeepSeek output Markdown when copied, which was very good. This had to be changed for the information above because it interfered with the formatting of this quarto document. Gemini had no copy function, requiring output be sent to Google Docs.\n\n\n\nGiven the length of all the responses, they appeared trite. All of the responses, except Llama and Copilot were broken down by groups, such as healthcare. This was not helpful and meant that many areas were not covered.\n\n\n\nThe gap and trend analysis varied widely from model-to-model to the point that a more explicit request needs to be outlined for a new prompt to get the best results.\n\n\n\nAll of the hypotheses that were returned appeared testable, but all were rather specific. All required some domain specific knowledge, and many would have been very expensive to test.\n\n\n\nNot all the references were accurate or complete for any of the models, but the URL links were all live. Copilot only provided links, but no other sources.\n\n\n\n\nTask: Revise the prompt to address deficiencies in each model’s response, creating three tailored prompts—one for ChatGPT, one for Copilot, and one for Grok 3.\nExample Refined Prompt for Grok 3: “Imagine you’re a data scientist conducting a 2,000-word systematic literature review on how data mining and machine learning are applied in domains like healthcare, finance, and education. Outline a clear methodology, synthesize key findings with fresh insights, highlight trends and gaps, and propose one bold, testable hypothesis. Maintain a rigorous academic tone.”\nStudents test these refined prompts and compare the improved outputs.\n\n\nGetting to Grok was incovenient and DeepSeek has provided better responses than most models, so DeepSeek was substituted.\nBelow are the revised prompts for each of the models.\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper, less outlined, more rigorous\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper, less outlined, more rigorous\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\nIn general, the outputs were all improved. Even though the input prompts varied only a little from model-to-model the output was uniformily improved.\n\n\n\nTask: Integrate the best elements from each model’s output into a final systematic review. Write a new prompt for the student’s preferred model (e.g., Grok 3) to synthesize the results.\nExample Synthesis Prompt: “Using these drafts from three AI models [paste outputs], produce a 2,000-word structured systematic literature review on data mining and machine learning applications. Combine the strongest methodology, findings, trends, gaps, and hypothesis into a cohesive, academically sound document.”\nStudents submit their final review and justify their synthesis decisions.\n\n\nI took the output from DeepSeek, ChatGPT and Copilot to create a 2,000 word written report from ChatGPT with the following prompt. ChatGPT was chosen because I have a paid account, and it does not give me busy errors. The actual text input was not included for brevity. The prompt is short but useful.\n\nUsing the included information create a 2000 word written report.\n\nThis is the final output from the prompt above. I have found with the latest models from ChatGPT, simple guidance such as above have produced acceptable outputs. Counterintuitively, more detail has produced stranger results. One thought is that the effort the model puts into following specific request works against the output process.\n\nApplications of Data Mining and Machine Learning: A Systematic Literature Review\nAbstract\nThis systematic literature review explores the applications of data mining and machine learning across multiple domains, including healthcare, finance, education, and cybersecurity. By analyzing current trends and discussing future prospects, this review provides a comprehensive understanding of how these technologies are utilized and where opportunities for further research lie. Additionally, a testable hypothesis is proposed to guide future investigations in this rapidly evolving field.\nIntroduction\nData mining and machine learning (ML) have become indispensable tools in the modern era of big data, driving innovations across diverse fields. These technologies enable the extraction of meaningful patterns from large datasets, facilitating decision-making processes and predictive analytics. As the volume of data generated globally continues to grow exponentially, the applications of data mining and ML have expanded in scope and complexity. This paper presents a systematic literature review of the applications of data mining and ML, focusing on their methodologies, emerging trends, and future directions. The review also identifies gaps in the current research and proposes a testable hypothesis to guide future studies.\nMethodology\nThis review adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure methodological rigor and transparency. The process involved the following steps:\nData Collection\nA comprehensive search was conducted across several academic databases, including IEEE Xplore, PubMed, SpringerLink, ACM Digital Library, and Google Scholar. The search terms included “data mining applications,” “machine learning applications,” “trends in data mining,” “machine learning in healthcare,” and “data mining in finance.” The inclusion criteria were limited to peer-reviewed articles published between 2015 and 2023 that focused on real-world applications of data mining and ML. Articles without empirical evidence, non-English publications, and purely theoretical papers were excluded.\nData Analysis\nThe selected articles were analyzed thematically to identify recurring applications, methodologies, and trends. Quantitative data were extracted and synthesized to provide a comprehensive overview of the field. Gap analysis was performed by comparing the stated objectives of the studies with their outcomes, highlighting areas where further research is needed.\nTrends in Data Mining and Machine Learning Applications\nHealthcare\nHealthcare has been one of the most prominent domains for the application of data mining and ML. These technologies have been used for predictive diagnostics, personalized medicine, and patient outcome prediction. For instance, deep learning models, particularly convolutional neural networks (CNNs), have demonstrated diagnostic accuracy comparable to human experts in radiology. Similarly, natural language processing (NLP) techniques have been employed to analyze electronic health records (EHRs), enabling the extraction of valuable insights from unstructured text data.\nDespite these advancements, challenges remain. The interpretability of ML models, particularly deep learning, is a significant barrier to their widespread adoption in clinical settings. The “black-box” nature of deep learning models limits their utility in critical applications where transparency is essential.\nFinance\nIn the financial sector, data mining and ML have been widely adopted for fraud detection, algorithmic trading, and credit scoring. Ensemble learning methods, such as XGBoost and Random Forests, have shown superior performance in detecting fraudulent transactions compared to traditional statistical methods. These methods have reduced false positives significantly, improving the efficiency of fraud detection systems.\nHowever, the financial domain also faces challenges related to data quality and scalability. Noisy and incomplete datasets can hinder the performance of ML models, while the increasing volume of financial data necessitates the development of scalable algorithms.\nEducation\nEducational institutions have increasingly turned to data mining and ML to enhance learning outcomes and operational efficiency. Applications include learning analytics, student performance prediction, and adaptive learning systems. Clustering algorithms, such as K-means, have been used to segment students based on their learning behaviors, enabling targeted interventions.\nDespite these successes, the field of educational data mining faces challenges related to data privacy and ethical considerations. The use of student data for predictive analytics raises concerns about consent and the potential for bias in algorithmic decision-making.\nCybersecurity\nCybersecurity is another domain where data mining and ML have made significant contributions. These technologies have been used for intrusion detection, malware classification, and anomaly detection. Unsupervised learning techniques, such as autoencoders, have shown promise in identifying novel attack patterns that traditional rule-based systems might miss.\nHowever, the adversarial nature of cybersecurity presents unique challenges. Attackers can exploit vulnerabilities in ML models, leading to adversarial attacks that compromise system integrity. As a result, there is a growing need for robust and resilient ML systems in cybersecurity.\nFuture Outlook\nThe future of data mining and ML is shaped by several emerging trends and technologies. Explainable AI (XAI) is gaining traction as a means to address the interpretability challenges associated with deep learning models. XAI techniques aim to provide transparent and understandable explanations for model predictions, enhancing trust and adoption in critical applications.\nFederated learning is another promising area of research. This approach enables decentralized model training, allowing multiple parties to collaborate without sharing raw data. Federated learning has the potential to address data privacy concerns, particularly in domains such as healthcare and finance.\nQuantum machine learning is also on the horizon, leveraging the computational power of quantum computers to solve complex optimization problems. While still in its infancy, quantum machine learning holds the potential to revolutionize fields such as drug discovery and financial modeling.\nConclusion\nThis systematic literature review highlights the transformative impact of data mining and ML across various domains. While these technologies have enabled significant advancements, challenges related to interpretability, data quality, and scalability remain. Future research should focus on developing robust, scalable, and interpretable models to address these challenges. Additionally, ethical considerations, particularly in domains such as education and healthcare, must be carefully addressed to ensure the responsible use of data mining and ML.\nTestable Hypothesis\nOne easily testable hypothesis emerging from this review is:\n“Ensemble learning methods, such as XGBoost and Random Forests, will outperform traditional statistical methods in fraud detection tasks, achieving at least a 20% reduction in false positives.”\nThis hypothesis can be tested by comparing the performance of ensemble learning methods with traditional statistical methods on a standardized fraud detection dataset, such as the Credit Card Fraud Detection dataset available on Kaggle.\nReferences\nChandola, V., Banerjee, A., & Kumar, V. (2018). Anomaly detection: A survey. ACM Computing Surveys, 41(3), 1-58.\nDal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2015). Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications, 41(10), 4915-4928.\nEsteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115-118.\nYang, Q., Liu, Y., Chen, T., & Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 1-19.\n\n\n\nTask: Write a reflection answering:\n\nHow did each model approach the systematic review differently?\nWhich prompt refinements yielded the best results for each model?\nWhat did you learn about leveraging AI for structured academic reviews?\n\n\n\nThe response from each model was slightly different. It seems that each model has set different priorities on the types of responses included. Some of the responses are more structured and less written. Some the opposite.\nChatGPT: Perhaps a bit too structured when structure was requested. This might be great for outlining for a paper. The refinements required to remove the request for structure.\nCopilot seemed a bit dumbed-down. Although it included many words, it did not include much content. This required pushing the model for details.\nDeepSeek: This was my favorite model for this exercise. It had a good balance between structure and content. Model refinements were needed to get additional details. This model also seemed to produce valid sources.\nFor structured academic reviews, like most things, good, targeted questions lead to the best answers."
  },
  {
    "objectID": "assignment03.html#step-2-analyze-model-responses",
    "href": "assignment03.html#step-2-analyze-model-responses",
    "title": "Assignment 03",
    "section": "",
    "text": "Task: Assess each model’s output based on:\n\nStructure: Did it include a methodology section and follow a systematic review format?\nSynthesis: Were key findings from data mining and machine learning applications well-summarized?\nTrends and Gaps: Did it identify meaningful trends and research gaps?\nHypothesis: Was the proposed hypothesis testable and relevant?\nReferences: Are the citations accurate (check using Google Scholar or Semantic Scholar)\nStudents document strengths and weaknesses. For example, ChatGPT might provide broad coverage, Copilot might excel at concise structuring, and I (Grok 3) might offer a unique perspective on emerging trends.\n\n\n\n\n\nNo length requirement was give for the response, so each of the models must have used an internally defined length limit.\nEach of the models provided a general structure for their responses. ChatGPTs structure was weak and poorly implemented. Copilot was also weak in its structure. It seemed to be a bit more dumbed down than necessary. Meta AI was more professional and better structured. Gemini was produced in an outline form. It seemed more comprehensive than the average response. Llamas response was terse, and not really informative. Similar to Copilot. DeepSeek was also terse and written\nThe focus of this exercise is a literature review, but it seems that these are very poorly structured and written. They are also generally all similar.\nA quick note. The cut and paste responses above do not reflect the output of each of the models. In general, they looked better. DeepSeek output Markdown when copied, which was very good. This had to be changed for the information above because it interfered with the formatting of this quarto document. Gemini had no copy function, requiring output be sent to Google Docs.\n\n\n\nGiven the length of all the responses, they appeared trite. All of the responses, except Llama and Copilot were broken down by groups, such as healthcare. This was not helpful and meant that many areas were not covered.\n\n\n\nThe gap and trend analysis varied widely from model-to-model to the point that a more explicit request needs to be outlined for a new prompt to get the best results.\n\n\n\nAll of the hypotheses that were returned appeared testable, but all were rather specific. All required some domain specific knowledge, and many would have been very expensive to test.\n\n\n\nNot all the references were accurate or complete for any of the models, but the URL links were all live. Copilot only provided links, but no other sources.\n\n\n\n\nTask: Revise the prompt to address deficiencies in each model’s response, creating three tailored prompts—one for ChatGPT, one for Copilot, and one for Grok 3.\nExample Refined Prompt for Grok 3: “Imagine you’re a data scientist conducting a 2,000-word systematic literature review on how data mining and machine learning are applied in domains like healthcare, finance, and education. Outline a clear methodology, synthesize key findings with fresh insights, highlight trends and gaps, and propose one bold, testable hypothesis. Maintain a rigorous academic tone.”\nStudents test these refined prompts and compare the improved outputs.\n\n\nGetting to Grok was incovenient and DeepSeek has provided better responses than most models, so DeepSeek was substituted.\nBelow are the revised prompts for each of the models.\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper, less outlined, more rigorous\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper, less outlined, more rigorous\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\nIn general, the outputs were all improved. Even though the input prompts varied only a little from model-to-model the output was uniformily improved.\n\n\n\nTask: Integrate the best elements from each model’s output into a final systematic review. Write a new prompt for the student’s preferred model (e.g., Grok 3) to synthesize the results.\nExample Synthesis Prompt: “Using these drafts from three AI models [paste outputs], produce a 2,000-word structured systematic literature review on data mining and machine learning applications. Combine the strongest methodology, findings, trends, gaps, and hypothesis into a cohesive, academically sound document.”\nStudents submit their final review and justify their synthesis decisions.\n\n\nI took the output from DeepSeek, ChatGPT and Copilot to create a 2,000 word written report from ChatGPT with the following prompt. ChatGPT was chosen because I have a paid account, and it does not give me busy errors. The actual text input was not included for brevity. The prompt is short but useful.\n\nUsing the included information create a 2000 word written report.\n\nThis is the final output from the prompt above. I have found with the latest models from ChatGPT, simple guidance such as above have produced acceptable outputs. Counterintuitively, more detail has produced stranger results. One thought is that the effort the model puts into following specific request works against the output process.\n\nApplications of Data Mining and Machine Learning: A Systematic Literature Review\nAbstract\nThis systematic literature review explores the applications of data mining and machine learning across multiple domains, including healthcare, finance, education, and cybersecurity. By analyzing current trends and discussing future prospects, this review provides a comprehensive understanding of how these technologies are utilized and where opportunities for further research lie. Additionally, a testable hypothesis is proposed to guide future investigations in this rapidly evolving field.\nIntroduction\nData mining and machine learning (ML) have become indispensable tools in the modern era of big data, driving innovations across diverse fields. These technologies enable the extraction of meaningful patterns from large datasets, facilitating decision-making processes and predictive analytics. As the volume of data generated globally continues to grow exponentially, the applications of data mining and ML have expanded in scope and complexity. This paper presents a systematic literature review of the applications of data mining and ML, focusing on their methodologies, emerging trends, and future directions. The review also identifies gaps in the current research and proposes a testable hypothesis to guide future studies.\nMethodology\nThis review adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure methodological rigor and transparency. The process involved the following steps:\nData Collection\nA comprehensive search was conducted across several academic databases, including IEEE Xplore, PubMed, SpringerLink, ACM Digital Library, and Google Scholar. The search terms included “data mining applications,” “machine learning applications,” “trends in data mining,” “machine learning in healthcare,” and “data mining in finance.” The inclusion criteria were limited to peer-reviewed articles published between 2015 and 2023 that focused on real-world applications of data mining and ML. Articles without empirical evidence, non-English publications, and purely theoretical papers were excluded.\nData Analysis\nThe selected articles were analyzed thematically to identify recurring applications, methodologies, and trends. Quantitative data were extracted and synthesized to provide a comprehensive overview of the field. Gap analysis was performed by comparing the stated objectives of the studies with their outcomes, highlighting areas where further research is needed.\nTrends in Data Mining and Machine Learning Applications\nHealthcare\nHealthcare has been one of the most prominent domains for the application of data mining and ML. These technologies have been used for predictive diagnostics, personalized medicine, and patient outcome prediction. For instance, deep learning models, particularly convolutional neural networks (CNNs), have demonstrated diagnostic accuracy comparable to human experts in radiology. Similarly, natural language processing (NLP) techniques have been employed to analyze electronic health records (EHRs), enabling the extraction of valuable insights from unstructured text data.\nDespite these advancements, challenges remain. The interpretability of ML models, particularly deep learning, is a significant barrier to their widespread adoption in clinical settings. The “black-box” nature of deep learning models limits their utility in critical applications where transparency is essential.\nFinance\nIn the financial sector, data mining and ML have been widely adopted for fraud detection, algorithmic trading, and credit scoring. Ensemble learning methods, such as XGBoost and Random Forests, have shown superior performance in detecting fraudulent transactions compared to traditional statistical methods. These methods have reduced false positives significantly, improving the efficiency of fraud detection systems.\nHowever, the financial domain also faces challenges related to data quality and scalability. Noisy and incomplete datasets can hinder the performance of ML models, while the increasing volume of financial data necessitates the development of scalable algorithms.\nEducation\nEducational institutions have increasingly turned to data mining and ML to enhance learning outcomes and operational efficiency. Applications include learning analytics, student performance prediction, and adaptive learning systems. Clustering algorithms, such as K-means, have been used to segment students based on their learning behaviors, enabling targeted interventions.\nDespite these successes, the field of educational data mining faces challenges related to data privacy and ethical considerations. The use of student data for predictive analytics raises concerns about consent and the potential for bias in algorithmic decision-making.\nCybersecurity\nCybersecurity is another domain where data mining and ML have made significant contributions. These technologies have been used for intrusion detection, malware classification, and anomaly detection. Unsupervised learning techniques, such as autoencoders, have shown promise in identifying novel attack patterns that traditional rule-based systems might miss.\nHowever, the adversarial nature of cybersecurity presents unique challenges. Attackers can exploit vulnerabilities in ML models, leading to adversarial attacks that compromise system integrity. As a result, there is a growing need for robust and resilient ML systems in cybersecurity.\nFuture Outlook\nThe future of data mining and ML is shaped by several emerging trends and technologies. Explainable AI (XAI) is gaining traction as a means to address the interpretability challenges associated with deep learning models. XAI techniques aim to provide transparent and understandable explanations for model predictions, enhancing trust and adoption in critical applications.\nFederated learning is another promising area of research. This approach enables decentralized model training, allowing multiple parties to collaborate without sharing raw data. Federated learning has the potential to address data privacy concerns, particularly in domains such as healthcare and finance.\nQuantum machine learning is also on the horizon, leveraging the computational power of quantum computers to solve complex optimization problems. While still in its infancy, quantum machine learning holds the potential to revolutionize fields such as drug discovery and financial modeling.\nConclusion\nThis systematic literature review highlights the transformative impact of data mining and ML across various domains. While these technologies have enabled significant advancements, challenges related to interpretability, data quality, and scalability remain. Future research should focus on developing robust, scalable, and interpretable models to address these challenges. Additionally, ethical considerations, particularly in domains such as education and healthcare, must be carefully addressed to ensure the responsible use of data mining and ML.\nTestable Hypothesis\nOne easily testable hypothesis emerging from this review is:\n“Ensemble learning methods, such as XGBoost and Random Forests, will outperform traditional statistical methods in fraud detection tasks, achieving at least a 20% reduction in false positives.”\nThis hypothesis can be tested by comparing the performance of ensemble learning methods with traditional statistical methods on a standardized fraud detection dataset, such as the Credit Card Fraud Detection dataset available on Kaggle.\nReferences\nChandola, V., Banerjee, A., & Kumar, V. (2018). Anomaly detection: A survey. ACM Computing Surveys, 41(3), 1-58.\nDal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2015). Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications, 41(10), 4915-4928.\nEsteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115-118.\nYang, Q., Liu, Y., Chen, T., & Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 1-19.\n\n\n\nTask: Write a reflection answering:\n\nHow did each model approach the systematic review differently?\nWhich prompt refinements yielded the best results for each model?\nWhat did you learn about leveraging AI for structured academic reviews?\n\n\n\nThe response from each model was slightly different. It seems that each model has set different priorities on the types of responses included. Some of the responses are more structured and less written. Some the opposite.\nChatGPT: Perhaps a bit too structured when structure was requested. This might be great for outlining for a paper. The refinements required to remove the request for structure.\nCopilot seemed a bit dumbed-down. Although it included many words, it did not include much content. This required pushing the model for details.\nDeepSeek: This was my favorite model for this exercise. It had a good balance between structure and content. Model refinements were needed to get additional details. This model also seemed to produce valid sources.\nFor structured academic reviews, like most things, good, targeted questions lead to the best answers."
  },
  {
    "objectID": "assignment03.html#step-3-refine-the-prompt",
    "href": "assignment03.html#step-3-refine-the-prompt",
    "title": "Assignment 03",
    "section": "",
    "text": "Task: Revise the prompt to address deficiencies in each model’s response, creating three tailored prompts—one for ChatGPT, one for Copilot, and one for Grok 3.\nExample Refined Prompt for Grok 3: “Imagine you’re a data scientist conducting a 2,000-word systematic literature review on how data mining and machine learning are applied in domains like healthcare, finance, and education. Outline a clear methodology, synthesize key findings with fresh insights, highlight trends and gaps, and propose one bold, testable hypothesis. Maintain a rigorous academic tone.”\nStudents test these refined prompts and compare the improved outputs.\n\n\nGetting to Grok was incovenient and DeepSeek has provided better responses than most models, so DeepSeek was substituted.\nBelow are the revised prompts for each of the models.\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper, less outlined, more rigorous\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\n\n\nCreate a 2000 word systematic literature review on the applications of data mining and machine learning. make it like an academic paper, less outlined, more rigorous\nInclude: Methodology, Trends, Future Outlook, Conclusion and One easily Testable Hypothesis\nRequirements: All statements must be real and true, verified with google scholar A comprehensive list of sources, APSA format and academic tone\n\nIn general, the outputs were all improved. Even though the input prompts varied only a little from model-to-model the output was uniformily improved.\n\n\n\nTask: Integrate the best elements from each model’s output into a final systematic review. Write a new prompt for the student’s preferred model (e.g., Grok 3) to synthesize the results.\nExample Synthesis Prompt: “Using these drafts from three AI models [paste outputs], produce a 2,000-word structured systematic literature review on data mining and machine learning applications. Combine the strongest methodology, findings, trends, gaps, and hypothesis into a cohesive, academically sound document.”\nStudents submit their final review and justify their synthesis decisions.\n\n\nI took the output from DeepSeek, ChatGPT and Copilot to create a 2,000 word written report from ChatGPT with the following prompt. ChatGPT was chosen because I have a paid account, and it does not give me busy errors. The actual text input was not included for brevity. The prompt is short but useful.\n\nUsing the included information create a 2000 word written report.\n\nThis is the final output from the prompt above. I have found with the latest models from ChatGPT, simple guidance such as above have produced acceptable outputs. Counterintuitively, more detail has produced stranger results. One thought is that the effort the model puts into following specific request works against the output process.\n\nApplications of Data Mining and Machine Learning: A Systematic Literature Review\nAbstract\nThis systematic literature review explores the applications of data mining and machine learning across multiple domains, including healthcare, finance, education, and cybersecurity. By analyzing current trends and discussing future prospects, this review provides a comprehensive understanding of how these technologies are utilized and where opportunities for further research lie. Additionally, a testable hypothesis is proposed to guide future investigations in this rapidly evolving field.\nIntroduction\nData mining and machine learning (ML) have become indispensable tools in the modern era of big data, driving innovations across diverse fields. These technologies enable the extraction of meaningful patterns from large datasets, facilitating decision-making processes and predictive analytics. As the volume of data generated globally continues to grow exponentially, the applications of data mining and ML have expanded in scope and complexity. This paper presents a systematic literature review of the applications of data mining and ML, focusing on their methodologies, emerging trends, and future directions. The review also identifies gaps in the current research and proposes a testable hypothesis to guide future studies.\nMethodology\nThis review adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure methodological rigor and transparency. The process involved the following steps:\nData Collection\nA comprehensive search was conducted across several academic databases, including IEEE Xplore, PubMed, SpringerLink, ACM Digital Library, and Google Scholar. The search terms included “data mining applications,” “machine learning applications,” “trends in data mining,” “machine learning in healthcare,” and “data mining in finance.” The inclusion criteria were limited to peer-reviewed articles published between 2015 and 2023 that focused on real-world applications of data mining and ML. Articles without empirical evidence, non-English publications, and purely theoretical papers were excluded.\nData Analysis\nThe selected articles were analyzed thematically to identify recurring applications, methodologies, and trends. Quantitative data were extracted and synthesized to provide a comprehensive overview of the field. Gap analysis was performed by comparing the stated objectives of the studies with their outcomes, highlighting areas where further research is needed.\nTrends in Data Mining and Machine Learning Applications\nHealthcare\nHealthcare has been one of the most prominent domains for the application of data mining and ML. These technologies have been used for predictive diagnostics, personalized medicine, and patient outcome prediction. For instance, deep learning models, particularly convolutional neural networks (CNNs), have demonstrated diagnostic accuracy comparable to human experts in radiology. Similarly, natural language processing (NLP) techniques have been employed to analyze electronic health records (EHRs), enabling the extraction of valuable insights from unstructured text data.\nDespite these advancements, challenges remain. The interpretability of ML models, particularly deep learning, is a significant barrier to their widespread adoption in clinical settings. The “black-box” nature of deep learning models limits their utility in critical applications where transparency is essential.\nFinance\nIn the financial sector, data mining and ML have been widely adopted for fraud detection, algorithmic trading, and credit scoring. Ensemble learning methods, such as XGBoost and Random Forests, have shown superior performance in detecting fraudulent transactions compared to traditional statistical methods. These methods have reduced false positives significantly, improving the efficiency of fraud detection systems.\nHowever, the financial domain also faces challenges related to data quality and scalability. Noisy and incomplete datasets can hinder the performance of ML models, while the increasing volume of financial data necessitates the development of scalable algorithms.\nEducation\nEducational institutions have increasingly turned to data mining and ML to enhance learning outcomes and operational efficiency. Applications include learning analytics, student performance prediction, and adaptive learning systems. Clustering algorithms, such as K-means, have been used to segment students based on their learning behaviors, enabling targeted interventions.\nDespite these successes, the field of educational data mining faces challenges related to data privacy and ethical considerations. The use of student data for predictive analytics raises concerns about consent and the potential for bias in algorithmic decision-making.\nCybersecurity\nCybersecurity is another domain where data mining and ML have made significant contributions. These technologies have been used for intrusion detection, malware classification, and anomaly detection. Unsupervised learning techniques, such as autoencoders, have shown promise in identifying novel attack patterns that traditional rule-based systems might miss.\nHowever, the adversarial nature of cybersecurity presents unique challenges. Attackers can exploit vulnerabilities in ML models, leading to adversarial attacks that compromise system integrity. As a result, there is a growing need for robust and resilient ML systems in cybersecurity.\nFuture Outlook\nThe future of data mining and ML is shaped by several emerging trends and technologies. Explainable AI (XAI) is gaining traction as a means to address the interpretability challenges associated with deep learning models. XAI techniques aim to provide transparent and understandable explanations for model predictions, enhancing trust and adoption in critical applications.\nFederated learning is another promising area of research. This approach enables decentralized model training, allowing multiple parties to collaborate without sharing raw data. Federated learning has the potential to address data privacy concerns, particularly in domains such as healthcare and finance.\nQuantum machine learning is also on the horizon, leveraging the computational power of quantum computers to solve complex optimization problems. While still in its infancy, quantum machine learning holds the potential to revolutionize fields such as drug discovery and financial modeling.\nConclusion\nThis systematic literature review highlights the transformative impact of data mining and ML across various domains. While these technologies have enabled significant advancements, challenges related to interpretability, data quality, and scalability remain. Future research should focus on developing robust, scalable, and interpretable models to address these challenges. Additionally, ethical considerations, particularly in domains such as education and healthcare, must be carefully addressed to ensure the responsible use of data mining and ML.\nTestable Hypothesis\nOne easily testable hypothesis emerging from this review is:\n“Ensemble learning methods, such as XGBoost and Random Forests, will outperform traditional statistical methods in fraud detection tasks, achieving at least a 20% reduction in false positives.”\nThis hypothesis can be tested by comparing the performance of ensemble learning methods with traditional statistical methods on a standardized fraud detection dataset, such as the Credit Card Fraud Detection dataset available on Kaggle.\nReferences\nChandola, V., Banerjee, A., & Kumar, V. (2018). Anomaly detection: A survey. ACM Computing Surveys, 41(3), 1-58.\nDal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2015). Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications, 41(10), 4915-4928.\nEsteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115-118.\nYang, Q., Liu, Y., Chen, T., & Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 1-19.\n\n\n\nTask: Write a reflection answering:\n\nHow did each model approach the systematic review differently?\nWhich prompt refinements yielded the best results for each model?\nWhat did you learn about leveraging AI for structured academic reviews?\n\n\n\nThe response from each model was slightly different. It seems that each model has set different priorities on the types of responses included. Some of the responses are more structured and less written. Some the opposite.\nChatGPT: Perhaps a bit too structured when structure was requested. This might be great for outlining for a paper. The refinements required to remove the request for structure.\nCopilot seemed a bit dumbed-down. Although it included many words, it did not include much content. This required pushing the model for details.\nDeepSeek: This was my favorite model for this exercise. It had a good balance between structure and content. Model refinements were needed to get additional details. This model also seemed to produce valid sources.\nFor structured academic reviews, like most things, good, targeted questions lead to the best answers."
  },
  {
    "objectID": "assignment03.html#step-4-cross-model-collaboration",
    "href": "assignment03.html#step-4-cross-model-collaboration",
    "title": "Assignment 03",
    "section": "",
    "text": "Task: Integrate the best elements from each model’s output into a final systematic review. Write a new prompt for the student’s preferred model (e.g., Grok 3) to synthesize the results.\nExample Synthesis Prompt: “Using these drafts from three AI models [paste outputs], produce a 2,000-word structured systematic literature review on data mining and machine learning applications. Combine the strongest methodology, findings, trends, gaps, and hypothesis into a cohesive, academically sound document.”\nStudents submit their final review and justify their synthesis decisions.\n\n\nI took the output from DeepSeek, ChatGPT and Copilot to create a 2,000 word written report from ChatGPT with the following prompt. ChatGPT was chosen because I have a paid account, and it does not give me busy errors. The actual text input was not included for brevity. The prompt is short but useful.\n\nUsing the included information create a 2000 word written report.\n\nThis is the final output from the prompt above. I have found with the latest models from ChatGPT, simple guidance such as above have produced acceptable outputs. Counterintuitively, more detail has produced stranger results. One thought is that the effort the model puts into following specific request works against the output process.\n\nApplications of Data Mining and Machine Learning: A Systematic Literature Review\nAbstract\nThis systematic literature review explores the applications of data mining and machine learning across multiple domains, including healthcare, finance, education, and cybersecurity. By analyzing current trends and discussing future prospects, this review provides a comprehensive understanding of how these technologies are utilized and where opportunities for further research lie. Additionally, a testable hypothesis is proposed to guide future investigations in this rapidly evolving field.\nIntroduction\nData mining and machine learning (ML) have become indispensable tools in the modern era of big data, driving innovations across diverse fields. These technologies enable the extraction of meaningful patterns from large datasets, facilitating decision-making processes and predictive analytics. As the volume of data generated globally continues to grow exponentially, the applications of data mining and ML have expanded in scope and complexity. This paper presents a systematic literature review of the applications of data mining and ML, focusing on their methodologies, emerging trends, and future directions. The review also identifies gaps in the current research and proposes a testable hypothesis to guide future studies.\nMethodology\nThis review adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure methodological rigor and transparency. The process involved the following steps:\nData Collection\nA comprehensive search was conducted across several academic databases, including IEEE Xplore, PubMed, SpringerLink, ACM Digital Library, and Google Scholar. The search terms included “data mining applications,” “machine learning applications,” “trends in data mining,” “machine learning in healthcare,” and “data mining in finance.” The inclusion criteria were limited to peer-reviewed articles published between 2015 and 2023 that focused on real-world applications of data mining and ML. Articles without empirical evidence, non-English publications, and purely theoretical papers were excluded.\nData Analysis\nThe selected articles were analyzed thematically to identify recurring applications, methodologies, and trends. Quantitative data were extracted and synthesized to provide a comprehensive overview of the field. Gap analysis was performed by comparing the stated objectives of the studies with their outcomes, highlighting areas where further research is needed.\nTrends in Data Mining and Machine Learning Applications\nHealthcare\nHealthcare has been one of the most prominent domains for the application of data mining and ML. These technologies have been used for predictive diagnostics, personalized medicine, and patient outcome prediction. For instance, deep learning models, particularly convolutional neural networks (CNNs), have demonstrated diagnostic accuracy comparable to human experts in radiology. Similarly, natural language processing (NLP) techniques have been employed to analyze electronic health records (EHRs), enabling the extraction of valuable insights from unstructured text data.\nDespite these advancements, challenges remain. The interpretability of ML models, particularly deep learning, is a significant barrier to their widespread adoption in clinical settings. The “black-box” nature of deep learning models limits their utility in critical applications where transparency is essential.\nFinance\nIn the financial sector, data mining and ML have been widely adopted for fraud detection, algorithmic trading, and credit scoring. Ensemble learning methods, such as XGBoost and Random Forests, have shown superior performance in detecting fraudulent transactions compared to traditional statistical methods. These methods have reduced false positives significantly, improving the efficiency of fraud detection systems.\nHowever, the financial domain also faces challenges related to data quality and scalability. Noisy and incomplete datasets can hinder the performance of ML models, while the increasing volume of financial data necessitates the development of scalable algorithms.\nEducation\nEducational institutions have increasingly turned to data mining and ML to enhance learning outcomes and operational efficiency. Applications include learning analytics, student performance prediction, and adaptive learning systems. Clustering algorithms, such as K-means, have been used to segment students based on their learning behaviors, enabling targeted interventions.\nDespite these successes, the field of educational data mining faces challenges related to data privacy and ethical considerations. The use of student data for predictive analytics raises concerns about consent and the potential for bias in algorithmic decision-making.\nCybersecurity\nCybersecurity is another domain where data mining and ML have made significant contributions. These technologies have been used for intrusion detection, malware classification, and anomaly detection. Unsupervised learning techniques, such as autoencoders, have shown promise in identifying novel attack patterns that traditional rule-based systems might miss.\nHowever, the adversarial nature of cybersecurity presents unique challenges. Attackers can exploit vulnerabilities in ML models, leading to adversarial attacks that compromise system integrity. As a result, there is a growing need for robust and resilient ML systems in cybersecurity.\nFuture Outlook\nThe future of data mining and ML is shaped by several emerging trends and technologies. Explainable AI (XAI) is gaining traction as a means to address the interpretability challenges associated with deep learning models. XAI techniques aim to provide transparent and understandable explanations for model predictions, enhancing trust and adoption in critical applications.\nFederated learning is another promising area of research. This approach enables decentralized model training, allowing multiple parties to collaborate without sharing raw data. Federated learning has the potential to address data privacy concerns, particularly in domains such as healthcare and finance.\nQuantum machine learning is also on the horizon, leveraging the computational power of quantum computers to solve complex optimization problems. While still in its infancy, quantum machine learning holds the potential to revolutionize fields such as drug discovery and financial modeling.\nConclusion\nThis systematic literature review highlights the transformative impact of data mining and ML across various domains. While these technologies have enabled significant advancements, challenges related to interpretability, data quality, and scalability remain. Future research should focus on developing robust, scalable, and interpretable models to address these challenges. Additionally, ethical considerations, particularly in domains such as education and healthcare, must be carefully addressed to ensure the responsible use of data mining and ML.\nTestable Hypothesis\nOne easily testable hypothesis emerging from this review is:\n“Ensemble learning methods, such as XGBoost and Random Forests, will outperform traditional statistical methods in fraud detection tasks, achieving at least a 20% reduction in false positives.”\nThis hypothesis can be tested by comparing the performance of ensemble learning methods with traditional statistical methods on a standardized fraud detection dataset, such as the Credit Card Fraud Detection dataset available on Kaggle.\nReferences\nChandola, V., Banerjee, A., & Kumar, V. (2018). Anomaly detection: A survey. ACM Computing Surveys, 41(3), 1-58.\nDal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2015). Learned lessons in credit card fraud detection from a practitioner perspective. Expert Systems with Applications, 41(10), 4915-4928.\nEsteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., & Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639), 115-118.\nYang, Q., Liu, Y., Chen, T., & Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 1-19.\n\n\n\nTask: Write a reflection answering:\n\nHow did each model approach the systematic review differently?\nWhich prompt refinements yielded the best results for each model?\nWhat did you learn about leveraging AI for structured academic reviews?\n\n\n\nThe response from each model was slightly different. It seems that each model has set different priorities on the types of responses included. Some of the responses are more structured and less written. Some the opposite.\nChatGPT: Perhaps a bit too structured when structure was requested. This might be great for outlining for a paper. The refinements required to remove the request for structure.\nCopilot seemed a bit dumbed-down. Although it included many words, it did not include much content. This required pushing the model for details.\nDeepSeek: This was my favorite model for this exercise. It had a good balance between structure and content. Model refinements were needed to get additional details. This model also seemed to produce valid sources.\nFor structured academic reviews, like most things, good, targeted questions lead to the best answers."
  },
  {
    "objectID": "assignment03.html#step-5-reflection",
    "href": "assignment03.html#step-5-reflection",
    "title": "Assignment 03",
    "section": "",
    "text": "Task: Write a reflection answering:\n\nHow did each model approach the systematic review differently?\nWhich prompt refinements yielded the best results for each model?\nWhat did you learn about leveraging AI for structured academic reviews?\n\n\n\nThe response from each model was slightly different. It seems that each model has set different priorities on the types of responses included. Some of the responses are more structured and less written. Some the opposite.\nChatGPT: Perhaps a bit too structured when structure was requested. This might be great for outlining for a paper. The refinements required to remove the request for structure.\nCopilot seemed a bit dumbed-down. Although it included many words, it did not include much content. This required pushing the model for details.\nDeepSeek: This was my favorite model for this exercise. It had a good balance between structure and content. Model refinements were needed to get additional details. This model also seemed to produce valid sources.\nFor structured academic reviews, like most things, good, targeted questions lead to the best answers."
  },
  {
    "objectID": "EPPS6323.html",
    "href": "EPPS6323.html",
    "title": "EPPS6323 Files",
    "section": "",
    "text": "library(ggplot2)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]"
  },
  {
    "objectID": "EPPS6323.html#assignment-1-more-penguins",
    "href": "EPPS6323.html#assignment-1-more-penguins",
    "title": "EPPS6323 Files",
    "section": "",
    "text": "library(ggplot2)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]"
  },
  {
    "objectID": "Prepare09.html",
    "href": "Prepare09.html",
    "title": "Prepare for Class09",
    "section": "",
    "text": "#Prepare for Class 09\n\n\n(https://www.anaconda.com/download)\n\nCompleted\n\n\n\n\nRefer to menu\n\nCompleted\n\n\n\n\n\n\nTo build a predictive model, I would follow these general steps:\n\nCollect Data\nPreprocess Data\nSelect Features & Model Inputs\nChoose a Model\nTrain the Model\nRefine the Model\nGenerate Final Results\n\nEach of these steps would require additional work to accomplish, of course.\n\n\n\nThere are several ways to gather the data for a project like this including:\n\nStatistics from sources like KenPom, Sports-Reference, NCAA, etc.\nPast Results NCAA, newspapers\nBetting market data from various betting sites\nOther metric variables from sources like Barttorvik.com\nInjury Reports & Team News from newspapers, Reddit, etc.\nTravel factorsctors (travel distance, neutral-site performance)\n\n\n\n\nThere are several options for a dependent variable including:\n\nWin/Loss\n\nLogistic Regression\nClassification with Neural Networks\n\nPoint Spread\n\nLinear Regression\nRandom Forest Regression\n\nHighest Round Reached\n\nRegression\n\nUpset Likelihood\n\nCox Proportional Hazards Model\nTree-based Classifiers\n\nChampionship Probability\n\nCox Proportional Hazards Model\nTree-based Classifiers\n\n\n\n\n\n\nPerformance\n\n\nAdjusted Offensive Efficiency – Points scored per 100 possessions (accounts for opponent strength).\nAdjusted Defensive Efficiency (AdjDE) – Points allowed per 100 possessions.\nNet Efficiency – Offensive Efficiency minus Defensive Efficiency\nEffective Field Goal % – Adjusts for 3-pointers being more valuable.\nTurnover % (TOV%) – Can’t score if you don’t have the ball.\n\n\nTournament Factors\n\n\nSeed – 85% of champions since 1985 were a 1 or 2 seed.\nPercentage of minutes played by upperclassmen.\nCoach’s NCAA Tournament win percentage.\nRecent Performance in Last 10 Games.\nFree Throw % in Close Games.\n\n\nPlay Style\n\n\n3-Point Shooting & Defense.\nRebounding Margins – Offensive & Defensive\nAssist-to-Turnover Ratio – Share the ball and limit mistakes.\n\n\nIntangibles\n\n\nStrength of Schedule (SOS) – Teams tested against top competition fare better.\nNeutral-Site Performance – Championship games are on neutral courts.\nInjury Status – Missing key players are a problem.\n\n\nBetting Market & Consensus\n\n\n‘Vegas’ Odds – Sportsbooks can be efficient sentiment aggregators.\nPublic Betting Trends – Reddit as a predictor variable?"
  },
  {
    "objectID": "Prepare09.html#install-anaconda",
    "href": "Prepare09.html#install-anaconda",
    "title": "Prepare for Class09",
    "section": "",
    "text": "(https://www.anaconda.com/download)\n\nCompleted"
  },
  {
    "objectID": "Prepare09.html#assignment-4",
    "href": "Prepare09.html#assignment-4",
    "title": "Prepare for Class09",
    "section": "",
    "text": "Refer to menu\n\nCompleted"
  },
  {
    "objectID": "Prepare09.html#march-madness-ncaa-mens-basketball-di",
    "href": "Prepare09.html#march-madness-ncaa-mens-basketball-di",
    "title": "Prepare for Class09",
    "section": "",
    "text": "To build a predictive model, I would follow these general steps:\n\nCollect Data\nPreprocess Data\nSelect Features & Model Inputs\nChoose a Model\nTrain the Model\nRefine the Model\nGenerate Final Results\n\nEach of these steps would require additional work to accomplish, of course.\n\n\n\nThere are several ways to gather the data for a project like this including:\n\nStatistics from sources like KenPom, Sports-Reference, NCAA, etc.\nPast Results NCAA, newspapers\nBetting market data from various betting sites\nOther metric variables from sources like Barttorvik.com\nInjury Reports & Team News from newspapers, Reddit, etc.\nTravel factorsctors (travel distance, neutral-site performance)\n\n\n\n\nThere are several options for a dependent variable including:\n\nWin/Loss\n\nLogistic Regression\nClassification with Neural Networks\n\nPoint Spread\n\nLinear Regression\nRandom Forest Regression\n\nHighest Round Reached\n\nRegression\n\nUpset Likelihood\n\nCox Proportional Hazards Model\nTree-based Classifiers\n\nChampionship Probability\n\nCox Proportional Hazards Model\nTree-based Classifiers\n\n\n\n\n\n\nPerformance\n\n\nAdjusted Offensive Efficiency – Points scored per 100 possessions (accounts for opponent strength).\nAdjusted Defensive Efficiency (AdjDE) – Points allowed per 100 possessions.\nNet Efficiency – Offensive Efficiency minus Defensive Efficiency\nEffective Field Goal % – Adjusts for 3-pointers being more valuable.\nTurnover % (TOV%) – Can’t score if you don’t have the ball.\n\n\nTournament Factors\n\n\nSeed – 85% of champions since 1985 were a 1 or 2 seed.\nPercentage of minutes played by upperclassmen.\nCoach’s NCAA Tournament win percentage.\nRecent Performance in Last 10 Games.\nFree Throw % in Close Games.\n\n\nPlay Style\n\n\n3-Point Shooting & Defense.\nRebounding Margins – Offensive & Defensive\nAssist-to-Turnover Ratio – Share the ball and limit mistakes.\n\n\nIntangibles\n\n\nStrength of Schedule (SOS) – Teams tested against top competition fare better.\nNeutral-Site Performance – Championship games are on neutral courts.\nInjury Status – Missing key players are a problem.\n\n\nBetting Market & Consensus\n\n\n‘Vegas’ Odds – Sportsbooks can be efficient sentiment aggregators.\nPublic Betting Trends – Reddit as a predictor variable?"
  },
  {
    "objectID": "assignment04.html",
    "href": "assignment04.html",
    "title": "Assignment 04",
    "section": "",
    "text": "As a note, some of the content below was aided by the use of AI.\n\n\nof quanteda at https://tutorials.quanteda.io/\n\nRead\n\n\n\n\nquanteda_textanalytics01.R from Teams\nIt is copied and run below.\n\n# Sample program for using quanteda for text modeling and analysis\n# Use vignette(\"auth\", package = \"rtweet\") for authentication\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\n#install.packages(c(\"quanteda\",\"quanteda.textmodels\",\"quanteda.textstats\",\"quanteda.textplots\"))\n\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           159930 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\n\n# Network plot: tags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n# Network plot: Users\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 701 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  dfm() %&gt;% \n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Biden\",\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  # Increase min_termfreq from 5\n  dfm_trim(min_termfreq = 10, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\n\n\n\n\n# Locate keywords-in-context\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"trade\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"american\"),\n  kwic(tokens_inaugural, pattern = \"people\"),\n  kwic(tokens_inaugural, pattern = \"trade\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAll of the analyses are run above. Please refer to that for these responses.\n\n\n\nFor the top tags in the document feature matrix, the primary topics other than the participants and their countries were corona virus, and fentanyl.\nLooking at the two feature concurrence matrices,\n\nFCM1\n\n#breaking and #breakingnews co-occur with #biden, #china, and #usa moderately (values between 4 and 5).\n#biden and #china co-occur heavily, particularly #biden + #china = 443 times, and #china + #usa = 76 times, suggesting a strong relationship.\n#pray4america doesn’t seem to co-occur with any other terms.\n#biden and #china seem to be central, with #usa being smaller.\n\nFCM2\n\nThis matrix deals with more specific terms: #joebiden, #xijinping, #america, #americans.\n#biden and #china both strongly co-occur with #joebiden, #xijinping, #america, and #americans. Notably:\n\n#china + #xijinping = 434 times\n#biden + #xijinping = 370 times\n\n#usa and #pray4america have very weak connections in this matrix (almost all zeros or low numbers).\n\n\nLooking at the network plot of the feature concurrence matrix, #fentanyl, #biden / #joebiden, #american(s), #xijinping and # coronavirus are strongly associated.\nFor the FCM of the user DFM, there were few associations between people.\nIn the text plot, there were a couple of echo-chambers worth noticing, but they likely had little or nothing to do with Biden/ Xi.\n\n\n\n\nIn the inaugural speeches, the major features of the speeches centered around government, country, people and similar topics.\nBreaking this down by president, Trump and Obama had more to say about america(n) than Bush and Biden, who more touted freedom and democracy.\n\n\nLooking at the combined lexical dispersion plot for all the presidents, most, except Kennedy peppered their speeches with american and people, but few used trade. It does not appear that there are obvious patterns without further statistical analysis.\nExcept for Bush, presidents mentioned people. There may have been an increase in the use of american with Trump and Biden.\n\n\n\n\n\nFrom https://tutorials.quanted.io\n“Wordfish is a Poisson scaling model of one-dimensional document positions (Slapin and Proksch 2008). Wordfish also allows for scaling documents, but in comparison to Wordscores, reference scores/texts are not required. Wordfish is an unsupervised one-dimensional text scaling method, meaning that it estimates the positions of documents solely based on the observed word frequencies.”\nWordfish is an unsupervised scaling method.\n\n\n\n\nStart with an DFM.\nUse Wordfish to model word counts based on:\n\nα (document fixed effect) – controls for length of the document.\n\nβ (word fixed effect) – controls for how common a word.\nθ (document position) – the position of the document on the latent scale (this is what you’re looking for).\nψ (word discrimination parameter) – shows how a word is associated with different positions.\n\n\n\nSo, mathematically: \\[E_{(y_{ij})} = exp(\\alpha_i + \\beta_j + \\psi_j * \\theta_i)\\]\n\nOutput:\n\nDocument positions (θ): each document gets a score placing it on the latent scale.\n\nWord weights (ψ): tell you whether a word is more common in in one type or class of texts.\n\n\n\nSome analysis using Wordfish on Quanteda\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# U.S. inaugural speeches (post-1900)\ndata(\"data_corpus_inaugural\")\ncorpus &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1900)\n\n# Tokenize the corpus, remove punctuation and stopwords\ntoks &lt;- tokens(corpus, remove_punct = TRUE)\ntoks &lt;- tokens_remove(toks, stopwords(\"en\"))\n\n# Create document-feature matrix and trim low-frequency terms\ndfm &lt;- dfm(toks)\ndfm &lt;- dfm_trim(dfm, min_termfreq = 5)\n\n# Apply the Wordfish model using quanteda.textmodels with a directional anchor\nwf_model &lt;- textmodel_wordfish(dfm, dir = c(1, 2))\n\n# Extract word-level beta estimates (using margin = \"features\") and convert to numeric\nword_pos &lt;- data.frame(\n  word = featnames(dfm),\n  beta = as.numeric(coef(wf_model, margin = \"features\"))\n)\n\n# For a clearer plot, select the top 30 words with the most extreme beta values\nword_pos &lt;- word_pos %&gt;%\n  arrange(desc(abs(beta))) %&gt;%\n  head(30) %&gt;%\n  arrange(beta)\n\n# Base R plot of word positions\nplot(word_pos$beta, 1:nrow(word_pos),\n     pch = 16,\n     xlab = \"Word Position (Wordfish Estimate)\",\n     ylab = \"\",\n     main = \"Wordfish Scaling of Word Positions\\n(U.S. Inaugural Speeches)\",\n     yaxt = \"n\")  # suppress default y-axis labels\n\n# Add custom y-axis with word labels\naxis(2, at = 1:nrow(word_pos), labels = word_pos$word, las = 1, cex.axis = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nusing government documents selected from the govinfo.gov website (use sample program govtdata01.R)\nThis creates a smaller corpus because it will be run every time quarto is rendered.\n\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nlibrary(readr)\n\n## CSV method\ngovfiles1= read_csv(file=\"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/EPPS6323/Labs/govinfo.csv\")\n\nRows: 3 Columns: 14\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): collection, packageId, granuleId, title, detailsLink, pdfLink, htm...\ndbl  (1): index\nlgl  (4): xmlLink, otherLink1, otherLink2, historical\ndate (1): publishdate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#summary(govfiles1)\nprint(govfiles1)\n\n# A tibble: 3 × 14\n  index collection       packageId  granuleId title detailsLink pdfLink htmlLink\n  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;   \n1     1 Federal Register FR-2024-1… 2024-234… Cert… https://ww… https:… https:/…\n2     2 Federal Register FR-2024-1… 2024-272… Milk… https://ww… https:… https:/…\n3     3 Federal Register FR-2024-0… 2024-147… Milk… https://ww… https:… https:/…\n# ℹ 6 more variables: xmlLink &lt;lgl&gt;, otherLink1 &lt;lgl&gt;, otherLink2 &lt;lgl&gt;,\n#   teaser &lt;chr&gt;, historical &lt;lgl&gt;, publishdate &lt;date&gt;\n\n# Preparing for bulk download of government documents\ngovfiles1$id = govfiles1$packageId\npdf_govfiles_url = govfiles1$pdfLink\npdf_govfiles_id &lt;- govfiles1$id\n\npdf_govfiles_id\n\n[1] \"FR-2024-10-10\" \"FR-2024-12-02\" \"FR-2024-07-15\"\n\npdf_govfiles_url\n\n[1] \"https://www.govinfo.gov/content/pkg/FR-2024-10-10/pdf/2024-23448.pdf\"\n[2] \"https://www.govinfo.gov/content/pkg/FR-2024-12-02/pdf/2024-27228.pdf\"\n[3] \"https://www.govinfo.gov/content/pkg/FR-2024-07-15/pdf/2024-14769.pdf\"\n\n# Directory to save the pdf's\nsave_dir &lt;- \"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/EPPS6323/github/jmglendenning.github.io\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download files, potentially in parallel for speed\n## Try all\nresults &lt;- 1:length(pdf_govfiles_url) %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n#message(\"Finished downloads\")\n\n# Print results\nprint(results)\n\n[1] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/FR-2024-10-10/pdf/2024-23448.pdf\"\n[2] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/FR-2024-12-02/pdf/2024-27228.pdf\"\n[3] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/FR-2024-07-15/pdf/2024-14769.pdf\""
  },
  {
    "objectID": "assignment04.html#read-tutorial",
    "href": "assignment04.html#read-tutorial",
    "title": "Assignment 04",
    "section": "",
    "text": "of quanteda at https://tutorials.quanteda.io/\n\nRead"
  },
  {
    "objectID": "assignment04.html#download",
    "href": "assignment04.html#download",
    "title": "Assignment 04",
    "section": "",
    "text": "quanteda_textanalytics01.R from Teams\nIt is copied and run below.\n\n# Sample program for using quanteda for text modeling and analysis\n# Use vignette(\"auth\", package = \"rtweet\") for authentication\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\n#install.packages(c(\"quanteda\",\"quanteda.textmodels\",\"quanteda.textstats\",\"quanteda.textplots\"))\n\nlibrary(quanteda)\n\nPackage version: 4.2.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           159930 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\n\n# Network plot: tags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n# Network plot: Users\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 701 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  dfm() %&gt;% \n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Biden\",\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  # Increase min_termfreq from 5\n  dfm_trim(min_termfreq = 10, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\n\n\n\n\n# Locate keywords-in-context\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"trade\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"american\"),\n  kwic(tokens_inaugural, pattern = \"people\"),\n  kwic(tokens_inaugural, pattern = \"trade\")\n)"
  },
  {
    "objectID": "assignment04.html#analyze",
    "href": "assignment04.html#analyze",
    "title": "Assignment 04",
    "section": "",
    "text": "All of the analyses are run above. Please refer to that for these responses.\n\n\n\nFor the top tags in the document feature matrix, the primary topics other than the participants and their countries were corona virus, and fentanyl.\nLooking at the two feature concurrence matrices,\n\nFCM1\n\n#breaking and #breakingnews co-occur with #biden, #china, and #usa moderately (values between 4 and 5).\n#biden and #china co-occur heavily, particularly #biden + #china = 443 times, and #china + #usa = 76 times, suggesting a strong relationship.\n#pray4america doesn’t seem to co-occur with any other terms.\n#biden and #china seem to be central, with #usa being smaller.\n\nFCM2\n\nThis matrix deals with more specific terms: #joebiden, #xijinping, #america, #americans.\n#biden and #china both strongly co-occur with #joebiden, #xijinping, #america, and #americans. Notably:\n\n#china + #xijinping = 434 times\n#biden + #xijinping = 370 times\n\n#usa and #pray4america have very weak connections in this matrix (almost all zeros or low numbers).\n\n\nLooking at the network plot of the feature concurrence matrix, #fentanyl, #biden / #joebiden, #american(s), #xijinping and # coronavirus are strongly associated.\nFor the FCM of the user DFM, there were few associations between people.\nIn the text plot, there were a couple of echo-chambers worth noticing, but they likely had little or nothing to do with Biden/ Xi.\n\n\n\n\nIn the inaugural speeches, the major features of the speeches centered around government, country, people and similar topics.\nBreaking this down by president, Trump and Obama had more to say about america(n) than Bush and Biden, who more touted freedom and democracy.\n\n\nLooking at the combined lexical dispersion plot for all the presidents, most, except Kennedy peppered their speeches with american and people, but few used trade. It does not appear that there are obvious patterns without further statistical analysis.\nExcept for Bush, presidents mentioned people. There may have been an increase in the use of american with Trump and Biden."
  },
  {
    "objectID": "assignment04.html#what-is-wordfish",
    "href": "assignment04.html#what-is-wordfish",
    "title": "Assignment 04",
    "section": "",
    "text": "From https://tutorials.quanted.io\n“Wordfish is a Poisson scaling model of one-dimensional document positions (Slapin and Proksch 2008). Wordfish also allows for scaling documents, but in comparison to Wordscores, reference scores/texts are not required. Wordfish is an unsupervised one-dimensional text scaling method, meaning that it estimates the positions of documents solely based on the observed word frequencies.”\nWordfish is an unsupervised scaling method."
  },
  {
    "objectID": "assignment04.html#how-to-compare-positions-hint-wordfish-and-scaling-methods",
    "href": "assignment04.html#how-to-compare-positions-hint-wordfish-and-scaling-methods",
    "title": "Assignment 04",
    "section": "",
    "text": "Start with an DFM.\nUse Wordfish to model word counts based on:\n\nα (document fixed effect) – controls for length of the document.\n\nβ (word fixed effect) – controls for how common a word.\nθ (document position) – the position of the document on the latent scale (this is what you’re looking for).\nψ (word discrimination parameter) – shows how a word is associated with different positions.\n\n\n\nSo, mathematically: \\[E_{(y_{ij})} = exp(\\alpha_i + \\beta_j + \\psi_j * \\theta_i)\\]\n\nOutput:\n\nDocument positions (θ): each document gets a score placing it on the latent scale.\n\nWord weights (ψ): tell you whether a word is more common in in one type or class of texts.\n\n\n\nSome analysis using Wordfish on Quanteda\n\n# Load necessary libraries\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# U.S. inaugural speeches (post-1900)\ndata(\"data_corpus_inaugural\")\ncorpus &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1900)\n\n# Tokenize the corpus, remove punctuation and stopwords\ntoks &lt;- tokens(corpus, remove_punct = TRUE)\ntoks &lt;- tokens_remove(toks, stopwords(\"en\"))\n\n# Create document-feature matrix and trim low-frequency terms\ndfm &lt;- dfm(toks)\ndfm &lt;- dfm_trim(dfm, min_termfreq = 5)\n\n# Apply the Wordfish model using quanteda.textmodels with a directional anchor\nwf_model &lt;- textmodel_wordfish(dfm, dir = c(1, 2))\n\n# Extract word-level beta estimates (using margin = \"features\") and convert to numeric\nword_pos &lt;- data.frame(\n  word = featnames(dfm),\n  beta = as.numeric(coef(wf_model, margin = \"features\"))\n)\n\n# For a clearer plot, select the top 30 words with the most extreme beta values\nword_pos &lt;- word_pos %&gt;%\n  arrange(desc(abs(beta))) %&gt;%\n  head(30) %&gt;%\n  arrange(beta)\n\n# Base R plot of word positions\nplot(word_pos$beta, 1:nrow(word_pos),\n     pch = 16,\n     xlab = \"Word Position (Wordfish Estimate)\",\n     ylab = \"\",\n     main = \"Wordfish Scaling of Word Positions\\n(U.S. Inaugural Speeches)\",\n     yaxt = \"n\")  # suppress default y-axis labels\n\n# Add custom y-axis with word labels\naxis(2, at = 1:nrow(word_pos), labels = word_pos$word, las = 1, cex.axis = 0.8)"
  },
  {
    "objectID": "assignment04.html#create-a-corpus",
    "href": "assignment04.html#create-a-corpus",
    "title": "Assignment 04",
    "section": "",
    "text": "using government documents selected from the govinfo.gov website (use sample program govtdata01.R)\nThis creates a smaller corpus because it will be run every time quarto is rendered.\n\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nlibrary(readr)\n\n## CSV method\ngovfiles1= read_csv(file=\"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/EPPS6323/Labs/govinfo.csv\")\n\nRows: 3 Columns: 14\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): collection, packageId, granuleId, title, detailsLink, pdfLink, htm...\ndbl  (1): index\nlgl  (4): xmlLink, otherLink1, otherLink2, historical\ndate (1): publishdate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#summary(govfiles1)\nprint(govfiles1)\n\n# A tibble: 3 × 14\n  index collection       packageId  granuleId title detailsLink pdfLink htmlLink\n  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;   \n1     1 Federal Register FR-2024-1… 2024-234… Cert… https://ww… https:… https:/…\n2     2 Federal Register FR-2024-1… 2024-272… Milk… https://ww… https:… https:/…\n3     3 Federal Register FR-2024-0… 2024-147… Milk… https://ww… https:… https:/…\n# ℹ 6 more variables: xmlLink &lt;lgl&gt;, otherLink1 &lt;lgl&gt;, otherLink2 &lt;lgl&gt;,\n#   teaser &lt;chr&gt;, historical &lt;lgl&gt;, publishdate &lt;date&gt;\n\n# Preparing for bulk download of government documents\ngovfiles1$id = govfiles1$packageId\npdf_govfiles_url = govfiles1$pdfLink\npdf_govfiles_id &lt;- govfiles1$id\n\npdf_govfiles_id\n\n[1] \"FR-2024-10-10\" \"FR-2024-12-02\" \"FR-2024-07-15\"\n\npdf_govfiles_url\n\n[1] \"https://www.govinfo.gov/content/pkg/FR-2024-10-10/pdf/2024-23448.pdf\"\n[2] \"https://www.govinfo.gov/content/pkg/FR-2024-12-02/pdf/2024-27228.pdf\"\n[3] \"https://www.govinfo.gov/content/pkg/FR-2024-07-15/pdf/2024-14769.pdf\"\n\n# Directory to save the pdf's\nsave_dir &lt;- \"/Users/john/Library/Mobile Documents/com~apple~CloudDocs/Home/John/GIS/EPPS6323/github/jmglendenning.github.io\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download files, potentially in parallel for speed\n## Try all\nresults &lt;- 1:length(pdf_govfiles_url) %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n#message(\"Finished downloads\")\n\n# Print results\nprint(results)\n\n[1] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/FR-2024-10-10/pdf/2024-23448.pdf\"\n[2] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/FR-2024-12-02/pdf/2024-27228.pdf\"\n[3] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/FR-2024-07-15/pdf/2024-14769.pdf\""
  },
  {
    "objectID": "prog_test.knit.html",
    "href": "prog_test.knit.html",
    "title": "EPPS 6323: Project Test",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(textdata)\nlibrary(ggplot2)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n# Define the folder containing text files\nfolder_path &lt;- \"/Volumes/VirtMach/GIS/Corpus/Working/1990\"\n\n# Read all text files in the folder\nfiles &lt;- list.files(folder_path, full.names = TRUE)\n\n# Function to read and preprocess files\nread_text_files &lt;- function(file) {\n  text &lt;- readLines(file, warn = FALSE)\n  tibble(file = basename(file), text = text)\n}\n\n# Read and combine text files into a single dataframe\ndocuments &lt;- map_dfr(files, read_text_files)\nsummary(documents)\n\n     file               text          \n Length:1056700     Length:1056700    \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n# Unnest tokens: split text into words\ntidy_docs &lt;- documents %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)  # Remove common stop words\n\nJoining with `by = join_by(word)`\n\ntidy_docs %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 1000) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n# ---- TF-IDF Analysis ----\ntfidf &lt;- tidy_docs %&gt;%\n  count(file, word) %&gt;%\n  bind_tf_idf(word, file, n) %&gt;%\n  arrange(desc(tf_idf))\n\n# Display the top 10 high TF-IDF words\nprint(tfidf %&gt;% top_n(10, tf_idf))\n\n# A tibble: 10 × 6\n   file                                        word      n      tf   idf  tf_idf\n   &lt;chr&gt;                                       &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Haley-RuralSettlementConventus-1996.pdf     000     678 9.10e-3  1.18 0.0107 \n 2 Haley-RuralSettlementConventus-1996.pdf     415.2   126 1.69e-3  3.66 0.00620\n 3 Haley-RuralSettlementConventus-1996.pdf     629.…   105 1.41e-3  3.66 0.00517\n 4 Whitmarsh-GreekRomanDialogue-1999.pdf       532.2   114 1.72e-3  2.97 0.00510\n 5 Elsner-HagiographicGeographyTravel-1997.pdf 532.2    96 1.67e-3  2.97 0.00495\n 6 Whitmarsh-GreekRomanDialogue-1999.pdf       778.2    95 1.43e-3  2.97 0.00425\n 7 Elsner-HagiographicGeographyTravel-1997.pdf 778.2    80 1.39e-3  2.97 0.00413\n 8 Elsner-HagiographicGeographyTravel-1997.pdf link…    79 1.37e-3  2.97 0.00407\n 9 Whitmarsh-GreekRomanDialogue-1999.pdf       link…    86 1.30e-3  2.97 0.00385\n10 Leach-HoracesSabineTopography-1993.pdf      5100     64 9.04e-4  2.97 0.00269\n\n# ---- Bigram Analysis ----\nbigrams &lt;- documents %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %&gt;%\n  separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \") %&gt;%\n  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %&gt;%\n  count(word1, word2, sort = TRUE)\n\n# Display top 10 bigrams\nprint(bigrams %&gt;% head(10))\n\n# A tibble: 10 × 3\n   word1 word2           n\n   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n 1 &lt;NA&gt;  &lt;NA&gt;       174386\n 2 0     obj         57311\n 3 type  structelem  47709\n 4 2     0           47328\n 5 span  type        43747\n 6 0     0           20654\n 7 4     0            6946\n 8 0     4            5414\n 9 0     40           4292\n10 4     4            3670\n\n# ---- Sentiment Analysis ----\nsentiment_lexicon &lt;- get_sentiments(\"bing\")  # Use Bing sentiment lexicon\n\nsentiment_scores &lt;- tidy_docs %&gt;%\n  inner_join(sentiment_lexicon, by = \"word\") %&gt;%\n  count(file, sentiment) %&gt;%\n  spread(sentiment, n, fill = 0) %&gt;%\n  mutate(sentiment_score = positive - negative)\n\n# Display sentiment scores per document\nprint(sentiment_scores)\n\n# A tibble: 39 × 4\n   file                                        negative positive sentiment_score\n   &lt;chr&gt;                                          &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1 Astin-RoleCensorsRoman-1990.pdf                   71      157              86\n 2 Ault-KopronesOilPresses-1999.pdf                 194       24            -170\n 3 Bagnall-LandholdingLateRoman-1992.pdf            111       91             -20\n 4 Biers-MassProductionStandardized-1994.pdf        123       23            -100\n 5 Bowersock-BelochBirthDemography-1997.pdf          41       25             -16\n 6 Cascio-SizeRomanPopulation-1994.pdf              223      123            -100\n 7 CORNELL-HANNIBALSLEGACYEFFECTS-1996.pdf           27       10             -17\n 8 Elsner-HagiographicGeographyTravel-1997.pdf       29       84              55\n 9 Erdkamp-AgricultureUnderemploymentCost-199…      157       63             -94\n10 Fagan-ReliabilityRomanRebuilding-1996.pdf         51       17             -34\n# ℹ 29 more rows\n\n# ---- Topic Modeling (LDA) ----\ndtm &lt;- tidy_docs %&gt;%\n  count(file, word) %&gt;%\n  cast_dtm(file, word, n)\n\n# Fit LDA model with k topics\nk &lt;- 6  # Adjust as needed\nlda_model &lt;- LDA(dtm, k = k, control = list(seed = 1234))\n\n# Extract topics and terms\ntopics &lt;- tidy(lda_model, matrix = \"beta\")\n\n# View top terms for each topic\ntop_terms &lt;- topics %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;%\n  ungroup()\n\n# Display top terms\nprint(top_terms)\n\n# A tibble: 60 × 3\n   topic term    beta\n   &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1     1 0     0.0615\n 2     1 2     0.0271\n 3     1 7     0.0198\n 4     1 5     0.0197\n 5     1 9     0.0197\n 6     1 1     0.0197\n 7     1 8     0.0196\n 8     1 6     0.0192\n 9     1 3     0.0189\n10     1 4     0.0181\n# ℹ 50 more rows\n\n# ---- Topic Distribution per Document ----\ntopic_distribution &lt;- tidy(lda_model, matrix = \"gamma\") %&gt;%\n  rename(document = document, topic_probability = gamma)\n\n# Display document-topic probabilities\nprint(topic_distribution)\n\n# A tibble: 234 × 3\n   document                                        topic topic_probability\n   &lt;chr&gt;                                           &lt;int&gt;             &lt;dbl&gt;\n 1 Astin-RoleCensorsRoman-1990.pdf                     1       0.977      \n 2 Ault-KopronesOilPresses-1999.pdf                    1       0.000000163\n 3 Bagnall-LandholdingLateRoman-1992.pdf               1       0.975      \n 4 Biers-MassProductionStandardized-1994.pdf           1       0.000000327\n 5 Bowersock-BelochBirthDemography-1997.pdf            1       0.967      \n 6 CORNELL-HANNIBALSLEGACYEFFECTS-1996.pdf             1       0.000000812\n 7 Cascio-SizeRomanPopulation-1994.pdf                 1       0.000000172\n 8 Elsner-HagiographicGeographyTravel-1997.pdf         1       0.000000900\n 9 Erdkamp-AgricultureUnderemploymentCost-1999.pdf     1       0.000000257\n10 Fagan-ReliabilityRomanRebuilding-1996.pdf           1       0.964      \n# ℹ 224 more rows\n\n# ---- Visualization: Top Terms per Topic ----\ntop_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(term, beta, fill = as.factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(title = \"Top Terms in Each Topic\",\n       x = \"Terms\",\n       y = \"Beta Probability\")\n\n\n\n\n\n\n\n# ---- Visualization: Sentiment Scores ----\nsentiment_scores %&gt;%\n  ggplot(aes(x = file, y = sentiment_score, fill = sentiment_score &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Sentiment Scores per Document\",\n       x = \"Document\",\n       y = \"Sentiment Score\")\n\n\n\n\n\n\n\n# ---- Visualization: Topic Distribution ----\ntopic_distribution %&gt;%\n  ggplot(aes(x = document, y = topic_probability, fill = as.factor(topic))) +\n  geom_col(show.legend = TRUE) +\n  coord_flip() +\n  labs(title = \"Topic Distribution per Document\",\n       x = \"Document\",\n       y = \"Probability\")"
  },
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/n9/9b7__npd72z18bl4l57tvj4m0000gn/T//RtmpL5h40O/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#indexing-data-using",
    "href": "Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "Lab02.html#loading-data-from-github-remote",
    "href": "Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "Lab02.html#load-data-from-islr-website",
    "href": "Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "Lab02.html#linear-regression",
    "href": "Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/n9/9b7__npd72z18bl4l57tvj4m0000gn/T//RtmpL5h40O/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Lab02.html#multiple-linear-regression",
    "href": "Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#qualitative-predictors",
    "href": "Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "comparison01.html",
    "href": "comparison01.html",
    "title": "Comparison of Shmueli & Breiman",
    "section": "",
    "text": "Comparison of “To Explain or to Predict?” by Galit Shmueli and “Statistical Modeling: The Two Cultures” by Leo Breiman\n\nBackground\nThe two papers from Breiman and Shmueli are both critiques of traditional modeling approaches. Breiman advocates for shifting from traditional parametric modeling to algorithmic modeling. Schmueli advocates differentiating between explanatory modeling and predictive modeling.\n\n\nBreiman\nThe paper by Breiman (Breiman, 2001) critiques the “data modeling culture” in the field of traditional statistics. He argues that there has been an overreliance on statistical models and interpretability at the expense of algorithmic models, which favor predictability. “Usually, simple parametric models imposed on data generated by complex systems…result in a loss of accuracy and information as compared to algorithmic models.” This has led to models that may not capture real-world data complexities and assume an underlying data generation process that may not exist.\nThrough several examples, he examines the use of traditional modeling as compared to algorithmic modeling. His conclusion is that the “emphasis needs to be on the problem and not the data.”\nAt the end of this paper, there are comments and follow-up by some noted statisticians, which generally agree (except for Cox) with Breiman but bring about some interesting caveats. For instance, Hoadley warns that when creating algorithmic models, care should be taken not to overfit models.\n\n\nShmueli\nThe paper by Shmueli (Shmueli, 2010) looks at the difference between explanatory and predictive modeling, outlining that each supports different goals. Explanatory modeling tests causal hypotheses to describe the underlying mechanisms. Predictive modeling, according to Shmueli, “predictive modeling [is] the process of applying a statistical model or data mining algorithm to data for the purpose of predicting new or future observations.”\nThe paper emphasizes that understanding the difference between the two models is critical to proper model selection during the process. Understanding this difference can impact the study design, data collection, data processing, variable choices, methodologies, and nearly everything else in the modeling process.\nIn the end, it is suggested that it is important to be aware of how models are used and to acknowledge the difference between explanatory, predictive, and descriptive modeling.\n\n\nComparison\nBoth of these papers point out that it is important to distinguish between the different modeling methods while highlighting the strengths and weaknesses of each. They also both acknowledge the growing acceptance and usefulness of black-box predictive modeling for data science.\nWhile both support the role of modeling, Breiman advocates primarily for predictive modeling, and Shmueli advocates for making a conscious choice between the two. Shmueli also cautions against assuming that a strong explanatory model means it will have good predictive performance. Breiman states that the historical modeling focus has kept statisticians from effectively modeling real problems.\n\n\nConclusion\nThese papers discuss the evolving landscape of statistical modeling, urging researchers to be more deliberate in their methodological choices and recognize the trade-offs between explanation and prediction. Although both of these papers are decades old, the conclusions remain relevant.\n\n\nBibliography\nBreiman, Leo. 2001. “Statistical modeling: The two cultures (with comments and a rejoinder by the author)”. Statistical Science, 16(3), pp.199-231.\nShmueli, Galit. 2010. “To explain or to predict?.” Statistical Science 25, no. 3: 289-310."
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9940492\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9940492\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Prepare08.html",
    "href": "Prepare08.html",
    "title": "Prepare for Class08",
    "section": "",
    "text": "Zhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min et al. 2023. “A survey of large language models.” arXiv preprint arXiv:2303.18223 1, no. 2.\n\n\nCompleted\n\n\nChang, Yupeng, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen et al. 2024. “A survey on evaluation of large language models.” ACM transactions on intelligent systems and technology 15, no. 31-45.\n\n\nCompleted\n\n\nMin, Bonan, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. “Recent advances in natural language processing via large pre-trained language models: A survey.” ACM Computing Surveys 56, no. 2: 1-40.\n\n\nCompleted\n\n\n\n\nDo a quick survey based on the readings how NLP and LLM have advanced Knowledge Mining as a field. You may use one to two AI models to help:\n\nThe following prompt, which was generated by DeepSeek, was used for the three documents to set a baseline using ChatGPT 4.5:\n\n\n“Write a concise review summarizing the advancements in Knowledge Mining, focusing on the impact of Natural Language Processing (NLP) and Large Language Models (LLMs). Highlight key developments, such as improved information extraction, enhanced semantic understanding, and the role of LLMs in automating knowledge discovery. Discuss how these advancements have transformed data analysis, decision-making, and knowledge representation across industries. Include examples of cutting-edge applications and their implications for the future of Knowledge Mining.”\n\n\n\nSummarize the findings\n\n\n\nThe rapid advancements in Natural Language Processing (NLP) and the developments in Large Language Models (LLMs) have transformed Knowledge Mining. These technologies have improved our abilities to extract information from large numbers of documents.\nPart of this transformation has been the change from statistical language models (SLMs) to neural language models (NLMs), and more so to pre-trained language models (PLMs). This has improved our abilities to accurately interpret documents. With the subsequent rise of LLMs, such as GPT-series models, there has been a improvement of scaling techniques, significantly expanding model capacities.\nThere are emergent capabilities in LLMs not previously found in other types of models.This has brought about new tools for text processing, including in-context learning, and chain-of-thought reasoning. This has allowed for the models to handle more complex tasks.\nLeveraging these developments are services such as automatic text classification, question-answer systems, etc., which all benefit from the new robust frameworks.\nKnowledge Mining, driven by ongoing research into NLP and LLMs, will develop deeper semantic insights, more accurate extraction techniques, and wider acceptance. Continuous development will be essential to address any limitations.\nA key development has been the dramatic improvement in information extraction capabilities. Modern LLMs can now reasonable identify complex entities, relationships, and context within large corpora, reducing reliance on manual curation, although text extraction from certain document types is still problematic. Enhanced semantic understanding through transformer-based architectures has enabled systems to better capture meanings and intent, finding insights into previously inaccessible data sources.\nLLMs like GPT-4, PaLM 2, and Claude have been instrumental in automating knowledge discovery. These models can synthesize widely varying information, generate human-sounding summaries, and recommend processes, which significantly accelerate workflows in many fields.\nThe impact on data analysis and decision-making is huge. Organizations now deploy knowledge mining software that integrate LLMs to perform tasks like automated document classification, assess risks, and analyze customer feedback. Fine-tuning various models have allowed LLMs to be more transparent and domain-specific.\nLLMs are now working with other smart tools, like knowledge graphs and models that understand pictures and text together. This helps computers organize information in smarter ways, linking facts and ideas. This allows problems to be solved and new ideas developed faster. They are getting better at pulling out important details from books, articles, and other messy text. They can now spot connections between different pieces of information, making research and decision-making much easier.\n\n\n\nIdentify the key concepts\n\n\n\n\n“A Survey of Large Language Models”\n\nEvolution - Models are evolving through research and training.\nEmergence - New abilities are developing based on model size\nScaling - Increasing model size improves existing capabilities\nTraining Techniques - New training techniques and drawing from frameworks improves models\nSpeed - Various methods are being tried to reduce the cost and lower time to build models\n\n“Recent advances in natural language processing via large pre-trained language models: A survey.”\n\nParadigm 1: Pretrain then Fine-Tune Start with a general-purpose model and fine-tune it with task-specific datasets.\nParadigm 2: Prompt-based learning Through instruction-based learning, template-based learning, or proxy-task-base learning to encourage PLMs to perform specific tasks.\nParadigm 3: NLP as text generation This reformulates NLP problems as text-to-text attempting to generate the correct output in a string form that encodes the classification labels or structures.\n\n“A survey on evaluation of large language models.” Evaluation of LLMs is imperative to the development of AI models. Key points:\n\nBenchmarks are necessary\nComplete Behavioral Evaluation in an open environment\nRobustness Evaluation to check on a wide variety of inputs\nDynamic and Evolving Evaluation so that evaluation methods mirror model capabilities\nPrincipled and Trustworthy Evaluation to maintain integrity\nUnified Evaluation to support a variety of tasks\n\n\n\n\n\nDraw knowledge map/graph to illustrate the advancements\n\n\n\n\n\n\n\ngraph TD\n\n%% Main Concept\nA[Knowledge Mining]\n\n%% Primary Branches\nA --&gt; B[NLP Evolution]\nA --&gt; C[LLM Capabilities]\nA --&gt; D[Applications]\nA --&gt; E[Evaluation & Trends]\n\n%% NLP Evolution\nB --&gt; B1[Statistical Models]\nB1 --&gt; B2[Neural Models]\nB2 --&gt; B3[Pre-trained Models]\nB3 --&gt; B4[Large Models]\n\n%% LLM Capabilities\nC --&gt; C1[In-Context Learning]\nC1 --&gt; C2[Instruction Following]\nC2 --&gt; C3[Reasoning]\n\n%% Applications\nD --&gt; D1[Extraction]\nD1 --&gt; D2[Semantic Understanding]\nD2 --&gt; D3[Discovery]\n\n%% Knowledge Discovery Applications\nD3 --&gt; D3a[Research]\nD3a --&gt; D3b[Legal Automation]\nD3b --&gt; D3c[Search]\nD3c --&gt; D3d[Analysis]\n\n%% Evaluation & Trends\nE --&gt; E1[Benchmarks]\nE1 --&gt; E2[Ethics & Robustness]\nE2 --&gt; E3[Multimodal]\nE3 --&gt; E4[Human-AI Alignment]\n\n\n\n\n\n\n\n\n\n\n\nHow NLP and LLM can assist in your research and provide data/assistance for your project? Give specific examples.\n\n\n\nMy project is based on NLP, so that technology and methodology is crucial to the work that I am doing. Where I have used LLMs:\nCode development: It is signficantly faster to request code chunks from an LLM than to write them myself. I can put together code like LEGO bricks, which is significantly faster than writing it cold, myself.\nText cleaning: I have been able to use LLMs very occassionally to strip text from documents to create a corpus.\nArticle summaries: Summarizing articles helps me understand what words in a word cloud might mean. For instance lead the element versus lead the verb.\nResearch: I have used the LLMs instead of search engines to answer questions. They have been faster and more successful that nearly all web-search queries.\n\n\nName limitations and suggest solutions\n\n\nCopyrights: Oddly enough the models that were trained on other peoples work without attribution, seem to reject processing these same works for me. I have tried to use LLMs for text extraction, and I will get either an specific copyright error, or the process will fail without an informative warning. The solution to this is likely to run my own models where I have more control over them.\nDocument Sizes: Most unpaid models have document size limits. This means that I either have to use a paid model or break documents into smaller chunks, losing valuable connections in the process.\nCode problems: Sometimes the code generated is bad or wrong. It may not perform as expected. Thorough review and testing is important.\nHallucinations & Errors: The responses are not perfect, and in some domains, a websearch is still more accurate. These issues are expected to improve over time. Any research should always check the answers to any questions, whether LLMs are used or something else.\n\n\nWrite an AI for research guide (you can use bullet points) incorporating ethnical, technical and acaademic concerns that could lead to more problems than solutions (e.g. Hinton).\n\n\n\nAI Research Guide\n\nIntroduction\n\nWhat is AI and Why is it Important?\n\nArtificial Intelligence (AI) is a technology that enables machines to mimic human thinking, helping automate and simplify complex tasks. It can enhance efficiency, accuracy, and productivity across various areas, including research, but it is not without issues.\n\nPurpose of this Guide\n\nThis guide explains the basic benefits and potential problems of using AI in research. It is designed to help you use AI responsibly and effectively.\nHow AI Helps with Research\n\nWhat AI Can Do\n\nAI analyzes quickly analyzes large sets of data, making it easier for researchers to find patterns and insights. It can also automate repetitive tasks, saving time and effort.\n\nExamples of AI in Research\n\nAI is used to summarize writing, do predictive analysis, write software and many other useful tasks. Multimedia uses AI include creating podcasts or making images.\nEthical Issues When Using AI\n\nProtect Privacy\n\nAI uses large amounts of data of dubious origin which raises concerns about privacy. The storage and useage of personal information is a very important issue that must be address. Protecting this type of data from misuse is of paramount concern. This requires careful attention to privacy laws, which are very limited in the United States, and adherence to ethical standards.\n\nBe Fair and Unbiased\n\nAI systems include biases from the data they are trained on. If a model is trained on a biased dataset, it too will be biased in the same way. For example, if a model does not know about a new version of a Python package, the code it writes may contain inaccuracies. AIs need to be checked for biases on a regular basis and adjusted to prevent biases.\n\nUnderstand How AI Makes Decisions\n\nTo effectively use AI, it is important to understand how AI arrives at decisions. This transparency to validate the findings is the only way to trust an AI.\nTechnical Problems to Be Careful About\n\nHow Reliable is AI?\n\nAI tools will produce inaccurate results some of the times depending on how the model was trained. All answers, like any good research, should be checked for accuracy.\n\nUsing AI Tools Correctly\n\nProper training to use AI tools is required. An understanding of how to engineer prompts leads to better responses, and conversely poor prompts can lead to misleading results or ineffective research.\n\nTechnology Needs and Sustainability\n\n\nAI uses significant compute power and energy which directly impacts sustainability. Using AI in the future will require considering energy-efficient solutions and how to use optimal technology resources.\n\nAcademic Integrity and AI\n\nPlagiarism\n\nGiven that the sources of the text from AI-generated content are unknown, using AI can inadvertently lead to accusations of plagiarism. Using AI must be properly cited. Always give credit when using AI-generated information.\n\nHonest Research\n\nA researcher’s findings must genuinely reflect their efforts and contributions. Transparency about its use maintains trust.\n\nAI to Helps, Not Replaces, Your Work\n\nThese tools should support your efforts, not replace them. As with using any tool, critical thinking and original insights are crucial to any research.\nRisks and Problems of Using AI\n\nMisuse of AI\n\nPoor or inattentive use of AI may lead to incorrect findings, misinformation, or even ethical violations. Responsible use will prevent this.\n\nLoss of Skills\n\nAn over-reliance on AI may reduce critical thinking and analytical skills, dulling one’s research. Critical analysis remains an important skill.\n\nWho Gets to Use AI?\n\nAs with every technology, access to it is not equal. Given the costs to train and run models, the best models will cost a significant amount of money, thus stratifying access. Without fair and equal access to this technology, much research may not be able to be peer reviewed properly.\nTips for Using Responsibly\n\nBasic Rules, loosely based on Asimov’s Three Laws:\n\nBe transparent and explainable.\nBe aware of bias and discrimination.\nPriorize people and privacy over results.\n\nLearning About AI\n\nKnow your tools and develop a basic understanding of how AI works. Regular training and learning is important.\n\nTransparency, Again\n\nClear citation and documentation of AI processes allows others to evaluate and trust your research.\nThe Future of AI in Research\n\nWhat’s Coming Next\n\nAI will be increasingly integrated into research with new applications and extended capabilities. Continuing education will be crucial.\n\nImportant Discussions About AI\n\nThe ethical, social, and technical implications of the technology will shape all future research practices.\nConclusion\n\nCritical assessment of all AI-generated information, ethical use, and clear documentation of its use in your work are important factors to ensure integrity in you research."
  },
  {
    "objectID": "Prepare08.html#assigned-reading",
    "href": "Prepare08.html#assigned-reading",
    "title": "Prepare for Class08",
    "section": "",
    "text": "Zhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min et al. 2023. “A survey of large language models.” arXiv preprint arXiv:2303.18223 1, no. 2.\n\n\nCompleted\n\n\nChang, Yupeng, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen et al. 2024. “A survey on evaluation of large language models.” ACM transactions on intelligent systems and technology 15, no. 31-45.\n\n\nCompleted\n\n\nMin, Bonan, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. “Recent advances in natural language processing via large pre-trained language models: A survey.” ACM Computing Surveys 56, no. 2: 1-40.\n\n\nCompleted"
  },
  {
    "objectID": "Prepare08.html#quick-survey",
    "href": "Prepare08.html#quick-survey",
    "title": "Prepare for Class08",
    "section": "",
    "text": "Do a quick survey based on the readings how NLP and LLM have advanced Knowledge Mining as a field. You may use one to two AI models to help:\n\nThe following prompt, which was generated by DeepSeek, was used for the three documents to set a baseline using ChatGPT 4.5:\n\n\n“Write a concise review summarizing the advancements in Knowledge Mining, focusing on the impact of Natural Language Processing (NLP) and Large Language Models (LLMs). Highlight key developments, such as improved information extraction, enhanced semantic understanding, and the role of LLMs in automating knowledge discovery. Discuss how these advancements have transformed data analysis, decision-making, and knowledge representation across industries. Include examples of cutting-edge applications and their implications for the future of Knowledge Mining.”\n\n\n\nSummarize the findings\n\n\n\nThe rapid advancements in Natural Language Processing (NLP) and the developments in Large Language Models (LLMs) have transformed Knowledge Mining. These technologies have improved our abilities to extract information from large numbers of documents.\nPart of this transformation has been the change from statistical language models (SLMs) to neural language models (NLMs), and more so to pre-trained language models (PLMs). This has improved our abilities to accurately interpret documents. With the subsequent rise of LLMs, such as GPT-series models, there has been a improvement of scaling techniques, significantly expanding model capacities.\nThere are emergent capabilities in LLMs not previously found in other types of models.This has brought about new tools for text processing, including in-context learning, and chain-of-thought reasoning. This has allowed for the models to handle more complex tasks.\nLeveraging these developments are services such as automatic text classification, question-answer systems, etc., which all benefit from the new robust frameworks.\nKnowledge Mining, driven by ongoing research into NLP and LLMs, will develop deeper semantic insights, more accurate extraction techniques, and wider acceptance. Continuous development will be essential to address any limitations.\nA key development has been the dramatic improvement in information extraction capabilities. Modern LLMs can now reasonable identify complex entities, relationships, and context within large corpora, reducing reliance on manual curation, although text extraction from certain document types is still problematic. Enhanced semantic understanding through transformer-based architectures has enabled systems to better capture meanings and intent, finding insights into previously inaccessible data sources.\nLLMs like GPT-4, PaLM 2, and Claude have been instrumental in automating knowledge discovery. These models can synthesize widely varying information, generate human-sounding summaries, and recommend processes, which significantly accelerate workflows in many fields.\nThe impact on data analysis and decision-making is huge. Organizations now deploy knowledge mining software that integrate LLMs to perform tasks like automated document classification, assess risks, and analyze customer feedback. Fine-tuning various models have allowed LLMs to be more transparent and domain-specific.\nLLMs are now working with other smart tools, like knowledge graphs and models that understand pictures and text together. This helps computers organize information in smarter ways, linking facts and ideas. This allows problems to be solved and new ideas developed faster. They are getting better at pulling out important details from books, articles, and other messy text. They can now spot connections between different pieces of information, making research and decision-making much easier.\n\n\n\nIdentify the key concepts\n\n\n\n\n“A Survey of Large Language Models”\n\nEvolution - Models are evolving through research and training.\nEmergence - New abilities are developing based on model size\nScaling - Increasing model size improves existing capabilities\nTraining Techniques - New training techniques and drawing from frameworks improves models\nSpeed - Various methods are being tried to reduce the cost and lower time to build models\n\n“Recent advances in natural language processing via large pre-trained language models: A survey.”\n\nParadigm 1: Pretrain then Fine-Tune Start with a general-purpose model and fine-tune it with task-specific datasets.\nParadigm 2: Prompt-based learning Through instruction-based learning, template-based learning, or proxy-task-base learning to encourage PLMs to perform specific tasks.\nParadigm 3: NLP as text generation This reformulates NLP problems as text-to-text attempting to generate the correct output in a string form that encodes the classification labels or structures.\n\n“A survey on evaluation of large language models.” Evaluation of LLMs is imperative to the development of AI models. Key points:\n\nBenchmarks are necessary\nComplete Behavioral Evaluation in an open environment\nRobustness Evaluation to check on a wide variety of inputs\nDynamic and Evolving Evaluation so that evaluation methods mirror model capabilities\nPrincipled and Trustworthy Evaluation to maintain integrity\nUnified Evaluation to support a variety of tasks\n\n\n\n\n\nDraw knowledge map/graph to illustrate the advancements\n\n\n\n\n\n\n\ngraph TD\n\n%% Main Concept\nA[Knowledge Mining]\n\n%% Primary Branches\nA --&gt; B[NLP Evolution]\nA --&gt; C[LLM Capabilities]\nA --&gt; D[Applications]\nA --&gt; E[Evaluation & Trends]\n\n%% NLP Evolution\nB --&gt; B1[Statistical Models]\nB1 --&gt; B2[Neural Models]\nB2 --&gt; B3[Pre-trained Models]\nB3 --&gt; B4[Large Models]\n\n%% LLM Capabilities\nC --&gt; C1[In-Context Learning]\nC1 --&gt; C2[Instruction Following]\nC2 --&gt; C3[Reasoning]\n\n%% Applications\nD --&gt; D1[Extraction]\nD1 --&gt; D2[Semantic Understanding]\nD2 --&gt; D3[Discovery]\n\n%% Knowledge Discovery Applications\nD3 --&gt; D3a[Research]\nD3a --&gt; D3b[Legal Automation]\nD3b --&gt; D3c[Search]\nD3c --&gt; D3d[Analysis]\n\n%% Evaluation & Trends\nE --&gt; E1[Benchmarks]\nE1 --&gt; E2[Ethics & Robustness]\nE2 --&gt; E3[Multimodal]\nE3 --&gt; E4[Human-AI Alignment]"
  },
  {
    "objectID": "Prepare08.html#prepare-for-discussion",
    "href": "Prepare08.html#prepare-for-discussion",
    "title": "Prepare for Class08",
    "section": "",
    "text": "How NLP and LLM can assist in your research and provide data/assistance for your project? Give specific examples."
  },
  {
    "objectID": "Work.html",
    "href": "Work.html",
    "title": "Work",
    "section": "",
    "text": "Work samples will go here.\n\n\n\nPrincess Donut"
  },
  {
    "objectID": "assignment02.html",
    "href": "assignment02.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Lab01, Lab02 in R (Download the .qmd files in Teams)\n\n\nRender and upload your files to GitHub website\nBe sure to uploaded associated folders (e.g. Lab01.html, Lab01_files, Lab02.html, Lab02_files)\n\n\n\nCompleted. In navigation bar under ‘Assignments.’\n\n\n\nReview Chapters 3 to 7 in R4DS, Section 3.5 in Data programming. Run an exploratory data analysis with R using the TEDS2016 dataset (sample codes as follows):\n\n\n\nCompleted. See below.\n\n\nlibrary(haven, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(knitr, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(dplyr, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(car, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(vcd, quietly = TRUE, warn.conflicts = FALSE)\nlibrary(viridis, quietly = TRUE, warn.conflicts = FALSE)\n\n# Read Data\nTEDS_2016 &lt;-\nread_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nhead(TEDS_2016)\n\n# A tibble: 6 × 54\n  District      Sex     Age     Edu     Arear   Career  Career8 Ethnic  Party   \n  &lt;dbl+lbl&gt;     &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n1 201 [Yi Lan … 2 [Fem… 4 [50-… 4 [Col… 1 [Tai… 1 [Hig… 1 [Civ… 1 [Tai… 25 [Neu…\n2 201 [Yi Lan … 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n3 201 [Yi Lan … 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n4 201 [Yi Lan … 1 [Mal… 4 [50-… 2 [Jun… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n5 201 [Yi Lan … 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 9 [Nor… 25 [Neu…\n6 201 [Yi Lan … 2 [Fem… 5 [Abo… 2 [Jun… 1 [Tai… 2 [Low… 7 [Hou… 1 [Tai…  6 [Som…\n# ℹ 45 more variables: PartyID &lt;dbl+lbl&gt;, Tondu &lt;dbl+lbl&gt;, Tondu3 &lt;dbl+lbl&gt;,\n#   nI2 &lt;dbl+lbl&gt;, votetsai &lt;dbl&gt;, green &lt;dbl&gt;, votetsai_nm &lt;dbl&gt;,\n#   votetsai_all &lt;dbl&gt;, Independence &lt;dbl&gt;, Unification &lt;dbl&gt;, sq &lt;dbl&gt;,\n#   Taiwanese &lt;dbl&gt;, edu &lt;dbl&gt;, female &lt;dbl&gt;, whitecollar &lt;dbl&gt;,\n#   lowincome &lt;dbl&gt;, income &lt;dbl&gt;, income_nm &lt;dbl&gt;, age &lt;dbl&gt;, KMT &lt;dbl&gt;,\n#   DPP &lt;dbl&gt;, npp &lt;dbl&gt;, noparty &lt;dbl&gt;, pfp &lt;dbl&gt;, South &lt;dbl&gt;, north &lt;dbl&gt;,\n#   Minnan_father &lt;dbl&gt;, Mainland_father &lt;dbl&gt;, Econ_worse &lt;dbl&gt;, …\n\nstr(TEDS_2016)    # Check structure of data\n\ntibble [1,690 × 54] (S3: tbl_df/tbl/data.frame)\n $ District       : dbl+lbl [1:1690] 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201...\n   ..@ label       : chr \"District\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:73] 201 401 501 502 701 702 703 704 801 802 ...\n   .. ..- attr(*, \"names\")= chr [1:73] \"Yi Lan County Single District\" \"Hsinchu County Single District\" \"Miaoli County 1st District\" \"Miaoli County 2nd District\" ...\n $ Sex            : dbl+lbl [1:1690] 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1,...\n   ..@ label       : chr \"Sex\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:2] 1 2\n   .. ..- attr(*, \"names\")= chr [1:2] \"Male\" \"Female\"\n $ Age            : dbl+lbl [1:1690] 4, 2, 5, 4, 5, 5, 5, 4, 5, 4, 5, 1, 5, 3, 4, 5, 4, 5,...\n   ..@ label       : chr \"Age\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"20-29\" \"30-39\" \"40-49\" \"50-59\" ...\n $ Edu            : dbl+lbl [1:1690] 4, 5, 5, 2, 1, 2, 1, 5, 1, 1, 1, 2, 1, 5, 5, 1, 3, 4,...\n   ..@ label       : chr \"Education\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:6] 1 2 3 4 5 9\n   .. ..- attr(*, \"names\")= chr [1:6] \"Below elementary school\" \"Junior high school\" \"Senior high school\" \"College\" ...\n $ Arear          : dbl+lbl [1:1690] 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n   ..@ label       : chr \"Area\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:6] 1 2 3 4 5 6\n   .. ..- attr(*, \"names\")= chr [1:6] \"Taipei, New Taipei, Keelung and Yi Lan\" \"Taoyuan, Hsinchu and Miaoli\" \"Taichung, Changhua and Nantou\" \"Yunlin, Chiayi and Tainan\" ...\n $ Career         : dbl+lbl [1:1690] 1, 2, 1, 4, 3, 2, 4, 1, 4, 3, 3, 5, 5, 4, 1, 5, 2, 2,...\n   ..@ label       : chr \"Occupations5\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"Hight-class WHITE COLLAR\" \"Low-class WHITE COLLAR\" \"FARMER\" \"WORKER\" ...\n $ Career8        : dbl+lbl [1:1690] 1, 3, 1, 4, 5, 7, 4, 2, 4, 5, 5, 7, 7, 7, 2, 7, 3, 1,...\n   ..@ label       : chr \"Occupation8\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:8] 1 2 3 4 5 6 7 8\n   .. ..- attr(*, \"names\")= chr [1:8] \"Civil servants\" \"Managers and  Professionals (priv.)\" \"CLERKS (priv.)\" \"Labor (priv.)\" ...\n $ Ethnic         : dbl+lbl [1:1690] 1, 2, 2, 1, 9, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 9, 2, 2,...\n   ..@ label       : chr \"Ethnic\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:4] 1 2 3 9\n   .. ..- attr(*, \"names\")= chr [1:4] \"Taiwanese\" \"Both\" \"Chinese\" \"Noresponse\"\n $ Party          : dbl+lbl [1:1690] 25, 25,  3, 25, 25,  6, 25, 24, 25, 25,  6,  5, 25,  ...\n   ..@ label       : chr \"Party Preference\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:26] 1 2 3 4 5 6 7 8 9 10 ...\n   .. ..- attr(*, \"names\")= chr [1:26] \"Strongly support KMT\" \"Somewhat support KMT\" \"Lean to KMT\" \"Somewhat lean to KMT\" ...\n $ PartyID        : dbl+lbl [1:1690] 9, 9, 1, 9, 9, 2, 9, 6, 9, 9, 2, 2, 9, 1, 1, 9, 9, 9,...\n   ..@ label       : chr \"Party Identification\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"KMT\" \"DPP\" \"NP\" \"PFP\" ...\n $ Tondu          : dbl+lbl [1:1690] 3, 5, 3, 5, 9, 4, 9, 6, 9, 9, 5, 5, 9, 5, 4, 9, 9, 4,...\n   ..@ label       : chr \"Position on unification and independence\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"Immediate unification\" \"Maintain the status quo,move toward unification\" \"Maintain the status quo, decide either unification or independence\" \"Maintain the status quo forever\" ...\n $ Tondu3         : dbl+lbl [1:1690] 2, 3, 2, 3, 9, 2, 9, 3, 9, 9, 3, 3, 9, 3, 2, 9, 9, 2,...\n   ..@ label       : chr \"3 categories of TONDU\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:4] 1 2 3 9\n   .. ..- attr(*, \"names\")= chr [1:4] \"Unification\" \"Maintain the status quo\" \"Independence\" \"Nonresponse\"\n $ nI2            : dbl+lbl [1:1690]  3, 98, 98,  3, 98, 98, 98,  3, 98,  1,  2, 98, 98,  ...\n   ..@ label       : chr \"Who is the current the premier of our country?\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 95 98\n   .. ..- attr(*, \"names\")= chr [1:5] \"Correct\" \"Incorrect\" \"I know but can't remember the name\" \"Refuse to answer\" ...\n $ votetsai       : num [1:1690] NA 1 0 NA NA 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ green          : num [1:1690] 0 0 0 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votetsai_nm    : num [1:1690] NA 1 0 NA NA 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votetsai_all   : num [1:1690] 0 1 0 0 0 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Independence   : num [1:1690] 0 1 0 1 0 0 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Unification    : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ sq             : num [1:1690] 1 0 1 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Taiwanese      : num [1:1690] 1 0 0 1 0 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ edu            : num [1:1690] 4 5 5 2 1 2 1 5 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ female         : num [1:1690] 1 1 0 0 1 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ whitecollar    : num [1:1690] 1 1 1 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ lowincome      : num [1:1690] 4 4 5 4 3 5 2 5 5 5 ...\n  ..- attr(*, \"label\")= chr \"How serious do you think low income of salaryman?\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income         : num [1:1690] 8 7 8 5 5.5 9 1 10 2 5.5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income_nm      : num [1:1690] 8 7 8 5 NA 9 1 10 2 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ age            : num [1:1690] 59 39 63 55 76 64 75 54 64 59 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ KMT            : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ DPP            : num [1:1690] 0 0 0 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ npp            : num [1:1690] 0 0 0 0 0 0 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ noparty        : num [1:1690] 1 1 0 1 1 0 1 0 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pfp            : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ South          : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ north          : num [1:1690] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Minnan_father  : num [1:1690] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Mainland_father: num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Econ_worse     : num [1:1690] 0 0 1 1 0 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Inequality     : num [1:1690] 1 1 1 1 0 1 0 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ inequality5    : num [1:1690] 4 5 5 5 3 5 3 5 5 5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ econworse5     : num [1:1690] 3 3 4 5 3 4 4 5 5 5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Govt_for_public: num [1:1690] 1 1 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pubwelf5       : num [1:1690] 5 5 4 1 3 2 2 1 3 2 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Govt_dont_care : num [1:1690] 0 0 1 1 0 1 1 1 0 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ highincome     : num [1:1690] 1 1 1 1 NA 1 0 1 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt        : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt_nm     : num [1:1690] NA 0 1 NA NA 0 0 0 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Blue           : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Green          : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ No_Party       : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ voteblue       : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ voteblue_nm    : num [1:1690] NA 0 1 NA NA 0 0 0 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votedpp_1      : num [1:1690] NA 1 0 NA NA 1 1 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt_1      : num [1:1690] NA 0 1 NA NA 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n\ndim(TEDS_2016)    # Get dimensions (rows, columns)\n\n[1] 1690   54\n\nnames(TEDS_2016)  # Column names\n\n [1] \"District\"        \"Sex\"             \"Age\"             \"Edu\"            \n [5] \"Arear\"           \"Career\"          \"Career8\"         \"Ethnic\"         \n [9] \"Party\"           \"PartyID\"         \"Tondu\"           \"Tondu3\"         \n[13] \"nI2\"             \"votetsai\"        \"green\"           \"votetsai_nm\"    \n[17] \"votetsai_all\"    \"Independence\"    \"Unification\"     \"sq\"             \n[21] \"Taiwanese\"       \"edu\"             \"female\"          \"whitecollar\"    \n[25] \"lowincome\"       \"income\"          \"income_nm\"       \"age\"            \n[29] \"KMT\"             \"DPP\"             \"npp\"             \"noparty\"        \n[33] \"pfp\"             \"South\"           \"north\"           \"Minnan_father\"  \n[37] \"Mainland_father\" \"Econ_worse\"      \"Inequality\"      \"inequality5\"    \n[41] \"econworse5\"      \"Govt_for_public\" \"pubwelf5\"        \"Govt_dont_care\" \n[45] \"highincome\"      \"votekmt\"         \"votekmt_nm\"      \"Blue\"           \n[49] \"Green\"           \"No_Party\"        \"voteblue\"        \"voteblue_nm\"    \n[53] \"votedpp_1\"       \"votekmt_1\"      \n\n# Are there any missing values?\n# Total number of NA in dataset\nsum(is.na(TEDS_2016))\n\n[1] 3008\n\ncolSums(is.na(TEDS_2016))\n\n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2        votetsai           green \n              0               0               0             429               0 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n            429             248               0               0               0 \n      Taiwanese             edu          female     whitecollar       lowincome \n              0              10               0               0               0 \n         income       income_nm             age             KMT             DPP \n              0             330               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n              0               0               0               0             330 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n              0             429               0               0               0 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n              0             429             187             187 \n\n\n\n\nWhat problems do you encounter when working with the dataset?\n\n\n\nFor this dataset, there are many ‘NA’ in the data. Primarily the NA are in the ‘vote…’ data.\nThere are also factor variables coded as numeric variables.\n\n\n# Total number of NA in dataset\nsum(is.na(TEDS_2016))\n\n[1] 3008\n\nsort(colSums(is.na(TEDS_2016)), decreasing = TRUE)\n\n       votetsai     votetsai_nm      votekmt_nm     voteblue_nm       income_nm \n            429             429             429             429             330 \n     highincome    votetsai_all       votedpp_1       votekmt_1             edu \n            330             248             187             187              10 \n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2           green    Independence \n              0               0               0               0               0 \n    Unification              sq       Taiwanese          female     whitecollar \n              0               0               0               0               0 \n      lowincome          income             age             KMT             DPP \n              0               0               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care         votekmt \n              0               0               0               0               0 \n           Blue           Green        No_Party        voteblue \n              0               0               0               0 \n\n# Visualize the missing data\nlibrary(ggplot2)\nlibrary(naniar)  # install.packages(\"naniar\")\ngg_miss_var(TEDS_2016) +  theme(axis.text.y = element_text(size = 6, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nHow to deal with missing values?\n\n\n\nHere are two methods for handling missing values:\n\nIgnore\n\nCompletely remove all data points with missing values. Not ideal with this data set.\nJust ignore the missing values in calculations.\n\nReplace Fill in the missing value with a substitute value, such as\n\nFill in with a median value.\nFill in an obvious, but fitting value.\n\n\nIn this case, probably best to ignore the NA in calculations. This is usually with a flag like rm.NA=TRUE or in glm() it is na.action=na.omit or na.action=na.exclude. According to The Companion to Applied Regression: “We encourage the use of na.exclude () for modeling and other functions that work with it.”\n\n\n\n(Next step) Explore the relationship between Tondu and other variables including female, DPP, age, income, edu, Taiwanese and Econ_worse. What methods would you use?\n\n\n\nTo see the relationship between the variables, I will:\n\nReview the summary statistics\nPlot a scatterplot matrix\nRun a generalized linear model on the data\nBox plots for categorical versus quantitative variables\nMosaic plots to compare categorical variables\n\nNone of the variables below have NA as is shown above.\nOf course, this is only the beginning, and not every variable has had a pairwise comparison.\n\n\nsummary(TEDS_2016[,c(\"Tondu\",\"female\",\"DPP\",\"age\",\"income\",\"edu\",\"Taiwanese\",\"Econ_worse\")])\n\n     Tondu           female            DPP              age        \n Min.   :1.000   Min.   :0.0000   Min.   :0.0000   Min.   : 20.00  \n 1st Qu.:3.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 35.00  \n Median :4.000   Median :0.0000   Median :0.0000   Median : 49.00  \n Mean   :4.127   Mean   :0.4864   Mean   :0.3497   Mean   : 49.11  \n 3rd Qu.:5.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 61.00  \n Max.   :9.000   Max.   :1.0000   Max.   :1.0000   Max.   :100.00  \n                                                                   \n     income            edu          Taiwanese        Econ_worse    \n Min.   : 1.000   Min.   :1.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 5.500   Median :3.000   Median :1.0000   Median :1.0000  \n Mean   : 5.324   Mean   :3.301   Mean   :0.6272   Mean   :0.5544  \n 3rd Qu.: 7.000   3rd Qu.:5.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :5.000   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :10                                       \n\n# Scatterplor Matrix\ncar::scatterplotMatrix(~Tondu + female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016)\n\n\n\n\n\n\n\ntondu_glm &lt;- glm(Tondu ~ female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016)\n\n# Summary of GLM\nsummary(tondu_glm)\n\n\nCall:\nglm(formula = Tondu ~ female + DPP + age + income + edu + Taiwanese + \n    Econ_worse, data = TEDS_2016)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.204138   0.274188  15.333  &lt; 2e-16 ***\nfemale       0.368598   0.082680   4.458 8.82e-06 ***\nDPP          0.121223   0.091598   1.323    0.186    \nage          0.001668   0.003143   0.531    0.596    \nincome      -0.025558   0.015862  -1.611    0.107    \nedu         -0.181096   0.036824  -4.918 9.61e-07 ***\nTaiwanese    0.768943   0.091437   8.410  &lt; 2e-16 ***\nEcon_worse  -0.248545   0.083640  -2.972    0.003 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.780522)\n\n    Null deviance: 5208.2  on 1679  degrees of freedom\nResidual deviance: 4649.0  on 1672  degrees of freedom\n  (10 observations deleted due to missingness)\nAIC: 6495.6\n\nNumber of Fisher Scoring iterations: 2\n\n# Relationship between 'Tondu' and 'DPP'\n# Example of Comparison of categorical variables\n\ntable(TEDS_2016$Tondu, TEDS_2016$DPP)\n\n   \n      0   1\n  1  26   1\n  2 147  33\n  3 378 168\n  4 256  72\n  5 144 236\n  6  38  70\n  9 110  11\n\nchisq.test(table(TEDS_2016$Tondu, TEDS_2016$DPP))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(TEDS_2016$Tondu, TEDS_2016$DPP)\nX-squared = 263.17, df = 6, p-value &lt; 2.2e-16\n\n# Boxplot for quantitative vs categorical data example\nboxplot(income ~ Tondu, data = TEDS_2016,\n        main = \"Income by Tondu Preference\",\n        xlab = \"Tondu\",\n        ylab = \"Income\",\n        col = viridis::viridis(length(unique(TEDS_2016$Tondu))),\n        las = 2)  # Rotate labels for readability\n\n\n\n\n\n\n\n# Another Boxplot for quantitative vs categorical data example\nboxplot(age ~ Tondu, data = TEDS_2016,\n        main = \"Age by Tondu Preference\",\n        xlab = \"Tondu\",\n        ylab = \"Age\",\n        col = viridis::viridis(length(unique(TEDS_2016$Tondu))),\n        las = 2)  # Rotate labels for readability\n\n\n\n\n\n\n\n# A mosaic plot to compare categorical variables\nmosaic(~ Tondu + DPP, data = TEDS_2016, shade = TRUE)\n\n\n\n\n\n\n\n\n\n\n(Next step) How about the votetsai variable (vote for DPP candidate Tsai Ing-wen)?\n\n\n\nTo see the relationship between the variables, I will perform the same analysis as above, but control for all the NA:\n\nReview the summary statistics\nPlot a scatterplot matrix\nRun a generalized linear model on the data\nBox plots for categorical versus quantitative variables\nMosaic plots to compare categorical variables\n\nOf course, this is only the beginning, and not every variable has had a pairwise comparison.\n\n\nsummary(TEDS_2016[,c(\"votetsai\",\"female\",\"DPP\",\"age\",\"income\",\"edu\",\"Taiwanese\",\"Econ_worse\")])\n\n    votetsai          female            DPP              age        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   : 20.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 35.00  \n Median :1.0000   Median :0.0000   Median :0.0000   Median : 49.00  \n Mean   :0.6265   Mean   :0.4864   Mean   :0.3497   Mean   : 49.11  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 61.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :100.00  \n NA's   :429                                                        \n     income            edu          Taiwanese        Econ_worse    \n Min.   : 1.000   Min.   :1.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 5.500   Median :3.000   Median :1.0000   Median :1.0000  \n Mean   : 5.324   Mean   :3.301   Mean   :0.6272   Mean   :0.5544  \n 3rd Qu.: 7.000   3rd Qu.:5.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :5.000   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :10                                       \n\ncar::scatterplotMatrix(~votetsai + female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016)\n\n\n\n\n\n\n\nvotetsai_glm &lt;- glm(votetsai ~ female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016, na.action=na.exclude)\n\nsummary(votetsai_glm)\n\n\nCall:\nglm(formula = votetsai ~ female + DPP + age + income + edu + \n    Taiwanese + Econ_worse, data = TEDS_2016, na.action = na.exclude)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.4408344  0.0727487   6.060  1.8e-09 ***\nfemale      -0.0256167  0.0209882  -1.221  0.22250    \nDPP          0.4798515  0.0228743  20.978  &lt; 2e-16 ***\nage         -0.0021676  0.0008307  -2.609  0.00918 ** \nincome      -0.0035722  0.0039547  -0.903  0.36655    \nedu         -0.0232000  0.0095903  -2.419  0.01570 *  \nTaiwanese    0.2531388  0.0239856  10.554  &lt; 2e-16 ***\nEcon_worse   0.0636800  0.0214794   2.965  0.00309 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.134158)\n\n    Null deviance: 294.26  on 1256  degrees of freedom\nResidual deviance: 167.56  on 1249  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 1052.2\n\nNumber of Fisher Scoring iterations: 2\n\ntable(TEDS_2016$votetsai, TEDS_2016$DPP)\n\n   \n      0   1\n  0 454  17\n  1 276 514\n\nchisq.test(table(TEDS_2016$votetsai, TEDS_2016$DPP))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(TEDS_2016$votetsai, TEDS_2016$DPP)\nX-squared = 454.62, df = 1, p-value &lt; 2.2e-16\n\n# Boxplot for quantitative vs categorical data example\nboxplot(income ~ votetsai, data = TEDS_2016,\n        main = \"Income by votetsai Preference\",\n        xlab = \"votetsai\",\n        ylab = \"Income\",\n        col = viridis::viridis(length(unique(TEDS_2016$votetsai))),\n        las = 2)  # Rotate labels for readability\n\n\n\n\n\n\n\n# Another Boxplot for quantitative vs categorical data example\nboxplot(age ~ votetsai, data = TEDS_2016,\n        main = \"Age by votetsai Preference\",\n        xlab = \"votetsai\",\n        ylab = \"Age\",\n        col = viridis::viridis(length(unique(TEDS_2016$votetsai))),\n        las = 2)  # Rotate labels for readability\n\n\n\n\n\n\n\n# A mosaic plot to compare categorical variables\nmosaic(~ votetsai + DPP, data = TEDS_2016, shade = TRUE)\n\n\n\n\n\n\n\n\n\n\n(Next step) Generate frequency table and barchart of the Tondu variable. Assign labels to the variable using the following:\n\n\nTEDS_2016$Tondu&lt;-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”,\n“Status quo, unif. in future”, “Status quo, decide later\", \"Status quo\nforever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\nNote that the dataset seems to have changed so that adding the labels is no longer necessary. The labels could be added with:\nTEDS_2016$Tondu &lt;- labelled(TEDS_2016$Tondu, \n                            c(\"Unification now\" = 1, \n                              \"Status quo, unif. in future\" = 2, \n                              \"Status quo, decide later\" = 3, \n                              \"Status quo forever\" = 4, \n                              \"Status quo, indep. in future\" = 5, \n                              \"Independence now\" = 6, \n                              \"No response\" = 9))\n\n\n\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6   9 \n 27 180 546 328 380 108 121 \n\n\n\n\nFrequency Table\n\n\n#head(TEDS_2016$Tondu)\n#tail(TEDS_2016$Tondu)\n\nTEDS_2016 %&gt;%\n  count(Tondu) %&gt;%\n  mutate(Percent = n / sum(n) * 100)\n\n# A tibble: 7 × 3\n  Tondu                                                                n Percent\n  &lt;dbl+lbl&gt;                                                        &lt;int&gt;   &lt;dbl&gt;\n1 1 [Immediate unification]                                           27    1.60\n2 2 [Maintain the status quo,move toward unification]                180   10.7 \n3 3 [Maintain the status quo, decide either unification or indepe…   546   32.3 \n4 4 [Maintain the status quo forever]                                328   19.4 \n5 5 [Maintain the status quo,move toward independence]               380   22.5 \n6 6 [Immediate independence]                                         108    6.39\n7 9 [Nonresponse]                                                    121    7.16\n\n\n\n# Compute frequencies\n\nTondu_freq &lt;- TEDS_2016 %&gt;%\n  count(Tondu) %&gt;%\n  mutate(Percent = n / sum(n) * 100)\n\n\n# Convert Tondu to factor (preserve labels)\nTEDS_2016$Tonduf &lt;- as_factor(TEDS_2016$Tondu)\n\n# Compute frequencies\nTondu_freq &lt;- table(TEDS_2016$Tonduf)  # Count occurrences\nPercent &lt;- prop.table(Tondu_freq) * 100  # Convert to percentages\nQuantity &lt;- as.vector(Tondu_freq)  # Extract counts as a numeric vector\n\n# Percentage bar chart\nbarplot(Percent,\n        names.arg = names(Tondu_freq),  # Use category labels\n        main = \"Distribution of Tondu Preferences Percentages\",\n        ylab = \"Percentage\",\n        col = viridis::viridis(length(Percent)),  # Assign colors\n        las = 2,  # Rotate x-axis labels for readability\n        cex.names = 0.5)  # Adjust text \n\n\n\n\n\n\n\n# Count bar chart\nbarplot(Quantity,\n        names.arg = names(Tondu_freq),  # Use category labels\n        main = \"Distribution of Tondu Preferences Count\",\n        ylab = \"Count\",\n        col = viridis::viridis(length(Percent)),  # Assign colors\n        las = 2,  # Rotate x-axis labels for readability\n        cex.names = 0.5)  # Adjust text"
  },
  {
    "objectID": "assignment01.html",
    "href": "assignment01.html",
    "title": "Assignment 01",
    "section": "",
    "text": "Review/refresh R programming:"
  },
  {
    "objectID": "assignment01.html#exercise-1",
    "href": "assignment01.html#exercise-1",
    "title": "Assignment 01",
    "section": "Exercise 1",
    "text": "Exercise 1\nAfter installation, type R.version. What version of R did you install? What is the nickname of that particular software build?\n\nR.version\n\n               _                           \nplatform       aarch64-apple-darwin20      \narch           aarch64                     \nos             darwin20                    \nsystem         aarch64, darwin20           \nstatus                                     \nmajor          4                           \nminor          4.3                         \nyear           2025                        \nmonth          02                          \nday            28                          \nsvn rev        87843                       \nlanguage       R                           \nversion.string R version 4.4.3 (2025-02-28)\nnickname       Trophy Case"
  },
  {
    "objectID": "assignment01.html#exercise-2",
    "href": "assignment01.html#exercise-2",
    "title": "Assignment 01",
    "section": "Exercise 2",
    "text": "Exercise 2\nOpen RStudio. In the console pane (usually at the bottom-left), type 1 + 1 and press Enter. What result do you get?\n\n1+1\n\n[1] 2"
  },
  {
    "objectID": "assignment01.html#exercise-3",
    "href": "assignment01.html#exercise-3",
    "title": "Assignment 01",
    "section": "Exercise 3",
    "text": "Exercise 3\nCreate a new R script (File &gt; New File &gt; R Script). Type print(“Hello, Data Science!”) and run the code. What output do you see in the console?\n\nprint(\"Hello, Data Science!\")\n\n[1] \"Hello, Data Science!\""
  },
  {
    "objectID": "assignment01.html#exercise-4",
    "href": "assignment01.html#exercise-4",
    "title": "Assignment 01",
    "section": "Exercise 4",
    "text": "Exercise 4\nUse pacman to install and load the tidyr package. Then, use p_functions() to list all functions in the tidyr package.\n\nlibrary(pacman)\np_functions(tidyr)\n\n [1] \"%&gt;%\"                      \"all_of\"                  \n [3] \"any_of\"                   \"as_tibble\"               \n [5] \"build_longer_spec\"        \"build_wider_spec\"        \n [7] \"check_pivot_spec\"         \"chop\"                    \n [9] \"complete\"                 \"complete_\"               \n[11] \"contains\"                 \"crossing\"                \n[13] \"crossing_\"                \"drop_na\"                 \n[15] \"drop_na_\"                 \"ends_with\"               \n[17] \"everything\"               \"expand\"                  \n[19] \"expand_\"                  \"expand_grid\"             \n[21] \"extract\"                  \"extract_\"                \n[23] \"extract_numeric\"          \"fill\"                    \n[25] \"fill_\"                    \"full_seq\"                \n[27] \"gather\"                   \"gather_\"                 \n[29] \"hoist\"                    \"last_col\"                \n[31] \"matches\"                  \"nest\"                    \n[33] \"nest_\"                    \"nest_legacy\"             \n[35] \"nesting\"                  \"nesting_\"                \n[37] \"num_range\"                \"one_of\"                  \n[39] \"pack\"                     \"pivot_longer\"            \n[41] \"pivot_longer_spec\"        \"pivot_wider\"             \n[43] \"pivot_wider_spec\"         \"replace_na\"              \n[45] \"separate\"                 \"separate_\"               \n[47] \"separate_longer_delim\"    \"separate_longer_position\"\n[49] \"separate_rows\"            \"separate_rows_\"          \n[51] \"separate_wider_delim\"     \"separate_wider_position\" \n[53] \"separate_wider_regex\"     \"spread\"                  \n[55] \"spread_\"                  \"starts_with\"             \n[57] \"tibble\"                   \"tidyr_legacy\"            \n[59] \"tribble\"                  \"unchop\"                  \n[61] \"uncount\"                  \"unite\"                   \n[63] \"unite_\"                   \"unnest\"                  \n[65] \"unnest_\"                  \"unnest_auto\"             \n[67] \"unnest_legacy\"            \"unnest_longer\"           \n[69] \"unnest_wider\"             \"unpack\""
  },
  {
    "objectID": "assignment01.html#exercise-5",
    "href": "assignment01.html#exercise-5",
    "title": "Assignment 01",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate a new folder on your computer called “DataScience”. Set this as your working directory in RStudio. Then, use getwd() to confirm it’s set correctly.\nNote: This was done, but running this in Quarto is problematic, so this output is copied from the Console.\n&gt; getwd()\n[1] \"/Users/john/DataScience\""
  },
  {
    "objectID": "assignment01.html#exercise-1-1",
    "href": "assignment01.html#exercise-1-1",
    "title": "Assignment 01",
    "section": "Exercise 1",
    "text": "Exercise 1\nCreate two variables c and d with values of your choice. Perform all the above operations on these variables and print the results.\n\nc &lt;- 11\nd &lt;- 13\n\nsum &lt;- c + d\ndifference &lt;- c - d\nproduct &lt;- c * d\nquotient &lt;- c / d\npower &lt;- c ^ d\nmodulus &lt;- c %% d\n\nprint(sum)\n\n[1] 24\n\nprint(difference)\n\n[1] -2\n\nprint(product)\n\n[1] 143\n\nprint(quotient)\n\n[1] 0.8461538\n\nprint(power)\n\n[1] 3.452271e+13\n\nprint(modulus)\n\n[1] 11"
  },
  {
    "objectID": "assignment01.html#exercise-2-1",
    "href": "assignment01.html#exercise-2-1",
    "title": "Assignment 01",
    "section": "Exercise 2",
    "text": "Exercise 2\nCreate variables of each data type we’ve discussed so far (numeric, character, logical). Use the class() function to verify their types.\n\nnumeric_vector &lt;- rnorm(5)\ncharacter_vector &lt;- c(\"carbon\", \"hydrogen\", \"oxygen\")\nlogical_vector &lt;- c(FALSE, FALSE, TRUE, TRUE)\n\nprint(numeric_vector)\n\n[1]  0.1456882 -0.8581445  0.9376645  0.6521906 -0.9676844\n\nprint(character_vector)\n\n[1] \"carbon\"   \"hydrogen\" \"oxygen\"  \n\nprint(logical_vector)\n\n[1] FALSE FALSE  TRUE  TRUE\n\ncolors &lt;- factor(c(\"red\", \"cornflowerblue\", \"green\", \"red3\", \"pink\"))\nprint(colors)\n\n[1] red            cornflowerblue green          red3           pink          \nLevels: cornflowerblue green pink red red3\n\nlevels(colors)\n\n[1] \"cornflowerblue\" \"green\"          \"pink\"           \"red\"           \n[5] \"red3\"          \n\nclass(numeric_vector)\n\n[1] \"numeric\"\n\nclass(character_vector)\n\n[1] \"character\"\n\nclass(logical_vector)\n\n[1] \"logical\"\n\nclass(colors)\n\n[1] \"factor\""
  },
  {
    "objectID": "assignment01.html#exercise-3-1",
    "href": "assignment01.html#exercise-3-1",
    "title": "Assignment 01",
    "section": "Exercise 3",
    "text": "Exercise 3\nThere is no Exercise 3."
  },
  {
    "objectID": "assignment01.html#exercise-4-1",
    "href": "assignment01.html#exercise-4-1",
    "title": "Assignment 01",
    "section": "Exercise 4",
    "text": "Exercise 4\nCreate a vector of numbers from 1 to 10. Then, use indexing to:\n\nExtract the 5th element\nExtract all elements except the 3rd\nExtract the 2nd, 4th, and 6th elements\n\n\n# Create a vector of numbers from 1 to 10\nnumbers &lt;- 1:10\n\n# Extract the 5th element\nfifth &lt;- numbers[5]\n\n# Extract all elements except the 3rd\nnot_third &lt;- numbers[-3]\n\n# Extract the 2nd, 4th, and 6th elements\nevens &lt;- numbers[c(2, 4, 6)]\n\n# Print results\nprint(numbers)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nprint(fifth)\n\n[1] 5\n\nprint(not_third)\n\n[1]  1  2  4  5  6  7  8  9 10\n\nprint(evens)\n\n[1] 2 4 6"
  },
  {
    "objectID": "assignment01.html#exercise-5-1",
    "href": "assignment01.html#exercise-5-1",
    "title": "Assignment 01",
    "section": "Exercise 5",
    "text": "Exercise 5\nWrite a conditional statement that checks if a number is positive, negative, or zero, and prints an appropriate message for each case.\n\n# Define a number\nnum &lt;- sample(-10:10, 1)  # generate number\nprint(num)\n\n[1] 6\n\n# Conditional statement to check if the number is positive, negative, or zero\nif (num &gt; 0) {\n  print(\"The number is positive.\")\n} else if (num &lt; 0) {\n  print(\"The number is negative.\")\n} else {\n  print(\"The number is zero.\")\n}\n\n[1] \"The number is positive.\""
  },
  {
    "objectID": "assignment01.html#exercise-6",
    "href": "assignment01.html#exercise-6",
    "title": "Assignment 01",
    "section": "Exercise 6",
    "text": "Exercise 6\nCreate a vector of 5 numbers and a vector of 5 names. Combine them into a data frame where each number corresponds to an age and each name corresponds to a person. Then, calculate the mean age and display a summary of the data frame.\n\n# Create a vector of 5 ages\nages &lt;- c(25, 30, 11, 28, 55)\n\n# Create a vector of 5 names\nnames &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\")\n\n# Combine into a data frame\npeople_df &lt;- data.frame(Name = names, Age = ages)\n\n# Calculate the mean age\nmean_age &lt;- mean(people_df$Age)\n\n# Display the summary of the data frame\nsummary(people_df)\n\n     Name                Age      \n Length:5           Min.   :11.0  \n Class :character   1st Qu.:25.0  \n Mode  :character   Median :28.0  \n                    Mean   :29.8  \n                    3rd Qu.:30.0  \n                    Max.   :55.0  \n\n# Print the mean age\nprint(paste(\"The mean age is:\", mean_age))\n\n[1] \"The mean age is: 29.8\""
  },
  {
    "objectID": "assignment01.html#exercise-6---advanced",
    "href": "assignment01.html#exercise-6---advanced",
    "title": "Assignment 01",
    "section": "Exercise 6 - Advanced",
    "text": "Exercise 6 - Advanced\nCreate a custom function that takes a vector of numbers as input and returns a list containing the following: 1. The square of each number in the vector. 2. A count of how many numbers in the vector are greater than a specified threshold. 3. The mean of the numbers in the vector, but only include numbers greater than a specified threshold in the calculation.\nTest your function with a vector of random numbers, using a threshold of your choice.\n\n# Define the custom function\nvector_out &lt;- function(numbers, target) {\n  squared_num &lt;- numbers^2\n  target_count &lt;- sum(numbers &gt; target)\n  target_mean &lt;- mean(numbers[numbers &gt; target])\n  \n  # Return a list with results\n  return(list(\n    Squared_Numbers = squared_num,\n    Count_Above_Target = target_count,\n    Mean_Above_Target = target_mean\n  ))\n}\n\n# Generate a vector of 10 random numbers between 1 and 50\nset.seed(111)  # Set seed for reproducibility\nrandom_numbers &lt;- sample(1:100, 10, replace = TRUE)\nprint(random_numbers)\n\n [1] 78 84 83 47 25 59 69 35 72 26\n\n# Define a threshold value\ntarget_value &lt;- median(random_numbers)\n\n# Call the function with the generated vector and threshold\nresult &lt;- vector_out(random_numbers, target_value)\n\n# Print the results\nprint(result)\n\n$Squared_Numbers\n [1] 6084 7056 6889 2209  625 3481 4761 1225 5184  676\n\n$Count_Above_Target\n[1] 5\n\n$Mean_Above_Target\n[1] 77.2"
  },
  {
    "objectID": "assignment01.html#part-1",
    "href": "assignment01.html#part-1",
    "title": "Assignment 01",
    "section": "Part 1",
    "text": "Part 1\n\nExercise 1\nVector Manipulation\n\nCreate a vector of 5 superhero ages.\nIncrease all ages by 2 years.\nFind which heroes are older than 40 after the increase.\nCreate a logical vector indicating if each hero is from DC (assume the first 3 are from DC).\n\n\n# Ages\nsuper_ages &lt;- c(40, 21, 132, 55, 33)\nplus_two &lt;- super_ages + 2\nprint(plus_two)\n\n[1]  42  23 134  57  35\n\n# Over 40\nis_old &lt;- plus_two &gt; 40\nprint(is_old)\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE\n\n# Publisher\n\npub_vector &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE)\nprint(pub_vector)\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\nclass(pub_vector)\n\n[1] \"logical\"\n\n\n\n\nExercise 2\nMatrix Manipulation\n\nCreate a 3x3 matrix of hero power levels (strength, speed, intelligence) for three new heroes.\nCalculate the average power level for each hero.\nFind which hero has the highest strength.\nScale all power levels by 1.5 and round to the nearest integer.\n\n\nsuper_df &lt;- data.frame(\n  Strength = c(110, 20, 180),\n  Speed = c(25, 95, 130),\n  Intelligence = c(19, 150, 91)\n)\n\nprint(super_df)\n\n  Strength Speed Intelligence\n1      110    25           19\n2       20    95          150\n3      180   130           91\n\n# Average for individuals\nprint(\"The average for individual superheros are:\")\n\n[1] \"The average for individual superheros are:\"\n\nprint(rowMeans(super_df[, 1:3]))\n\n[1]  51.33333  88.33333 133.66667\n\n# Average for type of power\nprint(\"The average for the categories are:\")\n\n[1] \"The average for the categories are:\"\n\nprint(paste(\"Strength:  \", mean(super_df$Strength))) \n\n[1] \"Strength:   103.333333333333\"\n\nprint(paste(\"Speed:  \",mean(super_df$Speed)))\n\n[1] \"Speed:   83.3333333333333\"\n\nprint(paste(\"Intelligence:  \",mean(super_df$Intelligence)))\n\n[1] \"Intelligence:   86.6666666666667\"\n\n# High Strength\nsuper_strength &lt;- super_df[which.max(super_df$Strength), ]\nprint(super_strength)\n\n  Strength Speed Intelligence\n3      180   130           91\n\n# Scale and round\nsuper_round &lt;- round(1.5*super_df)\nprint(super_round)\n\n  Strength Speed Intelligence\n1      165    38           28\n2       30   142          225\n3      270   195          136\n\n\n\n\nExercise 3\nData Frame Manipulation\n\nAdd a “PowerLevel” column that’s the average of Strength, Intelligence, and Speed.\nFilter the data frame to show only heroes with a PowerLevel above 85.\nSort the heroes by PowerLevel in descending order.\nCreate a new data frame with only the Name and PowerLevel columns for non-Marvel heroes.\n\n\n# Add a column for average power\nsuper_df$PowerLevel &lt;- rowMeans(super_df[, 1:3])\nprint(super_df)\n\n  Strength Speed Intelligence PowerLevel\n1      110    25           19   51.33333\n2       20    95          150   88.33333\n3      180   130           91  133.66667\n\n# Filter data for PowerLevel &gt; 85\ntemp_df &lt;- super_df[super_df$PowerLevel &gt; 85,]\nprint(temp_df)\n\n  Strength Speed Intelligence PowerLevel\n2       20    95          150   88.33333\n3      180   130           91  133.66667\n\n# Sort by PowerLevel (descending)\ntemp_df &lt;- super_df[order(-super_df$PowerLevel), ]\nprint(temp_df)\n\n  Strength Speed Intelligence PowerLevel\n3      180   130           91  133.66667\n2       20    95          150   88.33333\n1      110    25           19   51.33333\n\n# Add character type to df\npublisher &lt;- c(\"DC\", \"Dark Horse\", \"Marvel\")\nsuper_df$Publisher &lt;- publisher\nprint (super_df)\n\n  Strength Speed Intelligence PowerLevel  Publisher\n1      110    25           19   51.33333         DC\n2       20    95          150   88.33333 Dark Horse\n3      180   130           91  133.66667     Marvel\n\n# No Marvel df\ntemp_df &lt;- subset(super_df, Publisher != \"Marvel\")\nsuper_df_nomarvel &lt;- data.frame(Name = c(\"Batman\", \"Hellboy\"), \n                                PowerLevel = temp_df$PowerLevel)\nprint(super_df_nomarvel)\n\n     Name PowerLevel\n1  Batman   51.33333\n2 Hellboy   88.33333"
  },
  {
    "objectID": "assignment01.html#part-2",
    "href": "assignment01.html#part-2",
    "title": "Assignment 01",
    "section": "Part 2",
    "text": "Part 2\n\nExercise 1\n\nCreate a numeric vector containing the numbers 10, 20, 30, 40, and 50.\nCreate a character vector containing the names “Alice”, “Bob”, “Charlie”, “David”, and “Eve”.\nUse indexing to retrieve the third element from each of these vectors.\nModify the second element in the numeric vector to be 25.\nCalculate the sum of all elements in the numeric vector.\n\n\n# Numeric vector\nnum_vector &lt;- c(10, 20, 30, 40, 50)\n\n# Character vector\nchar_vector &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\")\n\n# Indexing\nnum_vector[3]\n\n[1] 30\n\nchar_vector[3]\n\n[1] \"Charlie\"\n\n# Second Element\nnum_vector[2] &lt;- 25\nprint(num_vector)\n\n[1] 10 25 30 40 50\n\n# Sum\nsum(num_vector)\n\n[1] 155\n\n\n\n\nExercise 2\nWorking with Factors\nObjective: Understand how to create and manipulate factors in R.\nInstructions:\n\nCreate a factor variable from the following vector: c(“low”, “medium”, “high”, “low”, “medium”, “high”).\nDisplay the levels of the factor variable.\nConvert the factor levels to an ordered factor where “low” &lt; “medium” &lt; “high”.\nCreate a bar plot to visualize the frequency of each level.\n\n\n# Create factor variable\nfactor_var &lt;- factor(c(\"low\", \"medium\", \"high\", \"low\", \"medium\", \"high\"))\n\nlevels(factor_var)\n\n[1] \"high\"   \"low\"    \"medium\"\n\n# Ordered factor\nord_factor &lt;- factor(factor_var, \n                     levels = c(\"low\", \"medium\", \"high\"), \n                     ordered = TRUE)\n\n# Create a bar plot\nbarplot(table(ord_factor))\n\n\n\n\n\n\n\n\n\n\nExercise 3\nData Frame Operations Objective: Learn how to create, access, and manipulate data frames.\nInstructions:\n\nCreate a data frame with the following columns: ID (1, 2, 3), Name (“Alice”, “Bob”, “Charlie”), and Score (85, 90, 88).\nAccess the Name column and print it.\nAdd a new column Pass that indicates whether the Score is greater than or equal to 90.\nCalculate the average Score for all students.\n\n\n# Create a data frame\ndf &lt;- data.frame(\n  ID = 1:3,\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Score = c(85, 90, 88)\n)\n\n# Access the Name column\ndf$Name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n# New column\ndf$Pass &lt;- df$Score &gt;= 90\nprint(df)\n\n  ID    Name Score  Pass\n1  1   Alice    85 FALSE\n2  2     Bob    90  TRUE\n3  3 Charlie    88 FALSE\n\n# Calculate the average score\nmean(df$Score)\n\n[1] 87.66667\n\n\n\n\nExercise 4\nText Data Manipulation with stringr Objective: Practice manipulating text data using the stringr package.\nInstructions: 1. Load the stringr package. 2. Create a character string: “The quick brown fox jumps over the lazy dog”. 3. Count the number of words in the string. 4. Extract the word “quick” from the string. 5. Replace the word “lazy” with “energetic”.\n\n# Load the stringr package\nlibrary(stringr)\n\n# Create a string\ntext_string &lt;- \"The quick brown fox jumps over the lazy dog\"\n\n# Count words\nstr_count(text_string, \"\\\\w+\")\n\n[1] 9\n\n# Extract \"quick\"\nstr_extract(text_string, \"quick\")\n\n[1] \"quick\"\n\n# Replace \"lazy\" with \"crazy\"\nstr_replace(text_string, \"lazy\", \"energetic\")\n\n[1] \"The quick brown fox jumps over the energetic dog\"\n\n# Try the following and count again?\ntext_string &lt;- \"The quick brown fox jumps over the lazy dog #\"\nstr_count(text_string, \"\\\\w+\")\n\n[1] 9\n\n\n\n\nExercise 5\nCreating and Analyzing a Document-Term Matrix (DTM) Objective: Learn how to create and analyze a Document-Term Matrix using text data.\nInstructions:\n\nLoad the tm package.\nCreate a small corpus using the following text documents:\n\n“R is a programming language for data analysis.”\n“Data analysis in R is powerful and flexible.”\n“Learning R can be fun and rewarding.”\n\nCreate a Document-Term Matrix (DTM) from the corpus.\nInspect the DTM to see the term frequency matrix.\nIdentify the term with the highest frequency across all documents.\n\n\n# Load tm package\nlibrary(tm)\n\nLoading required package: NLP\n\n# Create a corpus\ndocuments &lt;- c(\"R is a programming language for data analysis.\",\n          \"Data analysis in R is powerful and flexible.\",\n          \"Learning R can be fun and rewarding.\")\ncorpus &lt;- Corpus(VectorSource(documents))\n\n# Create a Document-Term Matrix\ndtm &lt;- DocumentTermMatrix(corpus)\n\n# Inspect the DTM\ninspect(dtm)\n\n&lt;&lt;DocumentTermMatrix (documents: 3, terms: 13)&gt;&gt;\nNon-/sparse entries: 15/24\nSparsity           : 62%\nMaximal term length: 11\nWeighting          : term frequency (tf)\nSample             :\n    Terms\nDocs analysis analysis. and can data flexible. for language powerful\n   1        0         1   0   0    1         0   1        1        0\n   2        1         0   1   0    1         1   0        0        1\n   3        0         0   1   1    0         0   0        0        0\n    Terms\nDocs programming\n   1           1\n   2           0\n   3           0\n\n# Find highest frequency\nterm_frequencies &lt;- colSums(as.matrix(dtm))\nmost_frequent_term &lt;- names(term_frequencies[which.max(term_frequencies)])\nmost_frequent_term\n\n[1] \"data\""
  },
  {
    "objectID": "assignment01.html#basic-statistical-functions---q1",
    "href": "assignment01.html#basic-statistical-functions---q1",
    "title": "Assignment 01",
    "section": "2.1 Basic Statistical Functions - Q1",
    "text": "2.1 Basic Statistical Functions - Q1\n\nCreate a vector data with the following values: c(12, 17, 14, 22, 15, 19, 16).\nCalculate the mean, median, standard deviation, and variance of data.\nDetermine the minimum, maximum, and range of the data.\nGenerate a summary of data.\n\n\n# Create vector\ndata &lt;- c(12, 17, 14, 22, 15, 19, 16)\n\n# Statistics\nmean_data &lt;- mean(data)\nmedian_data &lt;- median(data)\nsd_data &lt;- sd(data)\nvar_data &lt;- var(data)\n\nstat_data &lt;- c(Mean = mean_data, \n               Median = median_data, \n               SD = sd_data, \n               Var = var_data)\n\nprint(stat_data)\n\n     Mean    Median        SD       Var \n16.428571 16.000000  3.309438 10.952381 \n\n# Min, Max, Range\nmin_data &lt;- min(data)\nmax_data &lt;- max(data)\nrange_data &lt;- range(data)\n\nrange_data &lt;- c(Min = min_data, \n               Max = max_data, \n               Range = range_data \n               )\nprint(range_data)\n\n   Min    Max Range1 Range2 \n    12     22     12     22 \n\n# Summary statistics\nsummary_data &lt;- summary(data)\nprint(summary_data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   14.50   16.00   16.43   18.00   22.00"
  },
  {
    "objectID": "assignment01.html#matrix-operations---q2",
    "href": "assignment01.html#matrix-operations---q2",
    "title": "Assignment 01",
    "section": "2.2 Matrix Operations - Q2",
    "text": "2.2 Matrix Operations - Q2\n\nCreate a 2x3 matrix matrix_A with the values c(1, 2, 3, 4, 5, 6).\nCreate another 2x3 matrix matrix_B with the values c(6, 5, 4, 3, 2, 1).\nPerform element-wise addition and subtraction on matrix_A and matrix_B.\nTranspose matrix_A to create matrix_A_T.\nMultiply matrix_A_T by matrix_B (after ensuring their dimensions match).\n\n\n# Creating matrices\nmatrix_A &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)\nprint(matrix_A)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nmatrix_B &lt;- matrix(c(6, 5, 4, 3, 2, 1), nrow = 2, ncol = 3)\nprint(matrix_B)\n\n     [,1] [,2] [,3]\n[1,]    6    4    2\n[2,]    5    3    1\n\n# Operations\nmatrix_add &lt;- matrix_A + matrix_B\nprint(matrix_add)\n\n     [,1] [,2] [,3]\n[1,]    7    7    7\n[2,]    7    7    7\n\nmatrix_sub &lt;- matrix_A - matrix_B\nprint(matrix_sub)\n\n     [,1] [,2] [,3]\n[1,]   -5   -1    3\n[2,]   -3    1    5\n\n# Transpose\nmatrix_A_T &lt;- t(matrix_A)\nprint(matrix_A_T)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n# Multiplying AT X BT\nmatrix_mul &lt;- matrix_A_T %*% matrix_B\nprint(matrix_mul)\n\n     [,1] [,2] [,3]\n[1,]   16   10    4\n[2,]   38   24   10\n[3,]   60   38   16"
  },
  {
    "objectID": "assignment01.html#creating-functions---q3",
    "href": "assignment01.html#creating-functions---q3",
    "title": "Assignment 01",
    "section": "2.3 Creating Functions - Q3",
    "text": "2.3 Creating Functions - Q3\n\nWrite a function cube() that calculates the cube of a number.\nCreate a vector numbers with the values c(2, 3, 4), and use the cube() function to find the cube of each element in numbers.\nWrite a function is_positive() that checks if a number is positive, negative, or zero, and returns a corresponding message.\nTest the is_positive() function with the numbers 5, -3, and 0.\n\n\n# Cube Function\ncube &lt;- function(x) {\n  return(x^3)\n}\n\n# Cube a Vector\nnumbers &lt;- c(2, 3, 4)\ncubes &lt;- sapply(numbers, cube)\nprint(cubes)\n\n[1]  8 27 64\n\n# Sign Test\nis_positive &lt;- function(num) {\n  if (num &gt; 0) {\n    return(\"Positive\")\n  } else if (num &lt; 0) {\n    return(\"Negative\")\n  } else {\n    return(\"Zero\")\n  }\n}\n\n# Test is_positive function\nresult_1 &lt;- is_positive(5)  # Should return \"Positive\"\nprint(result_1)\n\n[1] \"Positive\"\n\nresult_2 &lt;- is_positive(-3) # Should return \"Negative\"\nprint(result_2)\n\n[1] \"Negative\"\n\nresult_3 &lt;- is_positive(0)  # Should return \"Zero\"\nprint(result_3)\n\n[1] \"Zero\""
  },
  {
    "objectID": "assignment01.html#q1",
    "href": "assignment01.html#q1",
    "title": "Assignment 01",
    "section": "Q1",
    "text": "Q1\n\n1. Import the Air_Quality data and view the column names.\n\n# 1.1.1 Direct import If the data is in the same folder as the current working directory\n\nair_data &lt;- read.csv(\"Air_Quality.csv\")\n\n# 1.1.2 Setting working directory before direct import\n\n#setwd('D:/Summer Coding Camp/Jeong_CodingCamp_draft_code/Jeong_CodingCamp_draft_code')\n#air_data &lt;- read.csv(\"Air_Quality.csv\")\n\n# 1.1.3 Reading by giving the absolute address\n\n#air_data &lt;- read.csv('D:/Summer Coding Camp/Jeong_CodingCamp_draft_code/Jeong_CodingCamp_draft_code/Air_Quality.csv')\n\n\n\n2. Calculate the mean of a numeric column of your choice.\n\nmean_air &lt;- mean(air_data$Year)\nprint(mean_air)\n\n[1] 2014.657\n\n\n\n\n3. Identify the number of unique values in a categorical column.\n\nnum_unique_values &lt;- length(unique(air_data$Name))\nprint(num_unique_values)\n\n[1] 18"
  },
  {
    "objectID": "assignment01.html#q2",
    "href": "assignment01.html#q2",
    "title": "Assignment 01",
    "section": "Q2",
    "text": "Q2\n\n1. Import the Air_Quality data and check the number of rows and columns.\n\ndimensions &lt;- dim(air_data)\nprint(\"Rows    Columns\")\n\n[1] \"Rows    Columns\"\n\nprint(dimensions)\n\n[1] 14077     9\n\n\n\n\n2. Calculate the median of a numeric column.\n\nmedian_air &lt;- median(air_data$Year)\nprint(median_air)\n\n[1] 2015\n\n\n\n\n3. Count the number of missing values in the Electric Vehicle data.\n\nlibrary(readxl)\nev_data &lt;- read_excel(\"Electric_Vehicle_Population_Data.xlsx\")\nnum_missing_values &lt;- sum(is.na(ev_data))\nprint(paste(\"Number of missing values in the entire data frame:\", num_missing_values))\n\n[1] \"Number of missing values in the entire data frame: 494\""
  },
  {
    "objectID": "assignment01.html#q3",
    "href": "assignment01.html#q3",
    "title": "Assignment 01",
    "section": "Q3",
    "text": "Q3\n\nExport the Electric Vehicle data to a new CSV file.\n\n\nwrite.csv(ev_data, \"electric_vehicle_data.csv\", row.names = FALSE)\n\n\nModify a column (e.g., replace missing values) and save the modified data.\n\n\nev_data$Postal_Code[is.na(ev_data$Postal_Code)] &lt;- \"XXXXX\"\nprint(ev_data)\n\n# A tibble: 194,232 × 17\n   `VIN (1-10)` County    City         State Postal_Code Model_Year Make   Model\n   &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 1C4RJXN66R   Snohomish Everett      WA    98204             2024 JEEP   WRAN…\n 2 KNDJX3AEXG   King      Renton       WA    98058             2016 KIA    SOUL \n 3 5YJ3E1EA3L   King      Seattle      WA    98125             2020 TESLA  MODE…\n 4 1G1RC6S5XH   Kitsap    Port Orchard WA    98367             2017 CHEVR… VOLT \n 5 5UXTA6C09P   Snohomish Monroe       WA    98272             2023 BMW    X5   \n 6 1FMCU0EZXN   Yakima    Moxee        WA    98936             2022 FORD   ESCA…\n 7 5YJSA1DNXD   Thurston  Olympia      WA    98506             2013 TESLA  MODE…\n 8 1N4AZ0CP8F   Snohomish Monroe       WA    98272             2015 NISSAN LEAF \n 9 WP1AE2A21J   Snohomish Everett      WA    98208             2018 PORSC… CAYE…\n10 1N4BZ1BV4N   Snohomish Everett      WA    98208             2022 NISSAN LEAF \n# ℹ 194,222 more rows\n# ℹ 9 more variables: `Electric Vehicle Type` &lt;chr&gt;,\n#   `Clean Alternative Fuel Vehicle (CAFV) Eligibility` &lt;chr&gt;,\n#   Electric_Range &lt;dbl&gt;, `Base MSRP` &lt;dbl&gt;, `Legislative District` &lt;dbl&gt;,\n#   `DOL Vehicle ID` &lt;dbl&gt;, `Vehicle Location` &lt;chr&gt;, `Electric Utility` &lt;chr&gt;,\n#   `2020 Census Tract` &lt;dbl&gt;"
  },
  {
    "objectID": "assignment01.html#creating-visualizations-for-age-adjusted-death-rate",
    "href": "assignment01.html#creating-visualizations-for-age-adjusted-death-rate",
    "title": "Assignment 01",
    "section": "2.1 Creating Visualizations for Age-adjusted Death Rate",
    "text": "2.1 Creating Visualizations for Age-adjusted Death Rate\n\n1. Create a basic histogram of Age_adjusted_Death_Rate with mean and median lines.\n\nlife_data &lt;- read.csv(\"US_Life_expectancy.csv\")\n# Replotting with mean and median\nmean_le &lt;- mean(life_data$Average_Life_Expectancy, na.rm = TRUE)\nmedian_le &lt;- median(life_data$Average_Life_Expectancy, na.rm = TRUE)\nhist(life_data$Average_Life_Expectancy,\n     main = \"Histogram of Average Life Expectancy with Mean and Median\",\n     xlab = \"Average Life Expectancy\",\n     col = \"lightblue\", border = \"black\")\nabline(v = mean_le, col = \"grey\", lwd = 2)\nabline(v = median_le, col = \"pink\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Mean\", \"Median\"), col = c(\"grey\", \"pink\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n2. Create a boxplot of Age_adjusted_Death_Rate by gender with colors.\n\nboxplot(Average_Life_Expectancy ~ Gender, data = life_data,\n        main = \"Boxplot of Life Expectancy by Gender\",\n        xlab = \"Gender\", ylab = \"Average Life Expectancy\",\n        col = c(\"lightblue\", \"lightgreen\"))\n\n\n\n\n\n\n\n\n\n\n3. Create a scatter plot of Year vs. Age_adjusted_Death_Rate with points colored by gender.\n\nplot(life_data$Year, life_data$Average_Life_Expectancy,\n     xlab = \"Year\", ylab = \"Average Life Expectancy\",\n     main = \"Scatter Plot of Life Expectancy Over Time by Gender\",\n     pch = 19, col = ifelse(life_data$Gender == \"Male\", \"grey\", \"cornflowerblue\"))\nlegend(\"topleft\", legend = c(\"Male\", \"Female\"), col = c(\"grey\", \"cornflowerblue\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n4. Create a line plot of Year vs. Age_adjusted_Death_Rate with lines for each gender.\n\nmale_data &lt;- subset(life_data, Gender == \"Male\")\nfemale_data &lt;- subset(life_data, Gender == \"Female\")\nplot(male_data$Year, male_data$Average_Life_Expectancy,\n     type = \"l\", col = \"lightblue\", lwd = 2,\n     xlab = \"Year\", ylab = \"Average Life Expectancy (years)\",\n     main = \"Life Expectancy by Gender Over Time\",\n     lty = 1,\n     xlim = c(min(life_data$Year), max(life_data$Year)), \n     ylim = c(min(life_data$Average_Life_Expectancy, na.rm = TRUE), \n              max(life_data$Average_Life_Expectancy, na.rm = TRUE)))\nlines(female_data$Year, female_data$Average_Life_Expectancy, col = \"red4\", lwd = 2, lty = 2)\n\nlegend(\"bottomright\", legend = c(\"Male\", \"Female\"), col = c(\"lightblue\", \"red4\"), lwd = 2, lty = c(1, 2))\ntext(1918, 75, \"End of World War I\", col = \"black\")\nabline(v = 1918, col = \"black\", lwd = 1, lty = 3)"
  },
  {
    "objectID": "assignment01.html#introduction",
    "href": "assignment01.html#introduction",
    "title": "Assignment 01",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThere were no exercises in this section."
  },
  {
    "objectID": "assignment01.html#loading-the-data",
    "href": "assignment01.html#loading-the-data",
    "title": "Assignment 01",
    "section": "1.2 Loading the Data",
    "text": "1.2 Loading the Data\nThere were no exercises in this section."
  },
  {
    "objectID": "assignment01.html#grouping-and-summarizing-data",
    "href": "assignment01.html#grouping-and-summarizing-data",
    "title": "Assignment 01",
    "section": "1.3 Grouping and Summarizing Data",
    "text": "1.3 Grouping and Summarizing Data\nGroup the data by Year, Aggregate by sum the Age_adjusted_Death_Rate\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nyear_data &lt;- life_data %&gt;%\n  group_by(Year) %&gt;%\n  summarize(Total_Death_Rate = sum(Age_adjusted_Death_Rate, na.rm = TRUE))\nhead(year_data)\n\n# A tibble: 6 × 2\n   Year Total_Death_Rate\n  &lt;int&gt;            &lt;dbl&gt;\n1  1900            5041.\n2  1901            4951 \n3  1902            4611.\n4  1903            4764.\n5  1904            5012.\n6  1905            4853."
  },
  {
    "objectID": "assignment01.html#joining-data-frames",
    "href": "assignment01.html#joining-data-frames",
    "title": "Assignment 01",
    "section": "1.4 Joining Data Frames",
    "text": "1.4 Joining Data Frames\nCreate a new dataset with a column specifying if the year is before or after 1945, and join it ON the original dataset, keeping the original dataset intact and without na values. Hint: which side of join you should use?\n\nadditional_data_war &lt;- data.frame(\n  Year = 1900:2020,\n  Year_Category = ifelse(1900:2020 &lt;1945, \"Before WW2\", \"After WW2\")\n)\n\n# Joining data frames\njoined_data_ww2 &lt;- left_join(life_data, additional_data_war, by = 'Year')\n\n# Viewing the joined data\nhead(joined_data_ww2)\n\n  Year Gender Average_Life_Expectancy Age_adjusted_Death_Rate Year_Category\n1 1900 Female                    48.3                  2410.4    Before WW2\n2 1901 Female                    50.6                  2350.5    Before WW2\n3 1902 Female                    53.4                  2162.8    Before WW2\n4 1903 Female                    52.0                  2250.6    Before WW2\n5 1904 Female                    49.1                  2358.8    Before WW2\n6 1905 Female                    50.2                  2287.7    Before WW2"
  },
  {
    "objectID": "assignment01.html#adding-data-using-function-with-mutate",
    "href": "assignment01.html#adding-data-using-function-with-mutate",
    "title": "Assignment 01",
    "section": "1.5 Adding data using function with mutate()",
    "text": "1.5 Adding data using function with mutate()\n\nlife_data_norm &lt;- mutate(\n  life_data, \n  life_normalized = (Average_Life_Expectancy - min(Average_Life_Expectancy)) / \n                               (max(Average_Life_Expectancy) - min(Average_Life_Expectancy))\n)\n\nhead(life_data_norm[,c(1,5)] )\n\n  Year life_normalized\n1 1900       0.2617450\n2 1901       0.3131991\n3 1902       0.3758389\n4 1903       0.3445190\n5 1904       0.2796421\n6 1905       0.3042506"
  },
  {
    "objectID": "assignment01.html#exercise-1-descriptive-statistics",
    "href": "assignment01.html#exercise-1-descriptive-statistics",
    "title": "Assignment 01",
    "section": "Exercise 1: Descriptive Statistics",
    "text": "Exercise 1: Descriptive Statistics\n\nQ1-1.\nCalculate the mean and standard deviation of the carat, price, and depth variables.\n\nlibrary(\"ggplot2\")\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:NLP':\n\n    annotate\n\ndata(\"diamonds\")\nmean_carat &lt;- mean(diamonds$carat, na.rm = TRUE)\nprint(mean_carat)\n\n[1] 0.7979397\n\nsd_carat &lt;- sd(diamonds$carat, na.rm = TRUE)\nprint(sd_carat)\n\n[1] 0.4740112\n\nmean_price &lt;- mean(diamonds$price, na.rm = TRUE)\nprint(mean_price)\n\n[1] 3932.8\n\nsd_price &lt;- sd(diamonds$price, na.rm = TRUE)\nprint(sd_price)\n\n[1] 3989.44\n\nmean_depth &lt;- mean(diamonds$depth, na.rm = TRUE)\nprint(mean_depth)\n\n[1] 61.7494\n\nsd_depth &lt;- sd(diamonds$depth, na.rm = TRUE)\nprint(sd_depth)\n\n[1] 1.432621\n\n\n\n\nQ1-2.\nUse the summary() function to get a detailed summary of the price variable.\n\nprint(summary(diamonds$price))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    326     950    2401    3933    5324   18823 \n\n\n\n\nQ1-3.\nDetermine the number of diamonds within each cut category using the table() function.\n\nprint(table(diamonds$cut))\n\n\n     Fair      Good Very Good   Premium     Ideal \n     1610      4906     12082     13791     21551"
  },
  {
    "objectID": "assignment01.html#exercise-2-t-test",
    "href": "assignment01.html#exercise-2-t-test",
    "title": "Assignment 01",
    "section": "Exercise 2: t-test",
    "text": "Exercise 2: t-test\n\nQ2\n\nSubset the diamonds dataset to include only diamonds with an “Ideal” or “Fair” cut.\nPerform a t-test to compare the mean prices between these two cut categories.\nInterpret the p-value to determine if the difference in mean prices is statistically significant.\n\n\ndiamonds_subset &lt;- subset(diamonds, cut %in% c(\"Ideal\", \"Fair\"))\nt_test_result &lt;- t.test(price ~ cut, data = diamonds_subset)\n\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  price by cut\nt = 9.7484, df = 1894.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Fair and group Ideal is not equal to 0\n95 percent confidence interval:\n  719.9065 1082.5251\nsample estimates:\n mean in group Fair mean in group Ideal \n           4358.758            3457.542 \n\n\nInterpretation:\n\nt = 9.7484, df = 1894.8\nThe alternative hypothesis is that the difference in means between the “Fair” and “Ideal” groups is not equal to 0. Since the p-value is near zero, we reject the null hypothesis. The means of the two groups are different.\nThe 95% CI is between 719.9065 and 1082.5251.\nThe mean for Fair is 4358.758.\nThe mean for Ideal is 3457.542."
  },
  {
    "objectID": "assignment01.html#exercise-3-anova",
    "href": "assignment01.html#exercise-3-anova",
    "title": "Assignment 01",
    "section": "Exercise 3 ANOVA",
    "text": "Exercise 3 ANOVA\n\nQ3-1\n\nPerform an ANOVA test to assess the effect of clarity on diamond price.\nIf significant differences are found, conduct a Tukey’s HSD post-hoc test to identify which clarity levels differ from each other.\n\n\nsummary(aov(price ~ clarity, data = diamonds))\n\n               Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \nclarity         7 2.331e+10 3.330e+09     215 &lt;2e-16 ***\nResiduals   53932 8.352e+11 1.549e+07                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClarity has a significant effect on price.\n\n\nQ3-2\n\nIf significant differences are found, conduct a Tukey’s HSD post-hoc test to identify which clarity levels differ from each other.\n\n\nTukeyHSD(aov(price ~ clarity, data = diamonds))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = price ~ clarity, data = diamonds)\n\n$clarity\n                   diff          lwr         upr     p adj\nSI2-I1     1138.8599147   683.395891  1594.32394 0.0000000\nSI1-I1       71.8324571  -378.570901   522.23582 0.9997320\nVS2-I1        0.8207037  -450.377702   452.01911 1.0000000\nVS1-I1      -84.7132999  -542.298929   372.87233 0.9992819\nVVS2-I1    -640.4316203 -1109.531923  -171.33132 0.0009165\nVVS1-I1   -1401.0540535 -1881.569711  -920.53840 0.0000000\nIF-I1     -1059.3295848 -1580.334655  -538.32451 0.0000000\nSI1-SI2   -1067.0274575 -1229.386830  -904.66808 0.0000000\nVS2-SI2   -1138.0392109 -1302.591274  -973.48715 0.0000000\nVS1-SI2   -1223.5732146 -1404.907129 -1042.23930 0.0000000\nVVS2-SI2  -1779.2915349 -1987.983831 -1570.59924 0.0000000\nVVS1-SI2  -2539.9139681 -2773.136347 -2306.69159 0.0000000\nIF-SI2    -2198.1894995 -2506.318797 -1890.06020 0.0000000\nVS2-SI1     -71.0117534  -220.988718    78.96521 0.8410824\nVS1-SI1    -156.5457571  -324.764949    11.67343 0.0899007\nVVS2-SI1   -712.2640774  -909.667681  -514.86047 0.0000000\nVVS1-SI1  -1472.8865106 -1696.064436 -1249.70859 0.0000000\nIF-SI1    -1131.1620420 -1431.760399  -830.56369 0.0000000\nVS1-VS2     -85.5340037  -255.870471    84.80246 0.7958312\nVVS2-VS2   -641.2523240  -840.463263  -442.04138 0.0000000\nVVS1-VS2  -1401.8747572 -1626.652874 -1177.09664 0.0000000\nIF-VS2    -1060.1502885 -1361.938605  -758.36197 0.0000000\nVVS2-VS1   -555.7183203  -769.001243  -342.43540 0.0000000\nVVS1-VS1  -1316.3407535 -1553.679770 -1079.00174 0.0000000\nIF-VS1     -974.6162849 -1285.873083  -663.35949 0.0000000\nVVS1-VVS2  -760.6224332 -1019.466585  -501.77828 0.0000000\nIF-VVS2    -418.8979645  -746.848084   -90.94785 0.0027364\nIF-VVS1     341.7244687    -2.356168   685.80510 0.0531204"
  },
  {
    "objectID": "assignment01.html#exercise-4-simple-linear-regression",
    "href": "assignment01.html#exercise-4-simple-linear-regression",
    "title": "Assignment 01",
    "section": "Exercise 4: Simple Linear Regression",
    "text": "Exercise 4: Simple Linear Regression\n\nQ4\n\nFit a simple linear regression model with carat as the predictor and price as the response variable.\nDisplay the summary of the regression model, including coefficients, R-squared, and p-value.\nCreate a scatter plot of price versus carat, and add the regression line to visualize the relationship.\n\n\ndiamond_lm &lt;- lm(price ~ carat, data = diamonds)\nsummary(diamond_lm)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Create a scatter plot and add the regression line\nplot(diamonds$carat, diamonds$price, main = \"Linear Regression of Price on Carat\",\n     xlab = \"Carat\", ylab = \"Price\",pch = 19, col = \"pink\")\nabline(diamond_lm, col = \"cornflowerblue\")"
  },
  {
    "objectID": "assignment01.html#exercise-1-fitting-a-multiple-linear-regression-model",
    "href": "assignment01.html#exercise-1-fitting-a-multiple-linear-regression-model",
    "title": "Assignment 01",
    "section": "Exercise 1: Fitting a Multiple Linear Regression Model",
    "text": "Exercise 1: Fitting a Multiple Linear Regression Model\n\nQ1-1\nUse the lm() function to create a multiple linear regression model with your selected variables.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndata(\"Boston\")\nstr(Boston)\n\n'data.frame':   506 obs. of  14 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n\n# Fitting the multiple linear regression model\nlm1 &lt;- lm(crim ~ indus + lstat + nox, data = Boston)\n\n\n\nQ1-2\nDisplay the summary of the model to interpret the coefficients, R-squared, and p-values.\n\n# Display the model summary\nsummary(lm1)\n\n\nCall:\nlm(formula = crim ~ indus + lstat + nox, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.330  -2.667  -0.557   1.169  81.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.35640    1.94271  -4.816 1.94e-06 ***\nindus        0.12046    0.07869   1.531  0.12644    \nlstat        0.35573    0.06050   5.880 7.49e-09 ***\nnox         12.84903    4.60314   2.791  0.00545 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.489 on 502 degrees of freedom\nMultiple R-squared:  0.2465,    Adjusted R-squared:  0.242 \nF-statistic: 54.74 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nQ1-3\nFor each independent variable, interpret the coefficient in terms of its impact on the dependent variable.\ncrim will be -9.36 indus, lstat, and nox are zero. This is the intercept and is significant.\nindus is 0.12046. For every 1 increase in indus, crim will increase 0.12. This is not significant.\nlstat is 0.35573. For every 1 increase in lstat, crim will increase 0.36. This has a strong significance.\nnox is 12.84903. For every 1 increase in nox, crim will increase 12.85. This is significant."
  },
  {
    "objectID": "assignment01.html#exercise-2-refining-the-model-with-stepwise-regression",
    "href": "assignment01.html#exercise-2-refining-the-model-with-stepwise-regression",
    "title": "Assignment 01",
    "section": "Exercise 2: Refining the Model with Stepwise Regression",
    "text": "Exercise 2: Refining the Model with Stepwise Regression\n\nQ2-1\nApply forward selection, backward elimination, or bidirectional elimination to your model using the step() function.\nForward\n\nforward_model &lt;- step(lm(crim ~ 1, data = Boston), \n                      scope = list(lower = lm(crim ~ 1, data = Boston), \n                                   upper = lm(crim ~ indus + lstat + nox, data = Boston)), \n                      direction = \"forward\")\n\nStart:  AIC=2178.76\ncrim ~ 1\n\n        Df Sum of Sq   RSS    AIC\n+ lstat  1    7756.3 29607 2063.0\n+ nox    1    6621.4 30742 2082.1\n+ indus  1    6176.5 31187 2089.3\n&lt;none&gt;               37363 2178.8\n\nStep:  AIC=2063.03\ncrim ~ lstat\n\n        Df Sum of Sq   RSS    AIC\n+ nox    1    1322.0 28285 2041.9\n+ indus  1    1016.5 28590 2047.3\n&lt;none&gt;               29607 2063.0\n\nStep:  AIC=2041.92\ncrim ~ lstat + nox\n\n        Df Sum of Sq   RSS    AIC\n+ indus  1    131.43 28154 2041.6\n&lt;none&gt;               28285 2041.9\n\nStep:  AIC=2041.56\ncrim ~ lstat + nox + indus\n\nsummary(forward_model)\n\n\nCall:\nlm(formula = crim ~ lstat + nox + indus, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.330  -2.667  -0.557   1.169  81.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.35640    1.94271  -4.816 1.94e-06 ***\nlstat        0.35573    0.06050   5.880 7.49e-09 ***\nnox         12.84903    4.60314   2.791  0.00545 ** \nindus        0.12046    0.07869   1.531  0.12644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.489 on 502 degrees of freedom\nMultiple R-squared:  0.2465,    Adjusted R-squared:  0.242 \nF-statistic: 54.74 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\nBackward\n\nbackward_model &lt;- step(lm(crim ~ indus + lstat + nox, data = Boston), \n                       direction = \"backward\")\n\nStart:  AIC=2041.56\ncrim ~ indus + lstat + nox\n\n        Df Sum of Sq   RSS    AIC\n&lt;none&gt;               28154 2041.6\n- indus  1    131.43 28285 2041.9\n- nox    1    436.98 28590 2047.3\n- lstat  1   1939.05 30093 2073.3\n\nsummary(backward_model)\n\n\nCall:\nlm(formula = crim ~ indus + lstat + nox, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.330  -2.667  -0.557   1.169  81.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.35640    1.94271  -4.816 1.94e-06 ***\nindus        0.12046    0.07869   1.531  0.12644    \nlstat        0.35573    0.06050   5.880 7.49e-09 ***\nnox         12.84903    4.60314   2.791  0.00545 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.489 on 502 degrees of freedom\nMultiple R-squared:  0.2465,    Adjusted R-squared:  0.242 \nF-statistic: 54.74 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\nBoth\n\nboth_model &lt;- step(lm(crim ~ 1, data = Boston), \n                   scope = list(lower = lm(crim ~ 1, data = Boston), \n                                upper = lm(crim ~ indus + lstat + nox, data = Boston)), \n                   direction = \"both\")\n\nStart:  AIC=2178.76\ncrim ~ 1\n\n        Df Sum of Sq   RSS    AIC\n+ lstat  1    7756.3 29607 2063.0\n+ nox    1    6621.4 30742 2082.1\n+ indus  1    6176.5 31187 2089.3\n&lt;none&gt;               37363 2178.8\n\nStep:  AIC=2063.03\ncrim ~ lstat\n\n        Df Sum of Sq   RSS    AIC\n+ nox    1    1322.0 28285 2041.9\n+ indus  1    1016.5 28590 2047.3\n&lt;none&gt;               29607 2063.0\n- lstat  1    7756.3 37363 2178.8\n\nStep:  AIC=2041.92\ncrim ~ lstat + nox\n\n        Df Sum of Sq   RSS    AIC\n+ indus  1    131.43 28154 2041.6\n&lt;none&gt;               28285 2041.9\n- nox    1   1322.02 29607 2063.0\n- lstat  1   2456.88 30742 2082.1\n\nStep:  AIC=2041.56\ncrim ~ lstat + nox + indus\n\n        Df Sum of Sq   RSS    AIC\n&lt;none&gt;               28154 2041.6\n- indus  1    131.43 28285 2041.9\n- nox    1    436.98 28590 2047.3\n- lstat  1   1939.05 30093 2073.3\n\nsummary(both_model)\n\n\nCall:\nlm(formula = crim ~ lstat + nox + indus, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.330  -2.667  -0.557   1.169  81.408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.35640    1.94271  -4.816 1.94e-06 ***\nlstat        0.35573    0.06050   5.880 7.49e-09 ***\nnox         12.84903    4.60314   2.791  0.00545 ** \nindus        0.12046    0.07869   1.531  0.12644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.489 on 502 degrees of freedom\nMultiple R-squared:  0.2465,    Adjusted R-squared:  0.242 \nF-statistic: 54.74 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nQ2-2\nCompare the new model with your initial model and discuss any differences in the selected variables and model performance.\nlstat and nox are the most significant predictors of crime rate. Adding indus only slightly improves the model."
  },
  {
    "objectID": "assignment01.html#selecting-variables-and-fitting-a-regression-model",
    "href": "assignment01.html#selecting-variables-and-fitting-a-regression-model",
    "title": "Assignment 01",
    "section": "Selecting Variables and Fitting a Regression Model",
    "text": "Selecting Variables and Fitting a Regression Model\n\nQ1\n\nUse the lm() function to create a multiple linear regression model.\nDisplay the summary of the model to interpret the coefficients, R-squared, and p-values.\n\n\nlm1 &lt;- lm(price ~ color + depth + table, data = diamonds)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ color + depth + table, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6738  -2593  -1326   1302  15936 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -12295.321    992.830 -12.384  &lt; 2e-16 ***\ncolor.L       2051.961     56.700  36.190  &lt; 2e-16 ***\ncolor.Q        146.868     53.863   2.727   0.0064 ** \ncolor.C       -273.608     50.687  -5.398 6.77e-08 ***\ncolor^4         60.871     46.552   1.308   0.1910    \ncolor^5       -241.934     44.009  -5.497 3.87e-08 ***\ncolor^6         62.399     39.908   1.564   0.1179    \ndepth           52.593     12.282   4.282 1.85e-05 ***\ntable          229.054      7.876  29.083  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3896 on 53931 degrees of freedom\nMultiple R-squared:  0.0466,    Adjusted R-squared:  0.04646 \nF-statistic: 329.5 on 8 and 53931 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignment01.html#residual-analysis",
    "href": "assignment01.html#residual-analysis",
    "title": "Assignment 01",
    "section": "Residual Analysis",
    "text": "Residual Analysis\n\nQ2\n\nCreate residual plots to check for linearity, homoscedasticity, and normality.\nUse the par() function to display multiple plots at once.\n\n\npar(mfrow = c(2, 2))\nplot(lm1)"
  },
  {
    "objectID": "assignment01.html#checking-for-multicollinearity-and-heteroscedasticity",
    "href": "assignment01.html#checking-for-multicollinearity-and-heteroscedasticity",
    "title": "Assignment 01",
    "section": "Checking for Multicollinearity and Heteroscedasticity",
    "text": "Checking for Multicollinearity and Heteroscedasticity\n\nQ3\n\nCalculate VIF for each independent variable in your model.\nUse the bptest() function from the lmtest package to test for heteroscedasticity.\nInterpret the results of the test.\n\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nvif_values &lt;- vif(lm1)\nvif_values\n\n          GVIF Df GVIF^(1/(2*Df))\ncolor 1.007114  6        1.000591\ndepth 1.100311  1        1.048957\ntable 1.100780  1        1.049181\n\n\n\n# Install the lmtest package if not already installed\n# install.packages(\"lmtest\")\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n# Breusch-Pagan test for heteroscedasticity\nbp_test &lt;- bptest(lm1)\nbp_test\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm1\nBP = 785.22, df = 8, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "assignment01.html#model-comparison-and-evaluation",
    "href": "assignment01.html#model-comparison-and-evaluation",
    "title": "Assignment 01",
    "section": "Model Comparison and Evaluation",
    "text": "Model Comparison and Evaluation\n\nQ4\n\nFit an alternative model with a different set of independent variables.\nCompare the models based on Adjusted R-squared and AIC to determine which model is better.\n\n\n# Alternative model with different independent variables\nlm2 &lt;- lm(price ~ carat + cut + color, data = diamonds)"
  },
  {
    "objectID": "SPARQL.html",
    "href": "SPARQL.html",
    "title": "SPARQL",
    "section": "",
    "text": "Nomisma SPARQL Endpoint"
  },
  {
    "objectID": "SPARQL.html#example-1",
    "href": "SPARQL.html#example-1",
    "title": "SPARQL",
    "section": "Example 1",
    "text": "Example 1\nModified from R networks analysis / works\nSELECT DISTINCT ?hoard ?mint ?mintlat ?mintlong WHERE {\n{\n  ?hoard void:inDataset &lt;http://numismatics.org/chrr/&gt; ;\n  dcterms:tableOfContents/nmo:hasTypeSeriesItem/nmo:hasMint ?mint .\n}\nUNION\n{ ?hoard a nmo:Hoard ;\ndcterms:tableOfContents [ nmo:hasTypeSeriesItem ?tsi ] .\n?tsi nmo:hasMint ?mint .\n}\nOPTIONAL { ?hoard nmo:hasFindspot [\ngeo:lat ?hoardlat ;\ngeo:long ?hoardlong ] }\nOPTIONAL { ?mint geo:location [\ngeo:lat ?mintlat ;\ngeo:long ?mintlong ] }\n}"
  },
  {
    "objectID": "Prepare07.html",
    "href": "Prepare07.html",
    "title": "Prepare for Class07",
    "section": "",
    "text": "Prepare for Class 07\n\nAssignment 4\n\nNot completed. No assignment available yet.\n\nAssigned reading\n\nAttewell, Paul, and David Monaghan. 2015. Data mining for the social sciences: An introduction. Univ of California Press. (AM)\n\nReview Chapters 3, 4\n\nCompleted\n\nRead 5, 6\n\nCompleted\n\n\n\nWhat is cross-validation?\n\n\nFrom Gareth et al. in ILSR: Cross-validation “estimate[s] the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.”\n\n\nParaphrased from ChatGPT: Cross-validation is a resampling technique used to evaluate and improve the performance of a model by splitting the data into training and testing sets. This method helps ensure that the model generalizes well to unseen data and is not overfitting to a single training set.\n\n\nHow to sample big data? Why it is important? What are the difficulties? How to solve this problem?\n\nThere are several ways to sample big data:\n\nRandom: randomly select a sample of the data\nStratified: randomly sample from homogeneous subgroups\nClusters: randomly select clusters and sample all points within\nSystematic: select every k-th element\nReservoir: Keep a fixed-size reservoir, replacing as data streams\nImportance: sample points based on weights\nBernoulli: sample each data point with probability p\nAdaptive: adjust sampling rates based on initial results\nProgressive: Start with a small sample and increase size\nOthers\n\nThese methods are useful because:\n\nEfficiency: Large datasets can be computationally expensive to process.\nSpeed: Larger datasets take longer to process. Smaller datasets can be processed real-time.\nCost: Data storage and compute power cost money. Smaller datasets need less.\nPracticality: We need the processing done in a reasonable amount of time\nFocus: Large datasets can be noisy. A reduced dataset can improve quality.\nScale: Sampling will allow for larger data volumes\nInitial Analysis: Understanding the structure and patterns is easier through sampling\nBias detection: Analyzing differences in sample sets allows for pattern detection\nInference: Proper sampling gives confidence that predictions are accurate\n\nDifficulties (somewhat repetitive):\n\nData Volume: Big data can be very big\nStorage Constraints: Some volume is so large that storing historical data is not possible\nDistribution: Data can be coming from disparate sources that are not co-located\nData distributions imbalance: Some data sets are so large that anomolies may not appear in the sample\nBias: The act of sampling could itself be a bias. For instance active versus inactive users.\nStreaming: real-time data such as media feeds or stock prices have continuous data streams\nCost: The costs for processing the data can be high\nPrivacy and security: personal or sensitive data are a challenge to sample properly\nTime issues: Data may only be valid for a specific time frame\nUnstructured data: Some data may be unstructured such as videos\n\nHow can these problems be mitigated?\n\nData Volume and Storage: use distributed frameworks for parallel processing with compression\nData issues: Choose a proper sampling technique\nDistributed data: set presampled subsets\nBias: use techniques like cross-validation\nReal-time data: use sampling techniques like sliding-window or resevoir sampling\nComputation costs: use approximate query processing and don’t load entire datasets\nPrivacy and security: anonymize the data\nTime-based data: use windowed or adaptive sampling\nBandwidth constraints: process samples before sending data\nUnstructured data: use cluster based sampling or apply pre-training"
  },
  {
    "objectID": "Prepare05.html",
    "href": "Prepare05.html",
    "title": "Prepare for Class05",
    "section": "",
    "text": "Assigned reading\n\nContinue on: “Mining Scientific Papers, Volume II: Knowledge Discovery and Data Exploitation”\n\n\nCompleted\n\n\nSilge, Julia & Robinson, David. 2019. Text mining with R: a tidy approach. Latest online edition (https://www.tidytextmining.com/)\n\n\nReviewed\n\nIdentify a topic with text data you like to analyze (e.g. YouTube, news data)\nCompare the language used in several original gothic novels:\n\nThe Vampyre (1819) – John Polidori\nMelmoth the Wanderer (1820) – Charles Maturin\nZofloya, or The Moor (1806) – Charlotte Dacre\nThe Monk (1796) – Matthew Lewis\nFrankenstein; or the Modern Prometheus (1818) - Mary Shelly\n\n\nWhat data source?\nProject Gutenberg\nHow to get the data?\nUsing the text mining capabilities of TidyR, the data can be gathered and processed.\n\n\n\n\n\n\n\ngraph TD\n  B[Load Libraries: TidyR, gutenbergr, TidyText] \n  B --&gt; C[Search for Book in gutenberg_metadata] \n  C --&gt; D[Download Book Text Using gutenberg_download] \n  D --&gt; E[Preprocess Text: Remove Stop Words, Tokenize] \n  E --&gt; F[Perform NLP: Word Frequency, N-Grams] \n  F --&gt; G[Conduct Sentiment Analysis Using tidytext] \n  G --&gt; H[Visualize Results: Word Clouds, Sentiment Trends] \n\n\n\n\n\n\n\nReminder: Proposal due 2/25/2025 (Please read KM_Preparing Research Proposal.pdf)\n\nProposal"
  },
  {
    "objectID": "Prepare05.html#prepare-for-class",
    "href": "Prepare05.html#prepare-for-class",
    "title": "Prepare for Class05",
    "section": "",
    "text": "Assigned reading\n\nContinue on: “Mining Scientific Papers, Volume II: Knowledge Discovery and Data Exploitation”\n\n\nCompleted\n\n\nSilge, Julia & Robinson, David. 2019. Text mining with R: a tidy approach. Latest online edition (https://www.tidytextmining.com/)\n\n\nReviewed\n\nIdentify a topic with text data you like to analyze (e.g. YouTube, news data)\nCompare the language used in several original gothic novels:\n\nThe Vampyre (1819) – John Polidori\nMelmoth the Wanderer (1820) – Charles Maturin\nZofloya, or The Moor (1806) – Charlotte Dacre\nThe Monk (1796) – Matthew Lewis\nFrankenstein; or the Modern Prometheus (1818) - Mary Shelly\n\n\nWhat data source?\nProject Gutenberg\nHow to get the data?\nUsing the text mining capabilities of TidyR, the data can be gathered and processed.\n\n\n\n\n\n\n\ngraph TD\n  B[Load Libraries: TidyR, gutenbergr, TidyText] \n  B --&gt; C[Search for Book in gutenberg_metadata] \n  C --&gt; D[Download Book Text Using gutenberg_download] \n  D --&gt; E[Preprocess Text: Remove Stop Words, Tokenize] \n  E --&gt; F[Perform NLP: Word Frequency, N-Grams] \n  F --&gt; G[Conduct Sentiment Analysis Using tidytext] \n  G --&gt; H[Visualize Results: Word Clouds, Sentiment Trends] \n\n\n\n\n\n\n\nReminder: Proposal due 2/25/2025 (Please read KM_Preparing Research Proposal.pdf)\n\nProposal"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "flowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D{Result one}\n  C --&gt; E[Result two]"
  },
  {
    "objectID": "test.html#abstract",
    "href": "test.html#abstract",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "Abstract",
    "text": "Abstract\nThe book, The Ancient Economy by Moses Finley (1973) challenged assumptions about economic freedom in the ancient Greek and Roman Empires. Social status, political structures, and ideological norms were greater forces than market forces. This project will look at the literature decade-by-decade to see if and how it changed over time."
  },
  {
    "objectID": "project02.html",
    "href": "project02.html",
    "title": "Project - Proposal",
    "section": "",
    "text": "The book, The Ancient Economy by Moses Finley (1973) challenged assumptions about economic freedom in the ancient Greek and Roman Empires. Social status, political structures, and ideological norms were greater forces than market forces, according to Finley. This project will look at the literature decade-by-decade to see if and how language about economics changed over time. Introduction\nMoses Finley (1973) challenged assumptions about economic freedom in the ancient Greek and Roman Empires. Social status, political structures, and ideological norms were greater forces than market forces. Unlike our contemporary capitalist economies, where individual economic decisions at various levels is central, the Greco-Roman world operated under hierarchies that limited economic freedom to the privileged few.\nSlavery formed the backbone of production leaving these laborers no control over their economic lives. Even free citizens faced restrictions such as lack of political privilege, land ownership issues, and state interventions in areas such as price control. Finley described the elite’s disdain for commerce and manual labor, reinforcing a preference for land-based wealth over market participation.\nIn his arguments, he emphasized the dominance of household economies, where self-sufficiency was prioritized over trade. Finley relied on the supposition that economic decisions were driven more by status, honor, and political power than by rational market incentives. He held that modern economic theories cannot or should not be applied to ancient societies, since the deeply embedded social structures constrained economic freedom.\nWhile this was a seminal writing on ancient economies, subsequent researchers have pushed back on his assertions. For instance Kevin Green (Green 2000) argues that Finley underestimated the impact that technology had by stating: “The principal conclusion is that Finley underestimated the spread of technological improvements, especially under the Romans.” An example of this is Roman grain production: “However, we now know that mechanical grain-mills driven by muscle power or water were common on Roman estates, in towns, and in military contexts throughout the empire.” These general sentiments have been echoed by other scholars.\nThis project’s primary purpose is to develop a workflow to use current tools to examine contemporary documents. It is not to prove or disprove any single theory, but rather to examine the change, if any, of sentiment over time in the scholarship regarding the circumstances of economic activities in the ancient world.\n\n\n\nFor this project, the initial literature search has shown no similar analysis of modern interpretations of the ancient world; however, this could change in the future. There are many instances of NLP being applied to surviving texts in ancient languages, as well as applications to things like social media streams, books, poetry, etc.\nValdez (2020) applied NLP techniques to ADHD on PubMed over time, investigating the change in words, which is similar to this undertaking. Underwood (2012) has provided a quick explainer on Topic Modeling.\n\n\n\n\n\nThe first step in this analysis is to develop a body of works. Initial efforts, including working with the McDermott Library staff have indicated that developing a citation list of articles will be difficult. Developing a citation list for books might not be possible. Downloading articles will have to be one-at-a-time. Books may or may not be possible. Given the time constraints, we are expecting this to be a proof-of-concept with best-efforts for data. If the work is expanded, this will have to be re-examined.\n\n\n\nThere are several tools that we expect to look at to find the best fit with the analysis. Initially, without examining the data, it seems that Latent Dirichlet Allocation (LDA) is the first choice to analyze the data, with Non-Negative Matrix Factorization (NMF) the next choice. Two newer methods, Top2Vec and BERTopic are available, but only in Python. Since this project is more proof-of-concept, and work has already started in R, this will likely have to wait unless there is some necessitating tragedy.\nThe current toolchain that is being built relies on TidyR text processing. If any other processes are required, they will be incorporated.\n\n\n\n\nFor this proposal, a text-based analysis will be performed on a corpus of works, which have been segmented into decades. This corpus will be as large as feasible given the allotted time.\n\n\n\nFinley, Moses I. 1973. The Ancient Economy. Berkeley: University of California Press.\nGreene, Kevin. “Technological Innovation and Economic Progress in the Ancient World: M. I. Finley Re-Considered.” The Economic History Review 53, no. 1 (2000): 29–59. https://doi.org/10.1111/1468-0289.00151.\nUnderwood, T. (2012, April 7). Topic modeling made just simple enough. Stone Shell. Available online at: https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/ (accessed February 24, 2025).\nValdez, Danny, and Patricia Goodson. “Language Bias in Health Research: External Factors That Influence Latent Language Patterns.” Frontiers in Research Metrics and Analytics 5 (2020): 4–4. https://doi.org/10.3389/frma.2020.00004."
  },
  {
    "objectID": "project02.html#abstract",
    "href": "project02.html#abstract",
    "title": "Project - Proposal",
    "section": "",
    "text": "The book, The Ancient Economy by Moses Finley (1973) challenged assumptions about economic freedom in the ancient Greek and Roman Empires. Social status, political structures, and ideological norms were greater forces than market forces, according to Finley. This project will look at the literature decade-by-decade to see if and how language about economics changed over time. Introduction\nMoses Finley (1973) challenged assumptions about economic freedom in the ancient Greek and Roman Empires. Social status, political structures, and ideological norms were greater forces than market forces. Unlike our contemporary capitalist economies, where individual economic decisions at various levels is central, the Greco-Roman world operated under hierarchies that limited economic freedom to the privileged few.\nSlavery formed the backbone of production leaving these laborers no control over their economic lives. Even free citizens faced restrictions such as lack of political privilege, land ownership issues, and state interventions in areas such as price control. Finley described the elite’s disdain for commerce and manual labor, reinforcing a preference for land-based wealth over market participation.\nIn his arguments, he emphasized the dominance of household economies, where self-sufficiency was prioritized over trade. Finley relied on the supposition that economic decisions were driven more by status, honor, and political power than by rational market incentives. He held that modern economic theories cannot or should not be applied to ancient societies, since the deeply embedded social structures constrained economic freedom.\nWhile this was a seminal writing on ancient economies, subsequent researchers have pushed back on his assertions. For instance Kevin Green (Green 2000) argues that Finley underestimated the impact that technology had by stating: “The principal conclusion is that Finley underestimated the spread of technological improvements, especially under the Romans.” An example of this is Roman grain production: “However, we now know that mechanical grain-mills driven by muscle power or water were common on Roman estates, in towns, and in military contexts throughout the empire.” These general sentiments have been echoed by other scholars.\nThis project’s primary purpose is to develop a workflow to use current tools to examine contemporary documents. It is not to prove or disprove any single theory, but rather to examine the change, if any, of sentiment over time in the scholarship regarding the circumstances of economic activities in the ancient world."
  },
  {
    "objectID": "project02.html#literature-review",
    "href": "project02.html#literature-review",
    "title": "Project - Proposal",
    "section": "",
    "text": "For this project, the initial literature search has shown no similar analysis of modern interpretations of the ancient world; however, this could change in the future. There are many instances of NLP being applied to surviving texts in ancient languages, as well as applications to things like social media streams, books, poetry, etc.\nValdez (2020) applied NLP techniques to ADHD on PubMed over time, investigating the change in words, which is similar to this undertaking. Underwood (2012) has provided a quick explainer on Topic Modeling."
  },
  {
    "objectID": "project02.html#methodology",
    "href": "project02.html#methodology",
    "title": "Project - Proposal",
    "section": "",
    "text": "The first step in this analysis is to develop a body of works. Initial efforts, including working with the McDermott Library staff have indicated that developing a citation list of articles will be difficult. Developing a citation list for books might not be possible. Downloading articles will have to be one-at-a-time. Books may or may not be possible. Given the time constraints, we are expecting this to be a proof-of-concept with best-efforts for data. If the work is expanded, this will have to be re-examined.\n\n\n\nThere are several tools that we expect to look at to find the best fit with the analysis. Initially, without examining the data, it seems that Latent Dirichlet Allocation (LDA) is the first choice to analyze the data, with Non-Negative Matrix Factorization (NMF) the next choice. Two newer methods, Top2Vec and BERTopic are available, but only in Python. Since this project is more proof-of-concept, and work has already started in R, this will likely have to wait unless there is some necessitating tragedy.\nThe current toolchain that is being built relies on TidyR text processing. If any other processes are required, they will be incorporated."
  },
  {
    "objectID": "project02.html#results",
    "href": "project02.html#results",
    "title": "Project - Proposal",
    "section": "",
    "text": "For this proposal, a text-based analysis will be performed on a corpus of works, which have been segmented into decades. This corpus will be as large as feasible given the allotted time."
  },
  {
    "objectID": "project02.html#bibliography",
    "href": "project02.html#bibliography",
    "title": "Project - Proposal",
    "section": "",
    "text": "Finley, Moses I. 1973. The Ancient Economy. Berkeley: University of California Press.\nGreene, Kevin. “Technological Innovation and Economic Progress in the Ancient World: M. I. Finley Re-Considered.” The Economic History Review 53, no. 1 (2000): 29–59. https://doi.org/10.1111/1468-0289.00151.\nUnderwood, T. (2012, April 7). Topic modeling made just simple enough. Stone Shell. Available online at: https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/ (accessed February 24, 2025).\nValdez, Danny, and Patricia Goodson. “Language Bias in Health Research: External Factors That Influence Latent Language Patterns.” Frontiers in Research Metrics and Analytics 5 (2020): 4–4. https://doi.org/10.3389/frma.2020.00004."
  }
]